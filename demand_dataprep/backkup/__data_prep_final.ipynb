{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "\n",
    "# To Get Combinatiobs\n",
    "import itertools\n",
    "\n",
    "# Datetime Libraries\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "# Trend Seasonality\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SAS Connection Library\n",
    "import swat\n",
    "\n",
    "# In Order To Read Config File\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../config.json\", \"r\")\n",
    "params_ = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_all_process = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Excel Files\n",
    "btt_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Demand Sensing Sales History\") or doc_.startswith(\"Demand_Sensing_Sales_\")])\n",
    "hor_lst = sorted([doc_ for doc_ in os.listdir(\"../data/08092021\") if doc_.startswith(\"Horizon_Saha_\") ])\n",
    "pas_lst = sorted([doc_ for doc_ in os.listdir(\"../data/08092021\") if doc_.startswith(\"Siparişe_göre_Sales_History\")])\n",
    "saha_aktivite_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Demand_Sensing_Saha_Aktivit\") or doc_.startswith(\"Demand Sensing Saha Aktivit\")])\n",
    "pasifik_aktivite_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Pasifik Aktivite Datası\")])\n",
    "fiyat_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if \"Fiyat List\" in doc_])\n",
    "portfoy_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Portföy\")])\n",
    "eslenik_kod_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Ürün Eşlenik kodlar\")])\n",
    "kapsam_listeli = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Listeli Ürün\")])\n",
    "pas_siparis_lst = sorted([doc_ for doc_ in os.listdir(\"../data/siparis\") if doc_.startswith(\"Siparişe_göre_Sales_History\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosya Listelerini Okuma İşlemi: 0:00:00.029997\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Dosya Listelerini Okuma İşlemi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Read Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historik data ve koli içi adetlerini okuyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales Data\n",
    "pasifik_df_all = []\n",
    "btt_df_all = []\n",
    "horizon_saha_df_all = []\n",
    "chng_cols_beginning = {'Year': 'Yıl', 'Quarter': 'Çeyrek', 'Month': 'Ay', \n",
    "                       'Company Code': 'Şirket Kodu', 'Main Category Name': 'Ana Kategori Adı', \n",
    "                       'Category Name': 'Kategori Adı', 'Brand Name': 'Marka Adı', 'Product Code': 'Ürün Kodu', \n",
    "                       'Product Name': 'Ürün Adı', \"Ürün Adı (Mobis)\": 'Ürün Adı'}\n",
    "for docs_ in btt_lst:\n",
    "    btt_df_all.append(pd.read_excel(\"../data/\"+docs_, skiprows=1, sheet_name=\"BTT SAP Satış\", usecols=\"B:N\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "for docs_ in hor_lst:\n",
    "    horizon_saha_df_all.append(pd.read_excel(\"../data/08092021/\"+docs_, skiprows=1, sheet_name=\"Horizon Saha Satış\", usecols=\"B:L\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "for docs_ in pas_lst:\n",
    "    pasifik_df_all.append(pd.read_excel(\"../data/08092021/\"+docs_, sheet_name=\"Ürün Bazlı\", usecols=\"B:O\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "# Pasifik ve Horizon historic datasında koli içi adetini bir önceki versiyon olan excelin içerisinden okuyoruz. Bu yüzden siparis klasörünün içindeki dosyalarda gezeceğiz.\n",
    "pasifik_df_koli_ici_adet = [] \n",
    "for docs_ in pas_siparis_lst:\n",
    "    pasifik_df_koli_ici_adet.append(pd.read_excel(\"../data/siparis/\"+docs_, skiprows=1, sheet_name=\"Koli içi adet\", usecols=\"B:D\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "horizon_df_eski = [] \n",
    "for docs_ in btt_lst:\n",
    "    horizon_df_eski.append(pd.read_excel(\"../data/\"+docs_, skiprows=1, sheet_name=\"Horizon Saha Satış\", usecols=\"B:N\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "pasifik_df_all = pd.concat(pasifik_df_all)\n",
    "btt_df_all = pd.concat(btt_df_all)\n",
    "horizon_saha_df_all = pd.concat(horizon_saha_df_all)\n",
    "\n",
    "pasifik_df_koli_ici_adet = pd.concat(pasifik_df_koli_ici_adet)\n",
    "horizon_df_eski = pd.concat(horizon_df_eski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historik datanın Jupytere yüklenme süresi: 0:08:04.265945\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Historik datanın Jupytere yüklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sadece Gerekli Sütunlar Tutuluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all.drop(columns=[\"Organizasyon Kodu\", \"Grup Kodu.\", \"Pladis-Non Pladis\"], axis=1, inplace=True)\n",
    "horizon_saha_df_all.drop(columns=[\"Çeyrek\"], axis=1, inplace=True)\n",
    "btt_df_all.drop(columns=[\"Çeyrek\", \"Şirket Kodu\"], axis=1, inplace=True)\n",
    "\n",
    "btt_df_all[\"Grup Adı\"] = \"BTT\"\n",
    "pasifik_df_all.rename(columns={\"Ana Kategori\": \"Ana Kategori Adı\", \"Kategori\": \"Kategori Adı\", \"Ürün Adı (Orjinal)\": \"Ürün Adı\", \"Sipariş Miktarı(Dönüş. Koli)\": \"Koli\", \n",
    "                               \"Sipariş Brüt Tutar\": \"KG\", \"Sipariş Brüt KG\": \"TL\"}, inplace=True)\n",
    "\n",
    "horizon_saha_df_all.rename(columns={\"Horizon müşteri grup\": \"Grup Adı\", \"Ürün Adı (Orjinal)\": \"Ürün Adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik için koli içi adetler \"Koli içi adet\" sheetinde tutuluyor. Buradan alıyoruz fakat historik datada bulunmayan ürün kodları da var. Bu ürün kodlarından bazıları alfabetik harfler içeriyor. Bu durumu ortadan kaldırmak için aşağıdaki işlemi uyguluyoruz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['Yıl', 'Ay', 'Grup Adı', 'Ana Kategori Adı', 'Kategori Adı', 'Marka Adı', 'Ürün Kodu', 'Ürün Adı', 'Koli İçi Adet', 'Koli', 'KG', 'TL']\n",
    "ltrs = list(string.ascii_letters) # Alfabede bulunan tüm harfleri tutan liste. Bunu, koli içi adet dataframe'deki harf içeren ürün kodlarını elemek için tutuyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_koli_ici_adet.drop_duplicates(subset=pasifik_df_koli_ici_adet.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True) # Her dosyada 300k'ya yakın satır var. Yaklaşık 2M kadar satır geliyor çünkü 5-6 yıllık data okuyoruz. Bu yüzden drop duplicates ile satır sayısını azaltıyoruz.\n",
    "pasifik_df_koli_ici_adet = pasifik_df_koli_ici_adet[~(pasifik_df_koli_ici_adet[\"Ürün Kodu\"].str.contains(\"|\".join(ltrs), regex=True))] # Alfabetik harflerle başlayan ürün kodlarını elemine ediyoruz.\n",
    "\n",
    "pasifik_df_koli_ici_adet[\"Ürün Kodu\"] = pasifik_df_koli_ici_adet[\"Ürün Kodu\"].str.replace(\" \", \"\") # Bazı ürün kodları 0015 01 şeklinde gelmiş. Yani nümerik gözükse de arada boşluk var. O yüzden boşlukları kaldırıyoruz.\n",
    "pasifik_df_koli_ici_adet[\"Ürün Kodu\"] = pasifik_df_koli_ici_adet[\"Ürün Kodu\"].astype(\"int64\") # Bir üst satırda boşlukları kaldırdıktan sonra integer hale getiriyoruz.\n",
    "pasifik_df_koli_ici_adet = pasifik_df_koli_ici_adet[[\"Ürün Kodu\", \"Koli İçi Adet\"]] # Bu iki satır kalabilir. Left join ile historik dataya ekleyeceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all.merge(pasifik_df_koli_ici_adet, on=\"Ürün Kodu\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her dosyada; Ürün Adı sütununda \"sum:\" diye bir gözlem bulunuyor. Dip toplam yapıp datayı atmışlar. Bu satırları sildim.\n",
    "horizon_saha_df_all = horizon_saha_df_all[~(horizon_saha_df_all[\"Ürün Adı\"].str.contains(\"Sum:|sum:\"))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_kategori = horizon_df_eski[['Ürün Kodu','Kategori Adı']]\n",
    "horizon_kategori.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "horizon_df_koli_ici_adet = horizon_df_eski[['Yıl','Ay','Ürün Kodu','Koli İçi Adet']]\n",
    "horizon_df_koli_ici_adet.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all = horizon_saha_df_all.merge(horizon_kategori, how='left', on='Ürün Kodu')\n",
    "horizon_saha_df_all = horizon_saha_df_all.merge(horizon_df_koli_ici_adet, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[col_order]\n",
    "horizon_saha_df_all = horizon_saha_df_all[col_order]\n",
    "btt_df_all = btt_df_all[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saha Aktiviteleri\n",
    "saha_aktivite_detay = []\n",
    "saha_aktivite_cat = []\n",
    "\n",
    "for docs_ in saha_aktivite_lst:\n",
    "    saha_aktivite_detay.append(pd.read_excel(\"../data/\"+docs_, skiprows=1, sheet_name=\"Ürün Detay\", usecols=\"B:M\"))\n",
    "    saha_aktivite_cat.append(pd.read_excel(\"../data/\"+docs_, skiprows=1, sheet_name=\"Kategori\", usecols=\"B:I\"))\n",
    "saha_aktivite_detay = pd.concat(saha_aktivite_detay)\n",
    "saha_aktivite_cat = pd.concat(saha_aktivite_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Pasifik Aktiviteleri\n",
    "pasifik_aktivite_df = pd.read_excel(\"../data/\"+pasifik_aktivite_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Fiyat Listesi\n",
    "fiyat_lst_pasifik = pd.read_excel(\"../data/\"+fiyat_lst[0])\n",
    "fiyat_lst_horizon = pd.read_excel(\"../data/\"+fiyat_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Portföy\n",
    "pasifik_portfoy_df = pd.read_excel(\"../data/\"+portfoy_lst[0], sheet_name=\"Pasifik Portföy\", skiprows=3, usecols=\"D:H\")\n",
    "btt_portfoy_df = pd.read_excel(\"../data/\"+portfoy_lst[0], sheet_name=\"BTT Portföy\", skiprows=2, usecols=\"D:H\")\n",
    "horizon_portfoy_df = pd.read_excel(\"../data/\"+portfoy_lst[0], sheet_name=\"Horizon Portföy\", skiprows=2, usecols=\"E:I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Eşlenik Kodları\n",
    "eslenik_kod_df = pd.read_excel(\"../data/\"+eslenik_kod_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Calender\n",
    "calender_df = pd.read_excel(\"../data/Calender_Monthly.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Plasiyer Yarışma\n",
    "yarisma_df = pd.read_excel(\"../data/2018-2021 Yarışmaları_v2.xlsx\", sheet_name=\"yarisma_historik_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eslenik_kod_df[\"En Güncel Kod\"] = eslenik_kod_df[\"En Güncel Kod\"].apply(lambda x: int(x) if x not in ['delist ', \"delist\", \"Delist\"] else x.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a101_kapsam = pd.read_excel(\"../data/\"+kapsam_listeli[0], sheet_name=\"A101 Portföy\")\n",
    "sok_kapsam = pd.read_excel(\"../data/\"+kapsam_listeli[0], sheet_name=\"Şok Portföy\")\n",
    "bim_kapsam = pd.read_excel(\"../data/\"+kapsam_listeli[0], sheet_name=\"Bim Portföy\")\n",
    "\n",
    "a101_kapsam[\"grup_adi\"] = \"A101\"\n",
    "sok_kapsam[\"grup_adi\"] = \"ŞOK\"\n",
    "bim_kapsam[\"grup_adi\"] = \"BİM\"\n",
    "\n",
    "kapsam_all = pd.concat([a101_kapsam, sok_kapsam, bim_kapsam], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Enflasyon\n",
    "enflasyon_df = pd.read_html('https://www.tcmb.gov.tr/wps/wcm/connect/TR/TCMB+TR/Main+Menu/Istatistikler/Enflasyon+Verileri/Tuketici+Fiyatlari')[0]\n",
    "enflasyon_df.columns=[\"date\", \"enflasyon_etkisi\", \"degisim\"]\n",
    "enflasyon_df.drop(\"degisim\", axis=1, inplace=True)\n",
    "enflasyon_df[\"date\"] = enflasyon_df[\"date\"].apply(lambda x: \"01-\"+x)\n",
    "enflasyon_df[\"date\"] = pd.to_datetime(enflasyon_df[\"date\"], format=\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diğer dataların Jupytere yüklenme süresi: 0:00:55.646205\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Diğer dataların Jupytere yüklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifik 2016 aktivite verileri olmadığı için 2016 Sales dataları çıkartıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[pasifik_df_all[\"Yıl\"] != 2016].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[~((pasifik_df_all[\"Yıl\"] == 2021) & (pasifik_df_all[\"Ay\"].isin([6, 7, 8, 9])))].reset_index(drop=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all[~((horizon_saha_df_all[\"Yıl\"] == 2021) & (horizon_saha_df_all[\"Ay\"].isin([6, 7, 8, 9])))].reset_index(drop=True)\n",
    "btt_df_all = btt_df_all[~((btt_df_all[\"Yıl\"] == 2021) & (btt_df_all[\"Ay\"].isin([6, 7, 8, 9])))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all[\"Yıl\"] = horizon_saha_df_all[\"Yıl\"].astype(int)\n",
    "horizon_saha_df_all[\"Ay\"] = horizon_saha_df_all[\"Ay\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Datası İçin Ürün Kod Eşleme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kategori_adi = btt_df_all.drop_duplicates(subset=[\"Marka Adı\", \"Kategori Adı\"], keep=\"first\")[[\"Marka Adı\", \"Kategori Adı\"]]\n",
    "kategori_adi.sort_values(by=[\"Marka Adı\", \"Kategori Adı\"], ignore_index=True, inplace=True)\n",
    "kategori_adi = dict(kategori_adi.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all[\"Kategori Adı\"] = horizon_saha_df_all[\"Kategori Adı\"].fillna(horizon_saha_df_all[\"Marka Adı\"].map(kategori_adi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pd.merge(pasifik_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "horizon_saha_df_all = pd.merge(horizon_saha_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "btt_df_all = pd.merge(btt_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ürün Eşleme Kodları dosyasında yer almayan kodlar için mevcut ürün kodları verildi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_ = pasifik_df_all[pd.isnull(pasifik_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = pasifik_df_all[~pd.isnull(pasifik_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "pasifik_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "pasifik_df_all = pasifik_df_all.sort_values(pasifik_df_all.columns.to_list()).reset_index(drop=True)\n",
    "\n",
    "empty_ = btt_df_all[pd.isnull(btt_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = btt_df_all[~pd.isnull(btt_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "btt_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "btt_df_all = btt_df_all.sort_values(btt_df_all.columns.to_list()).reset_index(drop=True)\n",
    "\n",
    "empty_ = horizon_saha_df_all[pd.isnull(horizon_saha_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = horizon_saha_df_all[~pd.isnull(horizon_saha_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "horizon_saha_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all.sort_values(horizon_saha_df_all.columns.to_list()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adet adında yeni bir kolon oluşturuldu. Koli Sayısı 100'den az olanlara 0 yazıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all[\"Adet\"] = pasifik_df_all[\"Koli İçi Adet\"] * pasifik_df_all[\"Koli\"]\n",
    "btt_df_all[\"Adet\"] = btt_df_all[\"Koli İçi Adet\"] * btt_df_all[\"Koli\"]\n",
    "horizon_saha_df_all[\"Adet\"] = horizon_saha_df_all[\"Koli İçi Adet\"] * horizon_saha_df_all[\"Koli\"]\n",
    "\n",
    "pasifik_df_all[\"Adet\"] = pasifik_df_all[\"Adet\"] = np.where(pasifik_df_all[\"Koli\"] < 100, 1, pasifik_df_all[\"Adet\"])\n",
    "btt_df_all[\"Adet\"] = btt_df_all[\"Adet\"] = np.where(btt_df_all[\"Koli\"] < 100, 1, btt_df_all[\"Adet\"])\n",
    "horizon_saha_df_all[\"Adet\"] = horizon_saha_df_all[\"Adet\"] = np.where(horizon_saha_df_all[\"Koli\"] < 100, 1, horizon_saha_df_all[\"Adet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delist olan ürünler veriden çıkartıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[pasifik_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "btt_df_all = btt_df_all[btt_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all[horizon_saha_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aynı yıl, ay, grup adı, ana kategori adı, kategori adı, marka adı ve SKU kodundaki ürünler için toplam alındı. Sadece Koli İçi Adet için maksimum olan alındı."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marka adı dahil değil groupby'a\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_to_sum = {\"Koli İçi Adet\": \"sum\", \"Koli\": \"sum\", \"KG\": \"sum\", \"TL\": \"sum\", \"Adet\": \"sum\"}\n",
    "\n",
    "pasifik_df_all2 = pasifik_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()\n",
    "btt_df_all2 = btt_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()\n",
    "horizon_saha_df_all2 = horizon_saha_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2[\"Date\"] = pasifik_df_all2[\"Yıl\"].astype(str) + \"-\" +  pasifik_df_all2[\"Ay\"].astype(str) + \"-01\"\n",
    "btt_df_all2[\"Date\"] = btt_df_all2[\"Yıl\"].astype(int).astype(str) + \"-\" +  btt_df_all2[\"Ay\"].astype(int).astype(str) + \"-01\"\n",
    "horizon_saha_df_all2[\"Date\"] = horizon_saha_df_all2[\"Yıl\"].astype(int).astype(str) + \"-\" +  horizon_saha_df_all2[\"Ay\"].astype(int).astype(str) + \"-01\"\n",
    "\n",
    "pasifik_df_all2[\"Date\"] = pd.to_datetime(pasifik_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "btt_df_all2[\"Date\"] = pd.to_datetime(btt_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "horizon_saha_df_all2[\"Date\"] = pd.to_datetime(horizon_saha_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon ve Pasifikte bulunan \"Diğer\"'lerin yanlarına \"_\" ile Diğer_Pasifik, Diğer_Horizon yazıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2[\"Grup Adı\"] = pasifik_df_all2[\"Grup Adı\"].apply(lambda x: \"Diğer_Pasifik\" if x == \"Diğer\" else x)\n",
    "horizon_saha_df_all2[\"Grup Adı\"] = horizon_saha_df_all2[\"Grup Adı\"].apply(lambda x: \"Diğer_Horizon\" if x == \"Diğer\" else x)\n",
    "df_all2 = pd.concat([pasifik_df_all2, horizon_saha_df_all2, btt_df_all2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifik Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_urun_isimleri = pasifik_df_all[[\"Marka Adı\", \"Ürün Adı\", \"En Güncel Kod\"]].drop_duplicates(subset=[\"Marka Adı\", \"En Güncel Kod\"],ignore_index=True,keep=\"first\")\n",
    "pasifik_urun_isimleri = pasifik_urun_isimleri[~((pasifik_urun_isimleri[\"Marka Adı\"] == \"DANKEK BATON\") & (pasifik_urun_isimleri[\"En Güncel Kod\"] == 80403))]\n",
    "pasifik_df_all2 = pd.merge(pasifik_df_all2, pasifik_urun_isimleri[[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"]].drop_duplicates(subset=[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"], keep=\"first\"), on=\"En Güncel Kod\", how=\"left\")\n",
    "pasifik_df_all2 = pasifik_df_all2[pasifik_df_all2.columns.to_list()[:5]+pasifik_df_all2.columns.to_list()[-2:]+[pasifik_df_all2.columns.to_list()[5]]+pasifik_df_all2.columns.to_list()[6:12]]\n",
    "\n",
    "\n",
    "\n",
    "horizon_urun_isimleri = horizon_saha_df_all[[\"Marka Adı\", \"Ürün Adı\", \"En Güncel Kod\"]].drop_duplicates(subset=[\"Marka Adı\", \"En Güncel Kod\"],ignore_index=True,keep=\"first\")\n",
    "horizon_urun_isimleri = horizon_urun_isimleri[~(((horizon_urun_isimleri[\"Marka Adı\"] == \"DANKEK BATON\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 80403)) | \n",
    "                                                ((horizon_urun_isimleri[\"Marka Adı\"] == \"MAVİ YEŞİL\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 11802)) |\n",
    "                                                ((horizon_urun_isimleri[\"Marka Adı\"] == \"MAVİ YEŞİL\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 74306)) |\n",
    "                                                ((horizon_urun_isimleri[\"Marka Adı\"] == \"AS KRAKER\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 190502)))]\n",
    "horizon_saha_df_all2 = pd.merge(horizon_saha_df_all2, horizon_urun_isimleri[[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "horizon_saha_df_all2 = horizon_saha_df_all2[horizon_saha_df_all2.columns.to_list()[:5]+horizon_saha_df_all2.columns.to_list()[-2:]+[horizon_saha_df_all2.columns.to_list()[5]]+horizon_saha_df_all2.columns.to_list()[6:12]]\n",
    "\n",
    "\n",
    "\n",
    "btt_urun_isimleri = btt_df_all[[\"Marka Adı\", \"Ürün Adı\", \"En Güncel Kod\"]].drop_duplicates(subset=[\"Marka Adı\", \"En Güncel Kod\"],ignore_index=True,keep=\"first\")\n",
    "btt_urun_isimleri = btt_urun_isimleri[~((btt_urun_isimleri[\"Marka Adı\"] == \"DANKEK BATON\") & (btt_urun_isimleri[\"En Güncel Kod\"] == 80403))]\n",
    "btt_df_all2 = pd.merge(btt_df_all2, btt_urun_isimleri[[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"]].drop_duplicates(subset=[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"], keep=\"first\"), on=\"En Güncel Kod\", how=\"left\")\n",
    "btt_df_all2 = btt_df_all2[btt_df_all2.columns.to_list()[:5]+btt_df_all2.columns.to_list()[-2:]+[btt_df_all2.columns.to_list()[5]]+btt_df_all2.columns.to_list()[6:12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all_filled = []\n",
    "for idx, test in pasifik_df_all2.groupby([\"En Güncel Kod\", \"Yıl\", \"Grup Adı\"]):\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, 13):\n",
    "        try:\n",
    "            if i == test.loc[i-1, \"Ay\"]:\n",
    "                if i == 13:\n",
    "                    break\n",
    "            else:\n",
    "                test.loc[-1] = test.loc[0]\n",
    "                test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "                test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "                test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        except:\n",
    "            test.loc[-1] = test.loc[0]\n",
    "            test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "            test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "            test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        test[\"Date\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    rows_to_drop = []\n",
    "    start = test.index[0]\n",
    "    length = 1\n",
    "    while (test.loc[start, \"Adet\"] == 1) and (length < len(test)):\n",
    "        rows_to_drop.append(start)\n",
    "        length+=1\n",
    "        start+=1\n",
    "    test.drop(index=rows_to_drop, inplace=True)\n",
    "    pasifik_df_all_filled.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2 = pd.concat(pasifik_df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik missing value düzenlenmesi süresi: 0:04:12.571834\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Horizon Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all_filled = []\n",
    "for idx, test in horizon_saha_df_all2.groupby([\"En Güncel Kod\", \"Yıl\", \"Grup Adı\"]):\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, 13):\n",
    "        try:\n",
    "            if i == test.loc[i-1, \"Ay\"]:\n",
    "                if i == 13:\n",
    "                    break\n",
    "            else:\n",
    "                test.loc[-1] = test.loc[0]\n",
    "                test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "                test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "                test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        except:\n",
    "            test.loc[-1] = test.loc[0]\n",
    "            test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "            test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "            test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        test[\"Date\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    \n",
    "    rows_to_drop = []\n",
    "    start = test.index[0]\n",
    "    length = 1\n",
    "    while (test.loc[start, \"Adet\"] == 1) and (length < len(test)):\n",
    "        rows_to_drop.append(start)\n",
    "        length+=1\n",
    "        start+=1\n",
    "    test.drop(index=rows_to_drop, inplace=True)\n",
    "    horizon_saha_df_all_filled.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all2 = pd.concat(horizon_saha_df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon missing value düzenlenmesi süresi: 0:08:40.862117\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# BTT Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_all_filled = []\n",
    "for idx, test in btt_df_all2.groupby([\"En Güncel Kod\", \"Yıl\", \"Grup Adı\"]):\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, 13):\n",
    "        try:\n",
    "            if i == test.loc[i-1, \"Ay\"]:\n",
    "                if i == 13:\n",
    "                    break\n",
    "            else:\n",
    "                test.loc[-1] = test.loc[0]\n",
    "                test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "                test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "                test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        except:\n",
    "            test.loc[-1] = test.loc[0]\n",
    "            test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "            test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "            test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        test[\"Date\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "    rows_to_drop = []\n",
    "    start = test.index[0]\n",
    "    length = 1\n",
    "    while (test.loc[start, \"Adet\"] == 1) and (length < len(test)):\n",
    "        rows_to_drop.append(start)\n",
    "        length+=1\n",
    "        start+=1\n",
    "    test.drop(index=rows_to_drop, inplace=True)\n",
    "    btt_df_all_filled.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_all2 = pd.concat(btt_df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTT missing value düzenlenmesi süresi: 0:00:48.470001\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_horizon_saha_df_all2['Yıl'] = new_horizon_saha_df_all2['Yıl'].astype(int)\n",
    "new_horizon_saha_df_all2['Ay'] = new_horizon_saha_df_all2['Ay'].astype(int)\n",
    "new_horizon_saha_df_all2['En Güncel Kod'] = new_horizon_saha_df_all2['En Güncel Kod'].astype(int)\n",
    "new_horizon_saha_df_all2['Date'] = new_horizon_saha_df_all2['Yıl'].astype(str) + '-' + new_horizon_saha_df_all2['Ay'].astype(str) + '-01'\n",
    "new_horizon_saha_df_all2[\"Date\"] = pd.to_datetime(new_horizon_saha_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all2 = pd.concat([new_pasifik_df_all2, new_horizon_saha_df_all2, btt_df_all2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aktivite Datası İçin Ürün Kod Eşleme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik Aktivite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left join ile güncel kodlar getirildi. Delist olan ürünler listeden çıkartıldı. \"Çeyrek\" sütunu silindi. En güncel kod sütunnuda bulunamayan değerler Ürün Kodu sütunundan çekildi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_aktivite_df2 = pd.merge(pasifik_aktivite_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "pasifik_aktivite_df2 = pasifik_aktivite_df2[pasifik_aktivite_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "pasifik_aktivite_df2.drop(\"Çeyrek\", inplace=True, axis=1)\n",
    "pasifik_aktivite_df2['En Güncel Kod'] = pasifik_aktivite_df2['En Güncel Kod'].fillna(pasifik_aktivite_df2['Ürün Kodu'])\n",
    "pasifik_aktivite_df2.drop(columns=\"Ürün Kodu\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik Aktivite Ciro - Promosyon Tutarı ve İskonto Tekilleştirme (ORTALAMA ALARAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_pas = {\"Raf Tavsiye Satış Fiyatı\": \"mean\", \"İndirimli Raf Satış Fiyatı\": \"mean\", \"İndirim %\": \"mean\", \"Aktivite Tipi\": \"first\"}\n",
    "pasifik_aktivite_df3 = pasifik_aktivite_df2.groupby([\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Müşteri Grup\"]).agg(ort_pas).reset_index()\n",
    "pasifik_aktivite_df3 = pd.merge(pasifik_aktivite_df3, pasifik_aktivite_df2[[\"Yıl\", \"Ay\", \"Müşteri Grup\", \"En Güncel Kod\", \n",
    "                                                                            \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\"]],\n",
    "                                how=\"left\", \n",
    "                                on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Müşteri Grup\"])\n",
    "\n",
    "pasifik_aktivite_df3.drop_duplicates(subset=pasifik_aktivite_df3.columns.to_list(), inplace=True)\n",
    "pasifik_aktivite_df3.reset_index(drop=True, inplace=True)\n",
    "pasifik_aktivite_df3 = pasifik_aktivite_df3[pasifik_aktivite_df2.drop(\"Ürün Adı\", axis=1).columns.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon Aktivite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay2 = pd.merge(saha_aktivite_detay, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "saha_aktivite_detay2 = saha_aktivite_detay2[saha_aktivite_detay2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "saha_aktivite_detay2.drop(\"Çeyrek\", inplace=True, axis=1)\n",
    "saha_aktivite_detay2['En Güncel Kod'] = saha_aktivite_detay2['En Güncel Kod'].fillna(saha_aktivite_detay2['Ürün Kodu'])\n",
    "saha_aktivite_detay2.drop(columns=\"Ürün Kodu\", axis=1, inplace=True)\n",
    "saha_aktivite_detay2[\"İskonto %\"].replace(\"#DIV/0\", np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon Aktivite Ciro - Promosyon Tutarı ve İskonto Tekilleştirme (ORTALAMA ALARAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort = {\"Ciro (Kull. İade Düş.)\": \"mean\", \"Promosyon Tutarı\": \"mean\", \"İskonto %\": \"mean\"}\n",
    "saha_aktivite_detay3 = saha_aktivite_detay2.groupby([\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Saha Müşteri Grup\"]).agg(ort).reset_index()\n",
    "\n",
    "saha_aktivite_detay3 = pd.merge(saha_aktivite_detay3, saha_aktivite_detay2[[\"Yıl\", \"Ay\", \"Saha Müşteri Grup\", \"En Güncel Kod\", \n",
    "                                                 \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\"]],\n",
    "                           how=\"left\", \n",
    "                           on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Saha Müşteri Grup\"])\n",
    "\n",
    "saha_aktivite_detay3.drop_duplicates(subset=saha_aktivite_detay3.columns.to_list(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3 = saha_aktivite_detay3[saha_aktivite_detay2.drop(\"Ürün Adı (Mobis)\", axis=1).columns.to_list()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3.rename(columns={\"Saha Müşteri Grup\": \"Grup Adı\"}, inplace=True)\n",
    "saha_aktivite_detay3[\"Grup Adı\"] = saha_aktivite_detay3[\"Grup Adı\"].apply(lambda x: \"Diğer_Horizon\" if x == \"Diğer\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiyat Listesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Fiyatları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon.drop_duplicates(subset=fiyat_lst_horizon.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].str.replace(\"-\", \"\")\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].astype(int)*1\n",
    "except:\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].astype(int)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df = fiyat_lst_horizon.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df[\"Baslangic_Yıl\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.year)\n",
    "fiyat_lst_horizon_df[\"Baslangic_Ay\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.month)\n",
    "fiyat_lst_horizon_df[\"Baslangic_Gun\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.day)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.year)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Ay\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.month)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Gun\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df[\"Baslangic_Yıl\"] = fiyat_lst_horizon_df[\"Baslangic_Yıl\"].apply(lambda x: (horizon_saha_df_all2[\"Date\"].max().year)+1 if x > horizon_saha_df_all2[\"Date\"].max().year else x)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"] = fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"].apply(lambda x: (horizon_saha_df_all2[\"Date\"].max().year)+1 if x > horizon_saha_df_all2[\"Date\"].max().year else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = horizon_saha_df_all2[\"Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fiyat_unique = []\n",
    "\n",
    "for malzeme in fiyat_lst_horizon_df[\"Malzeme\"].unique():\n",
    "    temp_time_df = pd.DataFrame({\"Fiyat\": [np.nan]}, index=time_index)\n",
    "    temp_time_df = temp_time_df.reset_index().rename(columns={\"index\":\"date\"})    \n",
    "    temp_time_df[\"En Güncel Kod\"] = malzeme\n",
    "    temp_time_df[\"fiyat_gecisi\"] = 0\n",
    "    malzeme_df = fiyat_lst_horizon_df[fiyat_lst_horizon_df[\"Malzeme\"] == malzeme].reset_index(drop=True)\n",
    "    malzeme_df.drop(columns=[\"KşTü\", \"Koşul türü\", \"Tanım\", \"Ana Kategori\", \"Kategori\", \"ÖB\"], axis=1, inplace=True)\n",
    "    malzeme_df.drop_duplicates(subset=malzeme_df.columns.to_list(), inplace=True, ignore_index=True)\n",
    "    malzeme_df.sort_values(by=[\"Baslangic_Yıl\", \"Baslangic_Ay\", \"Baslangic_Gun\"], ignore_index=True, inplace=True)\n",
    "    check_idx1 = []\n",
    "    if len(malzeme_df) > 1:\n",
    "        for row1 in malzeme_df.index:\n",
    "            for row2 in malzeme_df[row1+1:].index:\n",
    "                if (malzeme_df.loc[row1][\"Gecerlilik_Yıl\"] == malzeme_df.loc[row2][\"Baslangic_Yıl\"]) and (malzeme_df.loc[row1][\"Gecerlilik_Ay\"] == malzeme_df.loc[row2][\"Baslangic_Ay\"]):\n",
    "                    num_days = calendar.monthrange(int(malzeme_df.loc[row2][\"Baslangic_Yıl\"]), int(malzeme_df.loc[row2][\"Baslangic_Ay\"]))[1]\n",
    "                    fyt=((int(malzeme_df.loc[row1][\"Gecerlilik_Gun\"])*malzeme_df.loc[row1][\"     Tutar\"]) + (num_days - int(malzeme_df.loc[row2][\"Baslangic_Gun\"]) + 1)*malzeme_df.loc[row2][\"     Tutar\"])/num_days\n",
    "\n",
    "                    end_idx1 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx1 = temp_time_df[temp_time_df[\"date\"] == end_idx1].index\n",
    "                    temp_time_df.loc[final_idx1, \"Fiyat\"] = fyt\n",
    "                    temp_time_df.loc[final_idx1,\"fiyat_gecisi\"] = 1\n",
    "\n",
    "                elif (malzeme_df.loc[row1, \"Gecerlilik_Gun\"] == calendar.monthrange(int(malzeme_df.loc[row1][\"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1][\"Gecerlilik_Ay\"]))[1] \\\n",
    "                     and malzeme_df.loc[row2, \"Baslangic_Gun\"] == 1):\n",
    "                    fyt5=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                    fyt6=malzeme_df.loc[row2][\"     Tutar\"]\n",
    "                    end_idx5 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    end_idx6 =  datetime(int(malzeme_df.loc[row2, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row2, \"Baslangic_Ay\"]), 1)\n",
    "                    final_idx5 = temp_time_df[temp_time_df[\"date\"] == end_idx5].index\n",
    "                    final_idx6 = temp_time_df[temp_time_df[\"date\"] == end_idx6].index\n",
    "                    temp_time_df.loc[final_idx5, \"Fiyat\"] = fyt5\n",
    "                    temp_time_df.loc[final_idx6, \"Fiyat\"] = fyt6\n",
    "\n",
    "                else:\n",
    "                    fyt2=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                    start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx2 = temp_time_df[(temp_time_df[\"date\"] > start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                    temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "            if (row1 == len(malzeme_df)-1) or (row1 == len(malzeme_df)-2):\n",
    "                fyt3=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                final_idx3 = temp_time_df[(temp_time_df[\"date\"] > start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "\n",
    "    else:\n",
    "        for row1 in malzeme_df.index:\n",
    "            fyt4=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "            start_idx4 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "            end_idx4 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "            final_idx4 = temp_time_df[(temp_time_df[\"date\"] >= start_idx4) & (temp_time_df[\"date\"] <= end_idx4)].index\n",
    "            temp_time_df.loc[final_idx4, \"Fiyat\"] = fyt4\n",
    "\n",
    "    if (malzeme_df.loc[0, \"Baslangic_Yıl\"] >= min(horizon_saha_df_all2[\"Yıl\"].unique())) and (len(malzeme_df) > 1):\n",
    "        temp_time_df.loc[temp_time_df[~pd.isnull(temp_time_df[\"Fiyat\"])].index[0]-1, \"Fiyat\"] = malzeme_df.loc[0, \"     Tutar\"]\n",
    "    temp_time_df = temp_time_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    h_fiyat_unique.append(temp_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fiyat_unique = pd.concat(h_fiyat_unique)\n",
    "h_fiyat_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "h_fiyat_unique.rename(columns={\"En Güncel Kod\": \"Ürün Kodu\", \"date\": \"Date\"}, inplace=True)\n",
    "h_fiyat_unique = h_fiyat_unique.merge(eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\")\n",
    "h_fiyat_unique[\"En Güncel Kod\"].fillna(h_fiyat_unique[\"Ürün Kodu\"], inplace=True)\n",
    "h_fiyat_unique = h_fiyat_unique[h_fiyat_unique[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "h_fiyat_unique = h_fiyat_unique.sort_values(by=[\"En Güncel Kod\", \"Date\"]).reset_index(drop=True)\n",
    "h_fiyat_unique = h_fiyat_unique.drop(columns=\"Ürün Kodu\", axis=1)\n",
    "# Aynı aya denk gelen ürünlerin fiyatlarının ortalaması alınıp, herhangi birinde fiyat geçişi varsa 1 alınır.\n",
    "h_fiyat_unique = h_fiyat_unique.groupby([\"Date\", \"En Güncel Kod\"]).agg({\"Fiyat\": \"mean\", \"fiyat_gecisi\": \"max\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon fiyatların düzenlenmesi süresi: 0:01:22.726738\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon fiyatların düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Fiyatları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik.drop_duplicates(subset=fiyat_lst_pasifik.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik[\"Malzeme\"] = fiyat_lst_pasifik[\"Malzeme\"].str.replace(\"-\", \"\")\n",
    "fiyat_lst_pasifik[\"Malzeme\"] = fiyat_lst_pasifik[\"Malzeme\"].astype(int)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df = fiyat_lst_pasifik.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df[\"Baslangic_Yıl\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.year)\n",
    "fiyat_lst_pasifik_df[\"Baslangic_Ay\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.month)\n",
    "fiyat_lst_pasifik_df[\"Baslangic_Gun\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.day)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.year)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Ay\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.month)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Gun\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df[\"Baslangic_Yıl\"] = fiyat_lst_pasifik_df[\"Baslangic_Yıl\"].apply(lambda x: (pasifik_df_all2[\"Date\"].max().year)+1 if x > pasifik_df_all2[\"Date\"].max().year else x)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"] = fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"].apply(lambda x: (pasifik_df_all2[\"Date\"].max().year)+1 if x > pasifik_df_all2[\"Date\"].max().year else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = pasifik_df_all2[\"Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique = []\n",
    "\n",
    "for malzeme in fiyat_lst_pasifik_df[\"Malzeme\"].unique():\n",
    "    temp_time_df = pd.DataFrame({\"Fiyat\": [np.nan]}, index=time_index)\n",
    "    temp_time_df = temp_time_df.reset_index().rename(columns={\"index\":\"date\"})    \n",
    "    temp_time_df[\"En Güncel Kod\"] = malzeme\n",
    "    temp_time_df[\"fiyat_gecisi\"] = 0\n",
    "    malzeme_df = fiyat_lst_pasifik_df[fiyat_lst_pasifik_df[\"Malzeme\"] == malzeme].reset_index(drop=True)\n",
    "    malzeme_df.drop(columns=[\"KşTü\", \"KşTü.1\", \"Malzeme Tanım\", \"Ana Kategori\", \"Kategori\"], axis=1, inplace=True)\n",
    "    malzeme_df.drop_duplicates(subset=malzeme_df.columns.to_list(), inplace=True, ignore_index=True)\n",
    "    malzeme_df.sort_values(by=[\"Baslangic_Yıl\", \"Baslangic_Ay\", \"Baslangic_Gun\"], ignore_index=True, inplace=True)\n",
    "    check_idx1 = []\n",
    "    if len(malzeme_df) > 1:\n",
    "        for row1 in malzeme_df.index:\n",
    "            for row2 in malzeme_df[row1+1:].index:\n",
    "                if (malzeme_df.loc[row1][\"Gecerlilik_Yıl\"] == malzeme_df.loc[row2][\"Baslangic_Yıl\"]) and (malzeme_df.loc[row1][\"Gecerlilik_Ay\"] == malzeme_df.loc[row2][\"Baslangic_Ay\"]):\n",
    "                    num_days = calendar.monthrange(int(malzeme_df.loc[row2][\"Baslangic_Yıl\"]), int(malzeme_df.loc[row2][\"Baslangic_Ay\"]))[1]\n",
    "                    fyt=((int(malzeme_df.loc[row1][\"Gecerlilik_Gun\"])*malzeme_df.loc[row1][\"Koli TL\"]) + (num_days - int(malzeme_df.loc[row2][\"Baslangic_Gun\"])+1)*malzeme_df.loc[row2][\"Koli TL\"])/num_days\n",
    "\n",
    "                    end_idx1 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx1 = temp_time_df[temp_time_df[\"date\"] == end_idx1].index\n",
    "                    temp_time_df.loc[final_idx1, \"Fiyat\"] = fyt\n",
    "                    temp_time_df.loc[final_idx1,\"fiyat_gecisi\"] = 1\n",
    "\n",
    "                elif (malzeme_df.loc[row1, \"Gecerlilik_Gun\"] == calendar.monthrange(int(malzeme_df.loc[row1][\"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1][\"Gecerlilik_Ay\"]))[1] \\\n",
    "                     and malzeme_df.loc[row2, \"Baslangic_Gun\"] == 1):\n",
    "                    fyt5=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    fyt6=malzeme_df.loc[row2][\"Koli TL\"]\n",
    "                    end_idx5 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    end_idx6 =  datetime(int(malzeme_df.loc[row2, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row2, \"Baslangic_Ay\"]), 1)\n",
    "                    final_idx5 = temp_time_df[temp_time_df[\"date\"] == end_idx5].index\n",
    "                    final_idx6 = temp_time_df[temp_time_df[\"date\"] == end_idx6].index\n",
    "                    temp_time_df.loc[final_idx5, \"Fiyat\"] = fyt5\n",
    "                    temp_time_df.loc[final_idx6, \"Fiyat\"] = fyt6\n",
    "\n",
    "\n",
    "                else:\n",
    "                    if malzeme_df.loc[row1, \"Baslangic_Gun\"] != 1:\n",
    "                        fyt2=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                        start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                        end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                        final_idx2 = temp_time_df[(temp_time_df[\"date\"] > start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                        temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "                    else:\n",
    "                        fyt2=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                        start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                        end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                        final_idx2 = temp_time_df[(temp_time_df[\"date\"] >= start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                        temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "                        \n",
    "            if (row1 == len(malzeme_df)-1) or (row1 == len(malzeme_df)-2):\n",
    "                if malzeme_df.loc[row1, \"Baslangic_Gun\"] != 1:\n",
    "                    fyt3=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx3 = temp_time_df[(temp_time_df[\"date\"] > start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                    temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "                else:\n",
    "                    fyt3=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx3 = temp_time_df[(temp_time_df[\"date\"] >= start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                    temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "    else:\n",
    "        for row1 in malzeme_df.index:\n",
    "            fyt4=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "            start_idx4 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "            end_idx4 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "            final_idx4 = temp_time_df[(temp_time_df[\"date\"] >= start_idx4) & (temp_time_df[\"date\"] <= end_idx4)].index\n",
    "            temp_time_df.loc[final_idx4, \"Fiyat\"] = fyt4\n",
    "    \n",
    "    if (malzeme_df.loc[0, \"Baslangic_Yıl\"] >= min(pasifik_df_all2[\"Yıl\"].unique())) and (len(malzeme_df) > 1):\n",
    "        temp_time_df.loc[temp_time_df[~pd.isnull(temp_time_df[\"Fiyat\"])].index[0]-1, \"Fiyat\"] = malzeme_df.loc[0, \"Koli TL\"]\n",
    "    temp_time_df = temp_time_df.dropna().reset_index(drop=True)\n",
    "        \n",
    "    p_fiyat_unique.append(temp_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique = pd.concat(p_fiyat_unique)\n",
    "p_fiyat_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "p_fiyat_unique.rename(columns={\"En Güncel Kod\": \"Ürün Kodu\", \"date\": \"Date\"}, inplace=True)\n",
    "p_fiyat_unique = p_fiyat_unique.merge(eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\")\n",
    "p_fiyat_unique[\"En Güncel Kod\"].fillna(p_fiyat_unique[\"Ürün Kodu\"], inplace=True)\n",
    "p_fiyat_unique = p_fiyat_unique[p_fiyat_unique[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "p_fiyat_unique = p_fiyat_unique.sort_values(by=[\"En Güncel Kod\", \"Date\"]).reset_index(drop=True)\n",
    "p_fiyat_unique = p_fiyat_unique.drop(columns=\"Ürün Kodu\", axis=1)\n",
    "# Aynı aya denk gelen ürünlerin fiyatlarının ortalaması alınıp, herhangi birinde fiyat geçişi varsa 1 alınır.\n",
    "p_fiyat_unique = p_fiyat_unique.groupby([\"Date\", \"En Güncel Kod\"]).agg({\"Fiyat\": \"mean\", \"fiyat_gecisi\": \"max\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik fiyatların düzenlenmesi süresi: 0:01:29.364957\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik fiyatların düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 0'ları doldururken en son yıl ve ayın ötesi de 0 ile dolduruyor. (Örn: Sales datasında 2021'in 3. ayına kadar data olduğu durumda 0'lar ile doldururken 2021 12. aya kadar 0 atıyor.\n",
    "# Bu durumun önüne geçmek için aşağıdaki işlemler yapılmaktadır.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_pas = pasifik_df_all2.copy()\n",
    "backup_btt = btt_df_all2.copy()\n",
    "backup_hor = horizon_saha_df_all2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_max_year = pasifik_df_all[\"Yıl\"].max()\n",
    "pas_max_month = pasifik_df_all[pasifik_df_all[\"Yıl\"] == pasifik_df_all[\"Yıl\"].max()][\"Ay\"].max()\n",
    "hor_max_year = horizon_saha_df_all[\"Yıl\"].max()\n",
    "hor_max_month = horizon_saha_df_all[horizon_saha_df_all[\"Yıl\"] == horizon_saha_df_all[\"Yıl\"].max()][\"Ay\"].max()\n",
    "btt_max_year = btt_df_all[\"Yıl\"].max()\n",
    "btt_max_month = btt_df_all[btt_df_all[\"Yıl\"] == btt_df_all[\"Yıl\"].max()][\"Ay\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup_1 = pasifik_df_all2.copy()\n",
    "hor_backup_1 = horizon_saha_df_all2.copy()\n",
    "btt_backup_1 = btt_df_all2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2 = pasifik_df_all2[~((pasifik_df_all2[\"Yıl\"] == pas_max_year) & (pasifik_df_all2[\"Ay\"] > pas_max_month))].reset_index(drop=True)\n",
    "horizon_saha_df_all2 = horizon_saha_df_all2[~((horizon_saha_df_all2[\"Yıl\"] == hor_max_year) & \n",
    "                                            (horizon_saha_df_all2[\"Ay\"] > hor_max_month))].reset_index(drop=True)\n",
    "btt_df_all2 = btt_df_all2[~((btt_df_all2[\"Yıl\"] == btt_max_year) & (btt_df_all2[\"Ay\"] > btt_max_month))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portföy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_portfoy_df2 = pd.merge(pasifik_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "pasifik_portfoy_df2[\"En Güncel Kod\"] = pasifik_portfoy_df2[\"En Güncel Kod\"].fillna(pasifik_portfoy_df2[\"Kod\"])\n",
    "pasifik_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "pasifik_portfoy_df2 = pasifik_portfoy_df2[pasifik_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "pasifik_portfoy_df2[\"Portfoy\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_portfoy_df2 = pd.merge(horizon_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "horizon_portfoy_df2[\"En Güncel Kod\"] = horizon_portfoy_df2[\"En Güncel Kod\"].fillna(horizon_portfoy_df2[\"Kod\"])\n",
    "horizon_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "horizon_portfoy_df2 = horizon_portfoy_df2[horizon_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "horizon_portfoy_df2[\"Portfoy\"] = 1\n",
    "horizon_portfoy_df2 = horizon_portfoy_df2[~((horizon_portfoy_df2[\"Kod\"] == 135901))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTT Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_portfoy_df2 = pd.merge(btt_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "btt_portfoy_df2[\"En Güncel Kod\"] = btt_portfoy_df2[\"En Güncel Kod\"].fillna(btt_portfoy_df2[\"Kod\"])\n",
    "btt_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "btt_portfoy_df2 = btt_portfoy_df2[btt_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "btt_portfoy_df2[\"Portfoy\"] = 1\n",
    "btt_portfoy_df2 = btt_portfoy_df2[~((btt_portfoy_df2[\"Kod\"] == 135901))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portföy Kapsamındaki Sales Dataları"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pd.merge(pasifik_df_all2,pasifik_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "btt_df_all3 = pd.merge(btt_df_all2,btt_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "horizon_saha_df_all3 = pd.merge(horizon_saha_df_all2,horizon_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "pasifik_df_all3[\"Portfoy\"].fillna(0, inplace=True)\n",
    "btt_df_all3[\"Portfoy\"].fillna(0, inplace=True)\n",
    "horizon_saha_df_all3[\"Portfoy\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datalara Calender Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Jan 2016\n",
       "1     Feb 2016\n",
       "2     Mar 2016\n",
       "3     Apr 2016\n",
       "4     May 2016\n",
       "        ...   \n",
       "91    Aug 2023\n",
       "92    Sep 2023\n",
       "93    Oct 2023\n",
       "94    Nov 2023\n",
       "95    Dec 2023\n",
       "Name: DATE, Length: 96, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calender_df.pop(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pd.merge(pasifik_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")\n",
    "btt_df_all3 = pd.merge(btt_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")\n",
    "horizon_saha_df_all3 = pd.merge(horizon_saha_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataların Fiyat Ve Aktiviteler İle Birleştirilmesi\n",
    "---\n",
    "\n",
    "# Not:\n",
    "---\n",
    "### 1) BTT aktivite verisi için Horizon kısmındaki \"Geleneksel Kanal\" kullanılması istendi.\n",
    "### 2) BTT fiyat geçişleri için Horizon fiyat geçişleri baz alındı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pasifik_df_all3.merge(p_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "pasifik_aktivite_df3.rename(columns={\"Müşteri Grup\": \"Grup Adı\", \"Grup adı\": \"Grup Adı\"}, inplace=True)\n",
    "pasifik_df_all3 = pd.merge(pasifik_df_all3, pasifik_aktivite_df3[[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\", \n",
    "                                                                  \"Raf Tavsiye Satış Fiyatı\", \"İndirimli Raf Satış Fiyatı\", \"İndirim %\",\n",
    "                                                                  \"Aktivite Tipi\"]], \n",
    "                           left_on=[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\"], \n",
    "                           right_on=[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3.rename(columns={\"Grup adı\": \"Grup Adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(h_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(saha_aktivite_detay3[['Ciro (Kull. İade Düş.)', 'Promosyon Tutarı', \n",
    "                                                                        'İskonto %', 'En Güncel Kod', \"Yıl\", \"Ay\", \"Grup Adı\"]],\n",
    "                                                  on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Grup Adı\"], how=\"left\")\n",
    "btt_df_all3 = btt_df_all3.merge(h_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "btt_aktivite = saha_aktivite_detay3[saha_aktivite_detay3[\"Grup Adı\"] == \"GELENEKSEL KANAL\"].reset_index(drop=True)\n",
    "\n",
    "btt_df_all3 = btt_df_all3.merge(btt_aktivite[['Ciro (Kull. İade Düş.)', 'Promosyon Tutarı', \n",
    "                                              'İskonto %', 'En Güncel Kod', \"Yıl\", \"Ay\"]],\n",
    "                                on=[\"En Güncel Kod\", \"Yıl\", \"Ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sütun İsimlerini İngilizce Karaktere Çevirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_col_name(dff_):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    dff_: dataframe\n",
    "    Sütun ismini değiştirmek istediğiniz dataframe'i yazınız.\n",
    "    \n",
    "    Returns: Liste\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chng_letters = list(zip([\"ç\", \"ğ\", \"ı\", \"ö\", \"ş\", \"ü\", \" \", \"%\", \".\", \"(\", \")\", \"-\"], \n",
    "                            [\"c\", \"g\", \"i\", \"o\", \"s\", \"u\", \"_\", \"\", \"\", \"\", \"\", \"_\"]))\n",
    "    new_cols = []\n",
    "    for col in dff_.columns.str.lower():\n",
    "        for letter in range(len(chng_letters)):\n",
    "            col = col.replace(chng_letters[letter][0], chng_letters[letter][1])\n",
    "            if letter == len(chng_letters) - 1:\n",
    "                new_cols.append(col)\n",
    "            else:\n",
    "                pass\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3.columns = change_col_name(pasifik_df_all3)\n",
    "horizon_saha_df_all3.columns = change_col_name(horizon_saha_df_all3)\n",
    "btt_df_all3.columns = change_col_name(btt_df_all3)\n",
    "enflasyon_df.columns = change_col_name(enflasyon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifikte Aktivite Tipi Verisi Eksik Olan Verilere \"Yok\" yazıldı\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3[\"aktivite_tipi\"].fillna(\"Yok\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Enflasyon Verilerinin Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pasifik_df_all3.merge(enflasyon_df, on=[\"date\"], how=\"left\")\n",
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(enflasyon_df, on=[\"date\"], how=\"left\")\n",
    "btt_df_all3 = btt_df_all3.merge(enflasyon_df, on=[\"date\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df_all3.copy()\n",
    "horizon_saha_df_sorted = horizon_saha_df_all3.copy()\n",
    "btt_df_sorted = btt_df_all3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Satış Olmayan Aylar Flaglendi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"satis_var\"] = [0 if adet <= 1 else 1 for adet in pasifik_df_sorted[\"adet\"]]\n",
    "horizon_saha_df_sorted[\"satis_var\"] = [0 if adet <= 1 else 1 for adet in horizon_saha_df_sorted[\"adet\"]]\n",
    "btt_df_sorted[\"satis_var\"] = [0 if adet <= 1 else 1 for adet in btt_df_sorted[\"adet\"]]\n",
    "\n",
    "df_pasifik = pasifik_df_sorted.copy()\n",
    "df_btt = btt_df_sorted.copy()\n",
    "df_horizon = horizon_saha_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sütun isim uzunluğunun 32'yi geçmemesi için\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik.columns = [i[:32] if len(i) > 32 else i for i in df_pasifik.columns]\n",
    "df_horizon.columns = [i[:32] if len(i) > 32 else i for i in df_horizon.columns]\n",
    "df_btt.columns = [i[:32] if len(i) > 32 else i for i in df_btt.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sales datasındaki son yıl ve aya kadar ulaşmayan, yarıda kesilen verilerin, son tarihe kadar NaN ile doldurulması\n",
    "## Sales verisindeki en son tarih Haziran 2021 ise, herhangi bir SKU'nun son gözlemi Şubat 2019'da olsa dahi Haizran 2021'e kadar devam ettiriliyor\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_and_horizon_cols_to_drop = [\"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\",\n",
    "                                \"urun_adi\", \"en_guncel_kod\", \"portfoy\", \"koli_i̇ci_adet\", \"koli\", \"kg\", \"tl\", \"adet\", \n",
    "                                \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\",\n",
    "                                \"i̇skonto_\", \"satis_var\"]\n",
    "\n",
    "pasifik_cols_to_drop = [\"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\",\n",
    "                        \"urun_adi\", \"en_guncel_kod\", \"portfoy\", \"koli_i̇ci_adet\", \"koli\", \"kg\", \"tl\", \"adet\",\n",
    "                        \"fiyat\", \"fiyat_gecisi\", \"raf_tavsiye_satis_fiyati\", \"i̇ndirimli_raf_satis_fiyati\",\n",
    "                        \"aktivite_tipi\", \"i̇ndirim_\", \"satis_var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik = df_pasifik[df_pasifik[\"date\"] < datetime(2021, 6, 1)]\n",
    "df_horizon = df_horizon[df_horizon[\"date\"] < datetime(2021, 6, 1)]\n",
    "df_btt = df_btt[df_btt[\"date\"] < datetime(2021, 6, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_backup_for_null_date = df_horizon.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_template = df_pasifik.drop(columns=pasifik_cols_to_drop, axis=1)\n",
    "pasifik_template = pasifik_template.drop_duplicates(subset=pasifik_template.columns.to_list()).sort_values(by=[\"yil\", \"ay\"])\n",
    "\n",
    "btt_template = df_btt.drop(columns=btt_and_horizon_cols_to_drop, axis=1)\n",
    "btt_template = btt_template.drop_duplicates(subset=btt_template.columns.to_list()).sort_values(by=[\"yil\", \"ay\"])\n",
    "\n",
    "horizon_template = df_horizon.drop(columns=btt_and_horizon_cols_to_drop, axis=1)\n",
    "horizon_template = horizon_template.drop_duplicates(subset=horizon_template.columns.to_list()).sort_values(by=[\"yil\", \"ay\"])\n",
    "\n",
    "pas_date = pasifik_df_sorted.date.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "hor_date = horizon_saha_df_sorted.date.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "btt_date = btt_df_sorted.date.drop_duplicates().sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_sku_fill = []\n",
    "pasifik_grup_fill = []\n",
    "for sku in df_pasifik[\"en_guncel_kod\"].unique():\n",
    "    for grup in df_pasifik[\"grup_adi\"].unique():\n",
    "        temp = df_pasifik[(df_pasifik[\"en_guncel_kod\"] == sku) & (df_pasifik[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "        if temp[\"date\"].max() < pasifik_template[\"date\"].max():\n",
    "            pasifik_sku_fill.append(sku)\n",
    "            pasifik_grup_fill.append(grup)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        temp_date = pd.DataFrame(pas_date)[pd.DataFrame(pas_date)[\"date\"] >= temp[\"date\"].min()]\n",
    "        \n",
    "        if len(temp_date) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            pasifik_sku_fill.append(sku)\n",
    "            pasifik_grup_fill.append(grup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik missing imputation düzenlenmesi süresi: 0:00:24.192748\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing imputation düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_sku_fill = []\n",
    "btt_grup_fill = []\n",
    "for sku in df_btt[\"en_guncel_kod\"].unique():\n",
    "    for grup in df_btt[\"grup_adi\"].unique():\n",
    "        temp = df_btt[(df_btt[\"en_guncel_kod\"] == sku) & (df_btt[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "        if temp[\"date\"].max() < btt_template[\"date\"].max():\n",
    "            btt_sku_fill.append(sku)\n",
    "            btt_grup_fill.append(grup)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        temp_date = pd.DataFrame(btt_date)[pd.DataFrame(btt_date)[\"date\"] >= temp[\"date\"].min()]\n",
    "        \n",
    "        if len(temp_date) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            btt_sku_fill.append(sku)\n",
    "            btt_grup_fill.append(grup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTT missing imputation süresi: 0:00:01.798054\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing imputation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_sku_fill = []\n",
    "horizon_grup_fill = []\n",
    "for sku in df_horizon[\"en_guncel_kod\"].unique():\n",
    "    for grup in df_horizon[\"grup_adi\"].unique():\n",
    "        temp = df_horizon[(df_horizon[\"en_guncel_kod\"] == sku) & (df_horizon[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "        if temp[\"date\"].max() < horizon_template[\"date\"].max():\n",
    "            horizon_sku_fill.append(sku)\n",
    "            horizon_grup_fill.append(grup)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        temp_date = pd.DataFrame(hor_date)[pd.DataFrame(hor_date)[\"date\"] >= temp[\"date\"].min()]\n",
    "        \n",
    "        if len(temp_date) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            horizon_sku_fill.append(sku)\n",
    "            horizon_grup_fill.append(grup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon missing imputation düzenlenmesi süresi: 0:01:10.290103\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing imputation düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pasifik_to_append = []\n",
    "\n",
    "ffill_cols = ['grup_adi', 'ana_kategori_adi', 'kategori_adi',\n",
    "             'marka_adi', 'urun_adi', 'en_guncel_kod', \"portfoy\"]\n",
    "\n",
    "zero_fill_cols = ['koli_i̇ci_adet', 'koli', 'kg', 'tl', 'satis_var']\n",
    "\n",
    "one_fill_cols = [\"adet\"]\n",
    "\n",
    "for row in range(len(pasifik_sku_fill)):\n",
    "    temp = df_pasifik[(df_pasifik[\"en_guncel_kod\"] == pasifik_sku_fill[row]) & \n",
    "                      (df_pasifik[\"grup_adi\"] == pasifik_grup_fill[row])].reset_index(drop=True)\n",
    "    \n",
    "    pasifik_template2 = pasifik_template[pasifik_template[\"date\"] > temp.date.max()].reset_index(drop=True)    \n",
    "    df_to_append = temp.merge(pasifik_template2, how=\"outer\", on=list(set(temp).intersection(set(pasifik_template2))))\n",
    "    \n",
    "    temp_date = pd.DataFrame(pas_date)[pd.DataFrame(pas_date)[\"date\"] >= df_to_append[\"date\"].min()][\"date\"]\n",
    "    date_df = pd.DataFrame(list(set(temp_date) - set(df_to_append[\"date\"])), columns=[\"date\"])\n",
    "    \n",
    "    if len(date_df) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_to_append = df_to_append.merge(date_df, how=\"outer\", on=\"date\")\n",
    "        df_to_append[\"yil\"] = df_to_append[\"date\"].apply(lambda x: x.date().year)\n",
    "        df_to_append[\"ay\"] = df_to_append[\"date\"].apply(lambda x: x.date().month)\n",
    "        df_to_append = df_to_append.merge(pasifik_template2, how=\"outer\", on=list(set(df_to_append).intersection(set(pasifik_template2))))\n",
    "\n",
    "    for f in ffill_cols:\n",
    "        df_to_append[f].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    for z in zero_fill_cols:\n",
    "        df_to_append[z].fillna(0, inplace=True)\n",
    "    \n",
    "    for o in one_fill_cols:\n",
    "        df_to_append[o].fillna(1, inplace=True)\n",
    "    dfs_pasifik_to_append.append(df_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik missing append süresi: 0:01:17.045516\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing append süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_horizon_to_append = []\n",
    "\n",
    "ffill_cols = ['grup_adi', 'ana_kategori_adi', 'kategori_adi',\n",
    "             'marka_adi', 'urun_adi', 'en_guncel_kod', \"portfoy\"]\n",
    "\n",
    "zero_fill_cols = ['koli_i̇ci_adet', 'koli', 'kg', 'tl', 'satis_var']\n",
    "\n",
    "one_fill_cols = [\"adet\"]\n",
    "\n",
    "for row in range(len(horizon_sku_fill)):\n",
    "    temp = df_horizon[(df_horizon[\"en_guncel_kod\"] == horizon_sku_fill[row]) & \n",
    "                      (df_horizon[\"grup_adi\"] == horizon_grup_fill[row])].reset_index(drop=True)\n",
    "    \n",
    "    horizon_template2 = horizon_template[horizon_template[\"date\"] > temp.date.max()].reset_index(drop=True)\n",
    "    df_to_append = temp.merge(horizon_template2, how=\"outer\", on=list(set(temp).intersection(set(horizon_template2))))\n",
    "    \n",
    "    temp_date = pd.DataFrame(hor_date)[pd.DataFrame(hor_date)[\"date\"] >= df_to_append[\"date\"].min()][\"date\"]\n",
    "    date_df = pd.DataFrame(list(set(temp_date) - set(df_to_append[\"date\"])), columns=[\"date\"])\n",
    "    \n",
    "    if len(date_df) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_to_append = df_to_append.merge(date_df, how=\"outer\", on=\"date\")\n",
    "        df_to_append[\"yil\"] = df_to_append[\"date\"].apply(lambda x: x.date().year)\n",
    "        df_to_append[\"ay\"] = df_to_append[\"date\"].apply(lambda x: x.date().month)\n",
    "        df_to_append = df_to_append.merge(horizon_template2, how=\"outer\", on=list(set(df_to_append).intersection(set(horizon_template2))))\n",
    "\n",
    "    for f in ffill_cols:\n",
    "        df_to_append[f].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    for z in zero_fill_cols:\n",
    "        df_to_append[z].fillna(0, inplace=True)\n",
    "    \n",
    "    for o in one_fill_cols:\n",
    "        df_to_append[o].fillna(1, inplace=True)\n",
    "    dfs_horizon_to_append.append(df_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon missing append süresi: 0:03:40.238613\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing append süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_btt_to_append = []\n",
    "\n",
    "ffill_cols = ['grup_adi', 'ana_kategori_adi', 'kategori_adi',\n",
    "             'marka_adi', 'urun_adi', 'en_guncel_kod', \"portfoy\"]\n",
    "\n",
    "zero_fill_cols = ['koli_i̇ci_adet', 'koli', 'kg', 'tl', 'satis_var']\n",
    "\n",
    "one_fill_cols = [\"adet\"]\n",
    "\n",
    "for row in range(len(btt_sku_fill)):\n",
    "    temp = df_btt[(df_btt[\"en_guncel_kod\"] == btt_sku_fill[row]) & \n",
    "                  (df_btt[\"grup_adi\"] == btt_grup_fill[row])].reset_index(drop=True)\n",
    "    \n",
    "    btt_template2 = btt_template[btt_template[\"date\"] > temp.date.max()].reset_index(drop=True)\n",
    "\n",
    "    df_to_append = temp.merge(btt_template2, how=\"outer\", on=list(set(temp).intersection(set(btt_template2))))\n",
    "    \n",
    "    temp_date = pd.DataFrame(btt_date)[pd.DataFrame(btt_date)[\"date\"] >= df_to_append[\"date\"].min()][\"date\"]\n",
    "    date_df = pd.DataFrame(list(set(temp_date) - set(df_to_append[\"date\"])), columns=[\"date\"])\n",
    "    \n",
    "    if len(date_df) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_to_append = df_to_append.merge(date_df, how=\"outer\", on=\"date\")\n",
    "        df_to_append[\"yil\"] = df_to_append[\"date\"].apply(lambda x: x.date().year)\n",
    "        df_to_append[\"ay\"] = df_to_append[\"date\"].apply(lambda x: x.date().month)\n",
    "        df_to_append = df_to_append.merge(btt_template2, how=\"outer\", on=list(set(df_to_append).intersection(set(btt_template2))))\n",
    "\n",
    "    for f in ffill_cols:\n",
    "        df_to_append[f].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    for z in zero_fill_cols:\n",
    "        df_to_append[z].fillna(0, inplace=True)\n",
    "    \n",
    "    for o in one_fill_cols:\n",
    "        df_to_append[o].fillna(1, inplace=True)\n",
    "    dfs_btt_to_append.append(df_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTT missing append süresi: 0:00:10.206496\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing append süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik2 = df_pasifik.copy()\n",
    "df_horizon2 = df_horizon.copy()\n",
    "df_btt2 = df_btt.copy()\n",
    "\n",
    "dfs_pasifik_to_append = pd.concat(dfs_pasifik_to_append)\n",
    "dfs_horizon_to_append = pd.concat(dfs_horizon_to_append)\n",
    "dfs_btt_to_append = pd.concat(dfs_btt_to_append)\n",
    "\n",
    "df_pasifik3 = pd.concat([df_pasifik2, dfs_pasifik_to_append], axis=0, ignore_index=True)\n",
    "df_pasifik3.drop_duplicates(subset=df_pasifik3.columns.to_list(), ignore_index=True, inplace=True)\n",
    "\n",
    "df_horizon3 = pd.concat([df_horizon2, dfs_horizon_to_append], axis=0, ignore_index=True)\n",
    "df_horizon3.drop_duplicates(subset=df_horizon3.columns.to_list(), ignore_index=True, inplace=True)\n",
    "\n",
    "df_btt3 = pd.concat([df_btt2, dfs_btt_to_append], axis=0, ignore_index=True)\n",
    "df_btt3.drop_duplicates(subset=df_btt3.columns.to_list(), ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique.rename(columns={\"Date\": \"date\", \"En Güncel Kod\": \"en_guncel_kod\", \"Fiyat\": \"fiyat\"}, inplace=True)\n",
    "h_fiyat_unique.rename(columns={\"Date\": \"date\", \"En Güncel Kod\": \"en_guncel_kod\", \"Fiyat\": \"fiyat\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique[\"kanal\"] = \"pasifik\"\n",
    "h_fiyat_unique[\"kanal\"] = \"horizon\"\n",
    "fiyat_unique = pd.concat([p_fiyat_unique, h_fiyat_unique], axis=0, ignore_index=True)\n",
    "fiyat_unique.to_csv(\"../data/fiyat_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik4 = df_pasifik3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_pasifik4 = df_pasifik4.merge(p_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])\n",
    "\n",
    "df_horizon4 = df_horizon3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_horizon4 = df_horizon4.merge(h_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])\n",
    "\n",
    "df_btt4 = df_btt3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_btt4 = df_btt4.merge(h_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_aktivite_df3.columns = change_col_name(pasifik_aktivite_df3)\n",
    "saha_aktivite_detay3.columns = change_col_name(saha_aktivite_detay3)\n",
    "pasifik_aktivite_df3.rename(columns={\"i̇ndirim_\": \"indirim_\"}, inplace=True)\n",
    "saha_aktivite_detay3.rename(columns={\"i̇i̇skonto_\": \"iskonto_\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik4 = df_pasifik4.drop(columns=['raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', \n",
    "                                        'i̇ndirim_', 'aktivite_tipi'], axis=1)\n",
    "\n",
    "df_horizon4 = df_horizon4.drop(columns=['ciro_kull_i̇ade_dus', 'promosyon_tutari', 'i̇skonto_'], axis=1)\n",
    "df_btt4 = df_btt4.drop(columns=['ciro_kull_i̇ade_dus', 'promosyon_tutari', 'i̇skonto_'], axis=1)\n",
    "\n",
    "pasifik_aktivite_df3.rename(columns={\"Müşteri Grup\": \"Grup adı\"}, inplace=True)\n",
    "df_pasifik5 = pd.merge(df_pasifik4, pasifik_aktivite_df3[['yil', 'ay', 'grup_adi', 'en_guncel_kod', \n",
    "                                                          'raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', 'indirim_', \n",
    "                                                          'aktivite_tipi']], \n",
    "                           left_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                           right_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], how=\"left\")\n",
    "\n",
    "df_horizon5 = df_horizon4.merge(saha_aktivite_detay3[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                      'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], how=\"left\")\n",
    "\n",
    "btt_aktivite = saha_aktivite_detay3[saha_aktivite_detay3[\"grup_adi\"] == \"GELENEKSEL KANAL\"].reset_index(drop=True)\n",
    "btt_aktivite[\"grup_adi\"] = \"BTT\"\n",
    "\n",
    "df_btt5 = df_btt4.merge(btt_aktivite[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                      'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                        on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = df_pasifik5.copy()\n",
    "horizon_saha_df_sorted = df_horizon5.copy()\n",
    "btt_df_sorted = df_btt5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Yarışma ve Enflasyon Verisinin Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.drop(columns=\"enflasyon_etkisi\", axis=1, inplace=True)\n",
    "horizon_saha_df_sorted.drop(columns=\"enflasyon_etkisi\", axis=1, inplace=True)\n",
    "btt_df_sorted.drop(columns=\"enflasyon_etkisi\", axis=1, inplace=True)\n",
    "\n",
    "pasifik_df_sorted = pasifik_df_sorted.merge(enflasyon_df, how=\"left\", on=\"date\")\n",
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(enflasyon_df, how=\"left\", on=\"date\")\n",
    "btt_df_sorted = btt_df_sorted.merge(enflasyon_df, how=\"left\", on=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "yarisma_df[\"date\"] = pd.to_datetime(yarisma_df[\"date\"], errors=\"coerce\", format=\"%d.%m.%Y\")\n",
    "yarisma_df = yarisma_df.dropna().reset_index(drop=True)\n",
    "\n",
    "pasifik_df_sorted[\"yarisma\"] = 0\n",
    "horizon_saha_df_sorted[\"yarisma\"] = 0\n",
    "btt_df_sorted[\"yarisma\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(yarisma_df)):\n",
    "    yarisma_df[\"type\"].fillna(0, inplace=True)\n",
    "    if yarisma_df.loc[idx, \"type\"] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        temp_df = horizon_saha_df_sorted[(horizon_saha_df_sorted[\"date\"] == yarisma_df.loc[idx, \"date\"]) & \n",
    "                                         (horizon_saha_df_sorted[\"grup_adi\"] == yarisma_df.loc[idx, \"grup_adi\"]) & \n",
    "                                         (horizon_saha_df_sorted[yarisma_df.loc[idx, \"type\"]] == yarisma_df.loc[idx, \"col1\"])]\n",
    "        idx_to_replace = temp_df.index\n",
    "        horizon_saha_df_sorted.loc[idx_to_replace, \"yarisma\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yarışam verisinin eklenme süresi: 0:00:10.895194\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Yarışam verisinin eklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Arada tarih verisi olmayan gözlemler\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SKU Sayıları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month_ = datetime.now().month\n",
    "# year_ = datetime.now().year\n",
    "month_ = 5\n",
    "year_ = 2021\n",
    "\n",
    "date__ = datetime(year_, month_, 1)\n",
    "\n",
    "pas_df_all2 = pas_backup.copy()\n",
    "hor_df_all2 = hor_backup.copy()\n",
    "btt_df_all2_ = btt_backup.copy()\n",
    "\n",
    "pas_df_all2 = pas_df_all2[pas_df_all2[\"date\"] <= date__]\n",
    "hor_df_all2 = hor_df_all2[hor_df_all2[\"date\"] <= date__]\n",
    "btt_df_all2_ = btt_df_all2_[btt_df_all2_[\"date\"] <= date__]\n",
    "\n",
    "pas_df_all2.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "hor_df_all2.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "btt_df_all2_.rename(columns={\"Date\": \"date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku = []\n",
    "grup_adi = []\n",
    "portfoy = []\n",
    "oran = []\n",
    "gozlem_sayisi = []\n",
    "toplam_satir = []\n",
    "baslangic_tarih = []\n",
    "bitis_tarih = []\n",
    "son_kac_ay_eksik = []\n",
    "kanal = []\n",
    "repeat_num = [] # 1'lerin kaç aralıkla geldiği\n",
    "for idx, temp_df in pas_df_all2[[\"grup_adi\", \"en_guncel_kod\", \"portfoy\", \"adet\", \"date\"]].groupby([\"grup_adi\", \"en_guncel_kod\", \"portfoy\"]):\n",
    "    sku.append(temp_df[\"en_guncel_kod\"].iloc[0])\n",
    "    grup_adi.append(temp_df[\"grup_adi\"].iloc[0])\n",
    "    portfoy.append(temp_df[\"portfoy\"].iloc[0])\n",
    "    oran.append(len(temp_df[temp_df[\"adet\"] != 1]) / len(temp_df))\n",
    "    gozlem_sayisi.append(len(temp_df[temp_df[\"adet\"] != 1]))\n",
    "    baslangic_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].min())\n",
    "    bitis_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].max())\n",
    "    toplam_satir.append(len(temp_df))\n",
    "    son_kac_ay_eksik.append(round((pas_df_all2.date.max() - temp_df[temp_df[\"adet\"] != 1].date.max())/np.timedelta64(1, 'M'),1))\n",
    "    kanal.append(\"Pasifik\")\n",
    "\n",
    "    \n",
    "    lst = temp_df[temp_df[\"adet\"] == 1].index\n",
    "    lst = sorted(lst, reverse=True)\n",
    "    counter = 0\n",
    "    for idx_ in range(len(lst) - 1):\n",
    "        if lst[idx_] - lst[idx_+1] != 1:\n",
    "            counter+=1\n",
    "        else: \n",
    "            pass\n",
    "    if counter != 0:\n",
    "        counter+=1\n",
    "    repeat_num.append(counter)\n",
    "\n",
    "for idx, temp_df in hor_df_all2[[\"grup_adi\", \"en_guncel_kod\", \"portfoy\", \"adet\", \"date\"]].groupby([\"grup_adi\", \"en_guncel_kod\", \"portfoy\"]):\n",
    "    sku.append(temp_df[\"en_guncel_kod\"].iloc[0])\n",
    "    grup_adi.append(temp_df[\"grup_adi\"].iloc[0])\n",
    "    portfoy.append(temp_df[\"portfoy\"].iloc[0])\n",
    "    oran.append(len(temp_df[temp_df[\"adet\"] != 1]) / len(temp_df))\n",
    "    gozlem_sayisi.append(len(temp_df[temp_df[\"adet\"] != 1]))\n",
    "    baslangic_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].min())\n",
    "    bitis_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].max())\n",
    "    toplam_satir.append(len(temp_df))\n",
    "    son_kac_ay_eksik.append(round((hor_df_all2.date.max() - temp_df[temp_df[\"adet\"] != 1].date.max())/np.timedelta64(1, 'M'),1))\n",
    "    kanal.append(\"Horizon\")\n",
    "\n",
    "\n",
    "    lst = temp_df[temp_df[\"adet\"] == 1].index\n",
    "    lst = sorted(lst, reverse=True)\n",
    "    counter = 0\n",
    "    for idx_ in range(len(lst) - 1):\n",
    "        if lst[idx_] - lst[idx_+1] != 1:\n",
    "            counter+=1\n",
    "        else: \n",
    "            pass\n",
    "    if counter != 0:\n",
    "        counter+=1\n",
    "    repeat_num.append(counter)\n",
    "\n",
    "for idx, temp_df in btt_df_all2_[[\"grup_adi\", \"en_guncel_kod\", \"portfoy\", \"adet\", \"date\"]].groupby([\"grup_adi\", \"en_guncel_kod\", \"portfoy\"]):\n",
    "    sku.append(temp_df[\"en_guncel_kod\"].iloc[0])\n",
    "    grup_adi.append(temp_df[\"grup_adi\"].iloc[0])\n",
    "    portfoy.append(temp_df[\"portfoy\"].iloc[0])\n",
    "    oran.append(len(temp_df[temp_df[\"adet\"] != 1]) / len(temp_df))\n",
    "    gozlem_sayisi.append(len(temp_df[temp_df[\"adet\"] != 1]))\n",
    "    baslangic_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].min())\n",
    "    bitis_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].max())\n",
    "    toplam_satir.append(len(temp_df))\n",
    "    son_kac_ay_eksik.append(round((btt_df_all2_.date.max() - temp_df[temp_df[\"adet\"] != 1].date.max())/np.timedelta64(1, 'M'),1))\n",
    "    kanal.append(\"BTT\")\n",
    "\n",
    "\n",
    "    lst = temp_df[temp_df[\"adet\"] == 1].index\n",
    "    lst = sorted(lst, reverse=True)\n",
    "    counter = 0\n",
    "    for idx_ in range(len(lst) - 1):\n",
    "        if lst[idx_] - lst[idx_+1] != 1:\n",
    "            counter+=1\n",
    "        else: \n",
    "            pass\n",
    "    if counter != 0:\n",
    "        counter+=1\n",
    "    repeat_num.append(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_sayilari = pd.DataFrame({\"sku\": sku,\n",
    "                             \"grup_adi\": grup_adi,\n",
    "                             \"portfoy\": portfoy,\n",
    "                             \"gozlem_sayisi\": gozlem_sayisi,\n",
    "                             \"toplam_satir\": toplam_satir,\n",
    "                             \"oran\": oran,\n",
    "                             \"baslangic_tarih\": baslangic_tarih,\n",
    "                             \"bitis_tarih\": bitis_tarih,\n",
    "                             \"son_kac_ay_eksik\": son_kac_ay_eksik,\n",
    "                             \"eksik_repeat_sayisi\": repeat_num,\n",
    "                             \"kanal\": kanal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_sayilari.sort_values(by=[\"sku\", \"grup_adi\", \"oran\"], ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKU gözlem sayılarının elde edilme süresi: 0:00:29.142358\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('SKU gözlem sayılarının elde edilme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adetteki düzenlemeleri new_adet üzerinden devam ettiriyoruz\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"new_adet\"] = pasifik_df_sorted[\"adet\"]\n",
    "horizon_saha_df_sorted[\"new_adet\"] = horizon_saha_df_sorted[\"adet\"]\n",
    "btt_df_sorted[\"new_adet\"] = btt_df_sorted[\"adet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SKU Sayıları Dataya Ekleme, Scope Belirlenmesi İçin Gerekli\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_insights = sku_sayilari.copy()\n",
    "sku_insights.drop(columns=[\"portfoy\", \"kanal\"], axis=1, inplace=True)\n",
    "sku_insights.rename(columns={\"sku\": \"en_guncel_kod\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sku_sayilari = sku_sayilari[[\"sku\"] + sku_insights.columns.to_list()[1:]]\n",
    "    sku_sayilari.columns = sku_insights.columns\n",
    "except:\n",
    "    sku_sayilari.columns = sku_insights.columns    \n",
    "\n",
    "pasifik_df_sorted = pasifik_df_sorted.merge(sku_sayilari, on=[\"en_guncel_kod\", \"grup_adi\"], how=\"left\")\n",
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(sku_sayilari, on=[\"en_guncel_kod\", \"grup_adi\"], how=\"left\")\n",
    "btt_df_sorted = btt_df_sorted.merge(sku_sayilari, on=[\"en_guncel_kod\", \"grup_adi\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SKU Labellama\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"scope\"] = 0\n",
    "horizon_saha_df_sorted[\"scope\"] = 0\n",
    "btt_df_sorted[\"scope\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_pas1 = pasifik_df_sorted.copy()\n",
    "backup_hor1 = horizon_saha_df_sorted.copy()\n",
    "backup_btt1 = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope(data, obs_threshold, obs_mean_threshold, obs_ratio, latest_obs_date):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    obs_threshold:        int\n",
    "    obs_mean_threshold:   int\n",
    "    obs_ratio:            float\n",
    "    latest_obs_date:      datetime\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    obs_threshold:        Toplam kaç adet gözlemi olsun?\n",
    "    obs_mean_threshold:   Kaç gözlem altında ortalama basılıp geçilsin?\n",
    "    obs_ratio:            Gözlem oranı yüzde kaç olmalı? %50'nin altında ise alınmasın vb.\n",
    "    latest_obs_date:      En son hangi tarihte gözlemi olursa analize dahil edilsin?\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    data[\"scope\"] = 0\n",
    "    for sku in data[\"en_guncel_kod\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "            \n",
    "            temp_df = data[(data[\"en_guncel_kod\"] == sku) & \n",
    "                           (data[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            if len(temp_df) > 1:\n",
    "                gozlem_sayisi = temp_df[\"gozlem_sayisi\"][0]\n",
    "                oran = temp_df[\"oran\"][0]\n",
    "                bitis_tarihi = temp_df[\"bitis_tarih\"][0]\n",
    "                \n",
    "                if (gozlem_sayisi < obs_threshold) and (oran < obs_ratio) and (bitis_tarihi <= latest_obs_date):\n",
    "                    temp_df[\"scope\"] = 0 # Kapsam Dışı\n",
    "                else:\n",
    "                    if (gozlem_sayisi < obs_mean_threshold) and (bitis_tarihi > latest_obs_date):\n",
    "                        temp_df[\"scope\"] = 1 # Ortalama basılacak olan\n",
    "                    elif (gozlem_sayisi >= obs_threshold) and (oran >= obs_ratio) and (bitis_tarihi > latest_obs_date):\n",
    "                        temp_df[\"scope\"] = 2 # Time Series\n",
    "                    elif (gozlem_sayisi >= obs_mean_threshold) and (bitis_tarihi > latest_obs_date):\n",
    "                        temp_df[\"scope\"] = 3 # Regresyon\n",
    "            else:\n",
    "                pass\n",
    "            all_data.append(temp_df)\n",
    "    all_data = pd.concat(all_data)\n",
    "    all_data.reset_index(drop=True, inplace=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = scope(pasifik_df_sorted, 36, 12, 0.85, datetime(2020, 12, 1))\n",
    "horizon_saha_df_sorted = scope(horizon_saha_df_sorted, 36, 12, 0.85, datetime(2020, 12, 1))\n",
    "btt_df_sorted = scope(btt_df_sorted, 36, 12, 0.85, datetime(2020, 12, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kapsam labellaması süresi: 0:02:12.017085\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kapsam labellaması süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Kapsamı yeniden düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "kapsam_all.columns = change_col_name(kapsam_all)\n",
    "eslenik_kod_df.columns = change_col_name(eslenik_kod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "kapsam_all[\"urun_kodu\"] = kapsam_all[\"urun_kodu\"].apply(lambda x: int(x.split(\"-\")[0]+x.split(\"-\")[1]))\n",
    "kapsam_all = kapsam_all.merge(eslenik_kod_df[[\"urun_kodu\", \"en_guncel_kod\"]], how=\"left\", on=\"urun_kodu\")\n",
    "kapsam_all[\"en_guncel_kod\"].fillna(kapsam_all[\"urun_kodu\"], inplace=True)\n",
    "kapsam_all.drop_duplicates(subset=[\"en_guncel_kod\", \"grup_adi\"], inplace=True, ignore_index=True)\n",
    "kapsam_all = kapsam_all[kapsam_all[\"en_guncel_kod\"] != \"delist\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eşlenik Kod ve Ürün İsim Kod kısmında düzenleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urun_isim_kod = df2[[\"urun_adi\", \"en_guncel_kod\"]].drop_duplicates(subset=[\"urun_adi\", \"en_guncel_kod\"], ignore_index=True)\n",
    "urun_isim_kod.rename(columns={\"urun_adi\": \"urun\"}, inplace=True)\n",
    "\n",
    "eslenik_kod_df.rename(columns={\"En Güncel Kod\": \"en_guncel_kod\", \"Ürün Adı (Orjinal)\": \"new_urun\"}, inplace=True)\n",
    "eslenik_kod_df = eslenik_kod_df[eslenik_kod_df[\"en_guncel_kod\"] != \"delist\"].reset_index(drop=True)\n",
    "eslenik_kod_df[\"en_guncel_kod\"] = eslenik_kod_df[\"en_guncel_kod\"].astype(\"int64\")\n",
    "indexes = []\n",
    "for idx in eslenik_kod_df.index:\n",
    "    if eslenik_kod_df[\"Kod1\"][idx] == eslenik_kod_df[\"en_guncel_kod\"][idx]:\n",
    "        indexes.append(idx)\n",
    "\n",
    "eslenik_kod_df2 = eslenik_kod_df[eslenik_kod_df.index.isin(indexes)].reset_index(drop=True)\n",
    "eslenik_kod_df2 = eslenik_kod_df2[[\"en_guncel_kod\", \"new_urun\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKUP_PASIFIK = pasifik_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df_sorted.merge(kapsam_all[[\"en_guncel_kod\", \"grup_adi\", \"durum\"]], how=\"left\", on=[\"en_guncel_kod\", \"grup_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_write_deneme = list(pasifik_df_sorted[(pasifik_df_sorted[\"durum\"].isna()) & (pasifik_df_sorted[\"grup_adi\"].isin([\"A101\", \"ŞOK\", \"BİM\"]))].index)\n",
    "check = pasifik_df_sorted[(pasifik_df_sorted[\"grup_adi\"] == \"BİM\") & (pasifik_df_sorted[\"portfoy\"] == 1)]\n",
    "check = check[[\"en_guncel_kod\", \"grup_adi\", \"portfoy\", \"durum\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.loc[idx_to_write_deneme, \"durum\"] = \"DENEME\"\n",
    "horizon_saha_df_sorted[\"durum\"] = np.nan\n",
    "btt_df_sorted[\"durum\"] = np.nan\n",
    "pasifik_df_sorted[\"Kanal\"] = \"pasifik\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_backup_kapsam = pasifik_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_scope_index = pasifik_df_sorted[(pasifik_df_sorted[\"portfoy\"] == 1) & \n",
    "                                       (pasifik_df_sorted[\"Kanal\"] == \"pasifik\") & \n",
    "                                       (pasifik_df_sorted[\"durum\"].isin(['DENEME+BÖLGESEL SATIŞ', 'DENEME'])) & \n",
    "                                       (~pasifik_df_sorted[\"grup_adi\"].isin([\"Diğer_Pasifik\", \"MİGROS\"]))].index\n",
    "\n",
    "pasifik_df_sorted.loc[change_scope_index, \"scope\"] = 0\n",
    "pasifik_df_sorted.loc[change_scope_index, \"scope_type\"] = \"kapsam_disi\"\n",
    "pasifik_df_sorted.drop(\"Kanal\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Peak ve Dip Noktalarını Belirleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_detection(data, sigma):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    sigma:                float\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    sigma:                Standart sapma ile çarpılacak olan sayı. Verinin yüzdelik olarak hangi kısmının threshold olarak alınacağına bu değer ile karar verilir. Örn: 1.96 verilmesi durumunda %95'e tekabul eder. \n",
    "                          (https://www.socscistatistics.com/pvalues/normaldistribution.aspx)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    demand = data.copy()\n",
    "    demand[\"peak\"] = 0\n",
    "    demand_non_portfoy = demand[demand[\"portfoy\"] == 0]\n",
    "    demand = demand[demand[\"portfoy\"] == 1]\n",
    "\n",
    "    grup = demand[\"grup_adi\"].unique()\n",
    "    urun = demand[\"en_guncel_kod\"].unique()\n",
    "    for grp in grup:\n",
    "        for sku in urun:\n",
    "            df=demand[(demand['en_guncel_kod']==sku) & \n",
    "                      (demand[\"grup_adi\"] == grp)]\n",
    "            points=df['adet']\n",
    "\n",
    "            mean = points.mean()\n",
    "            std = points.std()\n",
    "\n",
    "            peaks=[]\n",
    "            index=[]\n",
    "            for idx in points.index:\n",
    "                if points[idx]>=(sigma*std+mean):\n",
    "                    index.append(idx)\n",
    "\n",
    "            for idx_ in index:\n",
    "                demand.loc[idx_, \"peak\"] = 1\n",
    "    demand = pd.concat([demand, demand_non_portfoy], axis=0)\n",
    "    demand.sort_index(inplace=True)\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_detection(data, sigma):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    sigma:                float\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    sigma:                Standart sapma ile çarpılacak olan sayı. Verinin yüzdelik olarak hangi kısmının threshold olarak alınacağına bu değer ile karar verilir. Örn: 1.96 verilmesi durumunda %95'e tekabul eder. \n",
    "                          (https://www.socscistatistics.com/pvalues/normaldistribution.aspx)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    demand = data.copy()\n",
    "    demand_non_portfoy = demand[demand[\"portfoy\"] == 0]\n",
    "    demand = demand[demand[\"portfoy\"] == 1]\n",
    "\n",
    "    grup = demand[\"grup_adi\"].unique()\n",
    "    urun = demand[\"en_guncel_kod\"].unique()\n",
    "    for grp in grup:\n",
    "        for sku in urun:\n",
    "            df=demand[(demand['en_guncel_kod']==sku) & \n",
    "                      (demand[\"grup_adi\"] == grp)]\n",
    "            points=df['adet']\n",
    "\n",
    "            mean = points.mean()\n",
    "            std = points.std()\n",
    "\n",
    "            peaks=[]\n",
    "            index=[]\n",
    "            if mean <= 100000:\n",
    "                for idx in points.index:\n",
    "                    if (points[idx]<=(mean-sigma*std)):\n",
    "                        index.append(idx)\n",
    "\n",
    "\n",
    "                for idx_ in sorted(index, reverse=True):\n",
    "                    try:\n",
    "                        demand.loc[idx_-1, \"new_adet\"] += demand.loc[idx_, \"new_adet\"] \n",
    "                        demand.loc[idx_, \"new_adet\"] = 1\n",
    "                    except KeyError:\n",
    "                        index.remove(idx_)\n",
    "            else:\n",
    "                for idx in points.index:\n",
    "                    if (points[idx]<=(mean-2*std)):\n",
    "                        index.append(idx)\n",
    "\n",
    "\n",
    "                for idx_ in sorted(index, reverse=True):\n",
    "                    try:\n",
    "                        demand.loc[idx_-1, \"new_adet\"] += demand.loc[idx_, \"new_adet\"] \n",
    "                        demand.loc[idx_, \"new_adet\"] = 1\n",
    "                    except KeyError:\n",
    "                        index.remove(idx_)\n",
    "    demand = pd.concat([demand, demand_non_portfoy], axis=0)\n",
    "    demand.sort_index(inplace=True)\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_distribution(data, sigma):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    sigma:                float\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    sigma:                Standart sapma ile çarpılacak olan sayı. Verinin yüzdelik olarak hangi kısmının threshold olarak alınacağına bu değer ile karar verilir. Örn: 1.96 verilmesi durumunda %95'e tekabul eder. \n",
    "                          (https://www.socscistatistics.com/pvalues/normaldistribution.aspx)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    demand = data.copy()\n",
    "    demand_non_portfoy = demand[demand[\"portfoy\"] == 0]\n",
    "    demand = demand[demand[\"portfoy\"] == 1]\n",
    "\n",
    "    grup = demand[\"grup_adi\"].unique()\n",
    "    urun = demand[\"en_guncel_kod\"].unique()\n",
    "    for grp in grup:\n",
    "        for sku in urun:\n",
    "            df=demand[(demand['en_guncel_kod']==sku) & \n",
    "                      (demand[\"grup_adi\"] == grp)]\n",
    "            points=df['adet']\n",
    "            \n",
    "            mean = points.mean()\n",
    "            std = points.std()\n",
    "            peak_index = df[df[\"peak\"] == 1].index\n",
    "            idx_to_add = list(df[(df[\"peak\"] != 1) & (df[\"adet\"] != 1)].index)\n",
    "            for idx in peak_index:\n",
    "                peak_value = demand.loc[idx, \"adet\"]\n",
    "                will_add = (peak_value - (mean+std*sigma)) / len(idx_to_add)\n",
    "                demand.loc[idx_to_add, \"new_adet\"] += will_add\n",
    "                demand.loc[idx, \"new_adet\"] = mean+std*sigma\n",
    "    demand = pd.concat([demand, demand_non_portfoy], axis=0)\n",
    "    demand.sort_index(inplace=True)\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pas_backup.copy()\n",
    "horizon_saha_df_sorted = hor_backup.copy()\n",
    "btt_df_sorted = btt_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted, btt_df_sorted, horizon_saha_df_sorted = peak_detection(pasifik_df_sorted, 2.0), peak_detection(btt_df_sorted, 2.0), peak_detection(horizon_saha_df_sorted, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Detection süresi: 0:00:07.726103\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Peak Detection süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup2 = pasifik_df_sorted.copy()\n",
    "hor_backup2 = horizon_saha_df_sorted.copy()\n",
    "btt_backup2 = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = peak_distribution(pasifik_df_sorted, 2.0)\n",
    "btt_df_sorted = peak_distribution(btt_df_sorted, 2.0)\n",
    "horizon_saha_df_sorted = peak_distribution(horizon_saha_df_sorted, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Distribution süresi: 0:00:13.882378\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Peak Distribution süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted, btt_df_sorted, horizon_saha_df_sorted = deep_detection(pasifik_df_sorted, 2.0), deep_detection(btt_df_sorted, 2.0), deep_detection(horizon_saha_df_sorted, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Detection süresi: 0:00:07.045467\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Deep Detection süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pas_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"raf_tavsiye_satis_fiyati\", \"i̇ndirimli_raf_satis_fiyati\", \n",
    "                        \"aktivite_tipi\", \"indirim_\"]\n",
    "\n",
    "    btt_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"i̇skonto_\"]\n",
    "\n",
    "    hor_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"i̇skonto_\"]\n",
    "\n",
    "    pasifik_df_sorted.drop(columns=pas_cols_to_drop, inplace=True)\n",
    "    horizon_saha_df_sorted.drop(columns=hor_cols_to_drop, inplace=True)\n",
    "    btt_df_sorted.drop(columns=btt_cols_to_drop, inplace=True)\n",
    "except:\n",
    "    pas_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"raf_tavsiye_satis_fiyati\", \"i̇ndirimli_raf_satis_fiyati\", \n",
    "                        \"aktivite_tipi\", \"indirim_\"]\n",
    "\n",
    "    btt_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"iskonto_\"]\n",
    "\n",
    "    hor_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"iskonto_\"]\n",
    "\n",
    "    pasifik_df_sorted.drop(columns=pas_cols_to_drop, inplace=True)\n",
    "    horizon_saha_df_sorted.drop(columns=hor_cols_to_drop, inplace=True)\n",
    "    btt_df_sorted.drop(columns=btt_cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pd.merge(pasifik_df_sorted, pasifik_aktivite_df3[['yil', 'ay', 'grup_adi', 'en_guncel_kod', \n",
    "                                                                      'raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', 'indirim_', \n",
    "                                                                      'aktivite_tipi']], \n",
    "                             left_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                             right_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                             how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(saha_aktivite_detay3[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                                            'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                                      on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], \n",
    "                                                      how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_sorted = btt_df_sorted.merge(btt_aktivite[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                  'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                    on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], \n",
    "                                    how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"indirim__\"] = [0 if akt < 0 else akt for akt in pasifik_df_sorted[\"indirim_\"]]\n",
    "pasifik_df_sorted.drop(\"indirim_\", axis=1, inplace=True)\n",
    "\n",
    "horizon_saha_df_sorted[\"i̇skonto__\"] = [0 if ((akt >= 0.35) or (akt <=0.01)) else akt for akt in horizon_saha_df_sorted[\"i̇skonto_\"]]\n",
    "horizon_saha_df_sorted.drop(\"i̇skonto_\", axis=1, inplace=True)\n",
    "\n",
    "btt_df_sorted[\"i̇skonto__\"] = [0 if ((akt >= 0.35) or (akt <=0.01)) else akt for akt in btt_df_sorted[\"i̇skonto_\"]]\n",
    "btt_df_sorted.drop(\"i̇skonto_\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "chng_cols = dict(zip(['Yıl', 'Ay', 'No_of_days', 'Weekdays_n', 'Weekdays_Ratio', 'Weekend_n',\n",
    "                      'Weekend_Ratio', 'Actual_Holiday_n', 'Actual_Holiday_Ratio',\n",
    "                      'Total_Holiday_n', 'Total_Holiday_Ratio', 'School_Day_n',\n",
    "                      'School_Day_Ratio', 'School_Day_brdg_n', 'School_Day_brdg_Ratio',\n",
    "                      'Ramadan_n', 'Ramadan_Ratio', 'Pandemic', 'Lockdown'],\n",
    "                     \n",
    "                     [\"yil\", \"ay\", \"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \n",
    "                      \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                      \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \n",
    "                      \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                      \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "calender_df.rename(columns=chng_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pd.merge(pasifik_df_sorted, calender_df, on=[\"yil\", \"ay\"], how=\"left\")\n",
    "btt_df_sorted = pd.merge(btt_df_sorted, calender_df, on=[\"yil\", \"ay\"], how=\"left\")\n",
    "horizon_saha_df_sorted = pd.merge(horizon_saha_df_sorted, calender_df, on=[\"yil\", \"ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"indirim__\"].fillna(0, inplace=True)\n",
    "horizon_saha_df_sorted[\"i̇skonto__\"].fillna(0, inplace=True)\n",
    "btt_df_sorted[\"i̇skonto__\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted.rename(columns={\"i̇skonto__\": \"indirim__\"}, inplace=True)\n",
    "btt_df_sorted.rename(columns={\"i̇skonto__\": \"indirim__\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bir önceki aya yansımış aktiviteleri düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aktivite_regulation(df):\n",
    "    df_all = []\n",
    "    for sku in df[\"en_guncel_kod\"].unique():\n",
    "        for grup in df[\"grup_adi\"].unique():\n",
    "            test = df[(df[\"en_guncel_kod\"] == sku) & (df[\"grup_adi\"] == grup)]\n",
    "            for idx in test.index:\n",
    "                if (idx-2 not in test.index) and (idx-1 in test.index): # bir öncekine bakacak. ilk ve sonraki satıra bakacak. -1 götür\n",
    "                    if test.loc[idx-1, \"adet\"] > test.loc[idx, \"adet\"]:\n",
    "                        test.loc[idx-1, \"indirim__\"] += test.loc[idx, \"indirim__\"]\n",
    "                        test.loc[idx, \"indirim__\"] = 0\n",
    "                elif (idx-1 not in test.index): # ilk satırdayız. pass\n",
    "                    pass\n",
    "                else:\n",
    "                    dic = {}\n",
    "                    dic.update({idx-2: test.loc[idx-2, \"adet\"], \n",
    "                                idx-1: test.loc[idx-1, \"adet\"],\n",
    "                                idx: test.loc[idx, \"adet\"]})\n",
    "                    max_idx = max(dic, key=dic.get)\n",
    "                    if max_idx == idx:\n",
    "                        pass\n",
    "                    else:\n",
    "                        test.loc[max_idx, \"indirim__\"] += test.loc[idx, \"indirim__\"]\n",
    "                        test.loc[idx, \"indirim__\"] = 0\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasifik_df_sorted, horizon_saha_df_sorted, btt_df_sorted = aktivite_regulation(pasifik_df_sorted), aktivite_regulation(horizon_saha_df_sorted), aktivite_regulation(btt_df_sorted)\n",
    "#PASİFİK İÇİN İPTAL\n",
    "horizon_saha_df_sorted, btt_df_sorted = aktivite_regulation(horizon_saha_df_sorted), aktivite_regulation(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktivite Regulation süresi: 0:02:52.970496\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Aktivite Regulation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pasifik_aktivite_regulation(df):\n",
    "    df_all = []\n",
    "    for sku in df[\"en_guncel_kod\"].unique():\n",
    "        for grup in df[\"grup_adi\"].unique():\n",
    "            test = df[(df[\"en_guncel_kod\"] == sku) & (df[\"grup_adi\"] == grup)]\n",
    "            aktivite_tip_index = test[~test.aktivite_tipi.isna()].index.to_list()\n",
    "            for idx in aktivite_tip_index:\n",
    "                if idx == test.index[0]: # ilk satırsa atla\n",
    "                    pass\n",
    "                elif idx-1 in aktivite_tip_index: #bir önceki satırda aktivite varsa atla\n",
    "                    pass\n",
    "                else:\n",
    "                    if test.loc[idx, 'adet'] < test.loc[idx-1, 'adet']: # adet sayısı bir önceki satırdan küçükse \n",
    "                        test.loc[idx-1, 'aktivite_tipi'] = test.loc[idx, 'aktivite_tipi'] # aktiviteyi bir önceki satıra yaz\n",
    "                        test.loc[idx, 'aktivite_tipi'] = np.nan\n",
    "                        aktivite_tip_index.remove(idx) #listeden remove et ki bir alt satırda varsa önceki var mı kontrolüne takılmasın\n",
    "                        aktivite_tip_index.insert(0,0) #döngü listedeki elementlerin index'ine göre devam ettiği için en başa 0 insert et\n",
    "\n",
    "            indirim_index = test[test.indirim__ != 0].index.to_list()\n",
    "            for idx in indirim_index:\n",
    "                if idx == test.index[0]: # ilk satır\n",
    "                    pass \n",
    "                elif idx-1 in indirim_index: #bir önceki satırda indirim yüzdesi varsa atla\n",
    "                    pass \n",
    "                else:\n",
    "                    if test.loc[idx, 'adet'] < test.loc[idx-1, 'adet']:\n",
    "                        test.loc[idx-1, 'indirim__'] = test.loc[idx, 'indirim__'] #bir üste taşındı\n",
    "                        test.loc[idx, 'indirim__'] = 0  \n",
    "                        indirim_index.remove(idx)  #listeden remove et ki bir alt satırda varsa önceki var mı kontrolüne takılmasın\n",
    "                        indirim_index.insert(0, 0) #döngü listedeki elementlerin index'ine göre devam ettiği için en başa 0 insert et\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_aktivite_regulation(pasifik_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik Aktivite Regulation süresi: 0:00:29.499250\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik Aktivite Regulation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Binslere ayırma\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, pasifik_df_sorted.indirim__.max()+1]\n",
    "#indirim_bins = [0, 0.009, 0.05, 0.10, 0.15, 0.20, 0.25, 0.50, pasifik_df_sorted.indirim__.max()+1]\n",
    "pasifik_df_sorted.indirim__.hist(bins=indirim_bins)\n",
    "len(indirim_bins) , pasifik_df_sorted.indirim__.value_counts(bins=indirim_bins).sort_index()\n",
    "pasifik_df_sorted['indirim__bins'] = pd.cut(pasifik_df_sorted.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, horizon_saha_df_sorted.indirim__.max()+1]\n",
    "horizon_saha_df_sorted.indirim__.hist(bins=indirim_bins)\n",
    "len(indirim_bins) , horizon_saha_df_sorted.indirim__.value_counts(bins=indirim_bins)\n",
    "horizon_saha_df_sorted['indirim__bins'] = pd.cut(horizon_saha_df_sorted.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, btt_df_sorted.indirim__.max()+1]\n",
    "btt_df_sorted.indirim__.hist(bins=indirim_bins)\n",
    "len(indirim_bins) , btt_df_sorted.indirim__.value_counts(bins=indirim_bins)\n",
    "btt_df_sorted['indirim__bins'] = pd.cut(btt_df_sorted.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Missing Imputation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_imputation(data):\n",
    "    df_all = []\n",
    "    data[\"new_adet\"] = data[\"adet\"]\n",
    "    for sku in data[\"en_guncel_kod\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "            test = data[(data[\"en_guncel_kod\"] == sku) & (data[\"grup_adi\"] == grup)]\n",
    "            index_full = list(test[test[\"new_adet\"] != 1].index)\n",
    "            for idx in range(len(index_full) - 1):\n",
    "                if index_full[idx+1] - index_full[idx] != 1:\n",
    "                    index_na = list(range(index_full[idx]+1, index_full[idx+1]))\n",
    "                    fark = test.loc[index_full[idx+1], \"new_adet\"] - test.loc[index_full[idx], \"new_adet\"]\n",
    "                    bol = len(index_na)\n",
    "                    ekle = fark/(bol+1)\n",
    "                    for i in index_na:\n",
    "                        test.loc[i, \"new_adet\"] = 0\n",
    "                        test.loc[i, \"new_adet\"] += ekle+test.loc[i-1, \"new_adet\"]\n",
    "                else:\n",
    "                    pass\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = missing_imputation(pasifik_df_sorted)\n",
    "horizon_saha_df_sorted = missing_imputation(horizon_saha_df_sorted)\n",
    "btt_df_sorted = missing_imputation(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Imputation süresi: 0:02:08.111157\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Missing Imputation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_regression = pasifik_df_sorted.copy()\n",
    "horizon_regression = horizon_saha_df_sorted.copy()\n",
    "btt_regression = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_regression = pasifik_regression[pasifik_regression[\"new_adet\"] != 1].reset_index(drop=True)\n",
    "horizon_regression = horizon_regression[horizon_regression[\"new_adet\"] != 1].reset_index(drop=True)\n",
    "btt_regression = btt_regression[btt_regression[\"new_adet\"] != 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_seasonality_decomp(data):\n",
    "    df_all = []\n",
    "    for sku in data[\"en_guncel_kod\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "#           print(grup, sku)\n",
    "            temp_df = data[(data[\"en_guncel_kod\"] == sku) & \n",
    "                           (data[\"grup_adi\"] == grup)]\n",
    "            if len(temp_df) > 2:\n",
    "#                print(sku, grup)\n",
    "                df_ts = temp_df[['new_adet','date']]\n",
    "                df_ts.set_index('date',inplace=True)\n",
    "\n",
    "                result = STL(df_ts).fit()\n",
    "                temp_df['season'] = list(result.seasonal)\n",
    "                temp_df['trend']  = list(result.trend)\n",
    "                temp_df['residual']  = list(result.resid)\n",
    "                df_all.append(temp_df)\n",
    "            else:\n",
    "                pass\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "horizon_saha_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "btt_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "\n",
    "btt_df_sorted.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\", \"date\"], keep=\"first\", ignore_index=True, inplace=True)\n",
    "horizon_saha_df_sorted.drop_duplicates(subset=[\"date\", \"grup_adi\", \"en_guncel_kod\"], keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_reg = pasifik_df_sorted[pasifik_df_sorted[\"scope\"] == 3]\n",
    "horizon_reg = horizon_saha_df_sorted[horizon_saha_df_sorted[\"scope\"] == 3]\n",
    "btt_reg = btt_df_sorted[btt_df_sorted[\"scope\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df = trend_seasonality_decomp(pasifik_df_sorted)\n",
    "horizon_df = trend_seasonality_decomp(horizon_saha_df_sorted)\n",
    "btt_df = trend_seasonality_decomp(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Seasonality süresi: 0:02:18.076500\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Trend Seasonality süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Yeni adet flaglendi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df[\"adet_flag\"] = np.where(pasifik_df[\"adet\"] - pasifik_df[\"new_adet\"] == 0, 0, 1)\n",
    "horizon_df[\"adet_flag\"] = np.where(horizon_df[\"adet\"] - horizon_df[\"new_adet\"] == 0, 0, 1)\n",
    "btt_df[\"adet_flag\"] = np.where(btt_df[\"adet\"] - btt_df[\"new_adet\"] == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df.copy()\n",
    "horizon_saha_df_sorted = horizon_df.copy()\n",
    "btt_df_sorted = btt_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted[\"aktivite_tipi\"] = np.nan\n",
    "btt_df_sorted[\"aktivite_tipi\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.rename(columns={'i̇ndirimli_raf_satis_fiyati': 'ciro_kull_i̇ade_dus',\n",
    "                                  'raf_tavsiye_satis_fiyati': 'promosyon_tutari'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"Kanal\"] = \"pasifik\"\n",
    "horizon_saha_df_sorted[\"Kanal\"] = \"horizon\"\n",
    "btt_df_sorted[\"Kanal\"] = \"btt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_dict = {0: \"kapsam_disi\", 1: \"ortalama_basilacak\", 2: \"ts\", 3: \"regresyon\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"scope_type\"] = pasifik_df_sorted[\"scope\"].map(scope_dict)\n",
    "horizon_saha_df_sorted[\"scope_type\"] = horizon_saha_df_sorted[\"scope\"].map(scope_dict)\n",
    "btt_df_sorted[\"scope_type\"] = btt_df_sorted[\"scope\"].map(scope_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([pasifik_df_sorted, horizon_saha_df_sorted, btt_df_sorted], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Aktivite Dictionary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "aktivite_dict = {0:0, 'Mağaza içi/Dağılım':2, 'İn&out':5, \n",
    "                 'Çoklu Alım':8, 'Mutluluk':11, 'Kasiyer':14, 'CRM':17}\n",
    "\n",
    "df_all.aktivite_tipi.fillna(0, inplace=True)\n",
    "df_all.aktivite_tipi = df_all.aktivite_tipi.map(aktivite_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datanın dışarıya aktarılması\n",
    "### Fiyata Göre Sıralanmış Datanın Dışarıya Aktarılması\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[~((df_all[\"yil\"] == 2021) & (df_all[\"ay\"].isin([6, 7, 8, 9])))]\n",
    "data_main_ = df_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"../data/_all_data_36_12_siparis_final_sigma2_0_new_seasonality.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datayı dışarıya çıkarma süresi: 0:00:14.591872\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Datayı dışarıya çıkarma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datanı altına kombinasyonların eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_all[(df_all[\"portfoy\"] == 1) & (df_all[\"scope_type\"] != \"kapsam_disi\")].reset_index(drop=True)\n",
    "data_backup = data.copy()\n",
    "sku_list = data.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], keep=\"first\", ignore_index=True)[[\"grup_adi\", \"en_guncel_kod\", \"Kanal\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enflasyon(df):\n",
    "#    month_ = datetime.now().month + 1\n",
    "#    year_ = datetime.now().year\n",
    "#    if month_ == 12:\n",
    "#        month_ = 1\n",
    "#    else:\n",
    "#        pass\n",
    "    month_ = 6\n",
    "    year_ = 2021\n",
    "    enflasyon_data = df[(df[\"date\"] >= datetime(year_, month_, 1)-relativedelta(month=2)) & (df[\"date\"] < datetime(year_, month_, 1))]\n",
    "    enflasyon_data = enflasyon_data[[\"yil\", \"ay\", \"date\", \"enflasyon_etkisi\"]].drop_duplicates(subset=[\"ay\", \"yil\", \"date\", \"enflasyon_etkisi\"], ignore_index=True)\n",
    "    enflasyon_data = pd.concat([enflasyon_data]*2, ignore_index=True)\n",
    "    for idx in range(4, len(enflasyon_data)):\n",
    "        enflasyon_data.loc[idx, \"enflasyon_etkisi\"] = np.nan\n",
    "        enflasyon_data.loc[idx, \"yil\"] = year_\n",
    "        enflasyon_data.loc[idx, \"ay\"] = idx+2\n",
    "        enflasyon_data.loc[idx, \"date\"] = datetime(year_, idx+2, 1)\n",
    "    for idx in range(4, len(enflasyon_data)):\n",
    "        enflasyon_data.loc[idx, \"enflasyon_etkisi\"] = enflasyon_data.loc[idx-4: idx-1, \"enflasyon_etkisi\"].mean()\n",
    "    enf_prev = data[data[\"date\"] < datetime(year_, month_, 1)][[\"yil\", \"ay\", \n",
    "                                                                \"date\", \"enflasyon_etkisi\"]].drop_duplicates(subset=[\"yil\", \"ay\", \n",
    "                                                                                                                     \"date\", \"enflasyon_etkisi\"]).sort_values(by=\"date\", ignore_index=True)\n",
    "    enflasyon_final = pd.concat([enf_prev, enflasyon_data.iloc[-4:, :]], axis=0, ignore_index=True)\n",
    "    return enflasyon_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "enflasyon_data = create_enflasyon(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "enf_dict_fill = {}\n",
    "for idx in range(len(enflasyon_data)):\n",
    "    enf_dict_fill.update({enflasyon_data[\"date\"][idx]: enflasyon_data[\"enflasyon_etkisi\"][idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comb(excels):\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "    month_ = 5\n",
    "    year_ = 2021\n",
    "    pas_s=list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0, 2, 5, 8, 11, 14, 17]))\n",
    "    diger_s=list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0]))\n",
    "    df_all__=[]\n",
    "    for i in range(len(sku_list)):\n",
    "        tmp_=excels[(excels[\"grup_adi\"] == sku_list.loc[i, \"grup_adi\"]) & \n",
    "                   (excels[\"en_guncel_kod\"] == sku_list.loc[i, \"en_guncel_kod\"])]\n",
    "        new_date = datetime(year_, month_, 1)\n",
    "        if tmp_['Kanal'].values[0]=='pasifik':\n",
    "            for trh in range(1, 5):\n",
    "                new_date_ = new_date + relativedelta(months=trh)\n",
    "                create_new = pd.DataFrame(tmp_.iloc[-1])\n",
    "                create_new.loc[\"yil\"] = new_date_.year\n",
    "                create_new.loc[\"ay\"] = new_date_.month\n",
    "                create_new.loc[\"date\"] = datetime(new_date_.year, new_date_.month, 1)\n",
    "                create_new.loc[\"new_adet\"] = np.nan\n",
    "                create_new.loc[\"adet\"] = np.nan\n",
    "                create_new.loc[\"koli\"] = np.nan\n",
    "                create_new.loc[\"kg\"] = np.nan\n",
    "                create_new.loc[\"tl\"] = np.nan\n",
    "                create_new.loc[\"satis_var\"] = np.nan\n",
    "                create_new.loc[\"enflasyon_etkisi\"] = np.nan\n",
    "                create_new.loc[\"yarisma\"] = np.nan\n",
    "                create_new.loc[\"peak\"] = 0\n",
    "                create_new.loc[\"fiyat\"] = np.nan\n",
    "                create_new.loc[\"fiyat_gecisi\"] = np.nan\n",
    "                create_new.loc[\"pandemic\"] = 1\n",
    "                create_new.loc[\"lockdown\"] = 0\n",
    "                create_new.loc[\"season\"] = np.nan\n",
    "                create_new.loc[\"trend\"] = np.nan\n",
    "                tmp = pd.concat([create_new.T]*len(pas_s), ignore_index=True)\n",
    "                for j in range(len(pas_s)):\n",
    "                    tmp.loc[j,'indirim__bins'] = pas_s[j][0]\n",
    "                    tmp.loc[j,'aktivite_tipi'] = pas_s[j][1]\n",
    "                df_all__.append(tmp)\n",
    "        else:\n",
    "            for trh in range(1, 5):\n",
    "                new_date_ = new_date + relativedelta(months=trh)\n",
    "                create_new = pd.DataFrame(tmp_.iloc[-1])\n",
    "                create_new.loc[\"yil\"] = new_date_.year\n",
    "                create_new.loc[\"ay\"] = new_date_.month\n",
    "                create_new.loc[\"date\"] = datetime(new_date_.year, new_date_.month, 1)\n",
    "                create_new.loc[\"new_adet\"] = np.nan\n",
    "                create_new.loc[\"adet\"] = np.nan\n",
    "                create_new.loc[\"koli\"] = np.nan\n",
    "                create_new.loc[\"kg\"] = np.nan\n",
    "                create_new.loc[\"tl\"] = np.nan\n",
    "                create_new.loc[\"satis_var\"] = np.nan\n",
    "                create_new.loc[\"enflasyon_etkisi\"] = np.nan\n",
    "                create_new.loc[\"yarisma\"] = np.nan\n",
    "                create_new.loc[\"peak\"] = 0\n",
    "                create_new.loc[\"fiyat\"] = np.nan\n",
    "                create_new.loc[\"fiyat_gecisi\"] = np.nan\n",
    "                create_new.loc[\"pandemic\"] = 1\n",
    "                create_new.loc[\"lockdown\"] = 0\n",
    "                create_new.loc[\"season\"] = np.nan\n",
    "                create_new.loc[\"trend\"] = np.nan\n",
    "                tmp = pd.concat([create_new.T]*len(diger_s), ignore_index=True)\n",
    "                for k in range(len(diger_s)):\n",
    "                    tmp.loc[k,'indirim__bins']=diger_s[k][0]\n",
    "                    tmp.loc[k,'aktivite_tipi']=diger_s[k][1]\n",
    "                df_all__.append(tmp)\n",
    "    df_all__ = pd.concat(df_all__, ignore_index=True)\n",
    "    return df_all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = create_comb(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = all_combinations[['new_adet', 'yil', 'ay', 'date', 'Kanal', 'grup_adi', 'ana_kategori_adi', \n",
    "                                     'kategori_adi', 'marka_adi', 'urun_adi', 'enflasyon_etkisi',  'peak', 'indirim__bins', \n",
    "                                     'aktivite_tipi', 'lockdown', 'season', 'trend', 'scope', 'scope_type', 'portfoy']]\n",
    "\n",
    "all_combinations[\"enflasyon_etkisi\"] = all_combinations[\"date\"].map(enf_dict_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodun çalışma süresi: 0:03:25.202499\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combination_df(df_):\n",
    "    \"\"\" \n",
    "    Tüm kombinasyonlar için bir dictionary içerisinde 91 tane dataframe oluşturuyor. Input olarak aldığı df'in sonuna 3 satır \n",
    "    ekleyip indirim__bins ve aktivite_tipi'ni df_indirimbins_aktivitetipi olacak şekilde giriyor. new_adet değişkenine son 3 satır için NA atıyor.  \n",
    "    \"\"\"\n",
    "\n",
    "    indirim_bins_list = ['minus_one','zero','one','two','three','four','five','six','seven','eight','nine','ten','eleven']\n",
    "    aktivite_tipi_list = ['zero','two','five','eight','eleven','fourteen','seventeen']\n",
    "    df_names_dict = list(itertools.product(indirim_bins_list, aktivite_tipi_list))\n",
    "\n",
    "    ind_akt = list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0, 2, 5, 8, 11, 14, 17]))\n",
    "\n",
    "    df_names = []\n",
    "    for idx in df_names_dict:\n",
    "        df_names.append('df_' + idx[0] + '_' + idx[1])\n",
    "\n",
    "    df_dict = {}\n",
    "    for i in range(len(df_names)):\n",
    "        main_data = data.copy()\n",
    "        tmp = df_[(df_[\"aktivite_tipi\"] == ind_akt[i][1]) & \n",
    "                  (df_[\"indirim__bins\"] == ind_akt[i][0])]\n",
    "\n",
    "        new_comb_with_main_data = pd.concat([main_data, tmp], ignore_index=True)\n",
    "        new_comb_with_main_data.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], inplace=True, ignore_index=True)\n",
    "        df_dict[df_names[i]] = new_comb_with_main_data.copy()\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combs_df = create_combination_df(all_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodun çalışma süresi: 0:00:18.720137\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to login...\n",
      "Accessed!\n"
     ]
    }
   ],
   "source": [
    "access_ = True\n",
    "while access_:\n",
    "    try:\n",
    "        conn = swat.CAS('yhtrcl-sasccnt1.yildiz.domain', 5570, username='tunahan.aktas', password='34m153294T...')\n",
    "        print(\"Trying to login...\")\n",
    "        access_ = False\n",
    "        print(\"Accessed!\")\n",
    "    except:\n",
    "        print(\"Got error. Trying to reconnect...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_MINUS_ONE_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_MINUS_ONE_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_MINUS_ONE_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_MINUS_ONE_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_MINUS_ONE_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_MINUS_ONE_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_MINUS_ONE_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ZERO_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ZERO_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ZERO_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ZERO_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ZERO_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ZERO_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ZERO_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ONE_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ONE_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ONE_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ONE_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ONE_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ONE_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ONE_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TWO_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TWO_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TWO_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TWO_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TWO_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TWO_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TWO_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_THREE_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_THREE_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_THREE_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_THREE_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_THREE_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_THREE_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_THREE_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FOUR_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FOUR_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FOUR_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FOUR_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FOUR_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FOUR_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FOUR_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FIVE_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FIVE_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FIVE_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FIVE_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FIVE_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FIVE_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_FIVE_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SIX_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SIX_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SIX_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SIX_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SIX_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SIX_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SIX_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SEVEN_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SEVEN_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SEVEN_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SEVEN_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SEVEN_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SEVEN_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_SEVEN_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_EIGHT_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_EIGHT_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_EIGHT_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_EIGHT_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_EIGHT_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_EIGHT_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_EIGHT_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_NINE_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_NINE_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_NINE_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_NINE_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_NINE_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_NINE_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_NINE_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TEN_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TEN_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TEN_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TEN_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TEN_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TEN_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_TEN_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_ZERO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ELEVEN_ZERO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_TWO in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ELEVEN_TWO has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_FIVE in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ELEVEN_FIVE has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_EIGHT in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ELEVEN_EIGHT has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_ELEVEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ELEVEN_ELEVEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_FOURTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ELEVEN_FOURTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_SEVENTEEN in caslib CKLMSZ20.\n",
      "NOTE: The table DF_ELEVEN_SEVENTEEN has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n"
     ]
    }
   ],
   "source": [
    "data_list = list(all_combs_df.keys())\n",
    "\n",
    "for dfs in data_list:\n",
    "    tmp = all_combs_df[dfs]\n",
    "    tmp = tmp[tmp[\"scope_type\"] == \"ts\"]\n",
    "    tmp = tmp[[\"date\", \"new_adet\", \"ana_kategori_adi\", \"grup_adi\", \"Kanal\", \"kategori_adi\", \"marka_adi\", \"urun_adi\", \"aktivite_tipi\",\n",
    "               \"enflasyon_etkisi\", \"indirim__bins\", \"lockdown\", \"peak\", \"season\", \"trend\"]]\n",
    "    tmp[\"enflasyon_etkisi\"] = tmp[\"date\"].map(enf_dict_fill)\n",
    "    tmp[\"new_adet\"] = tmp[\"new_adet\"].astype(\"float\")\n",
    "    tmp[\"aktivite_tipi\"] = tmp[\"aktivite_tipi\"].astype(\"float\")\n",
    "    tmp[\"indirim__bins\"] = tmp[\"indirim__bins\"].astype(\"float\")\n",
    "    tmp[\"lockdown\"] = tmp[\"lockdown\"].astype(\"float\")\n",
    "    tmp[\"peak\"] = tmp[\"peak\"].astype(\"float\")\n",
    "    tmp[\"date\"] = tmp[\"date\"].apply(lambda x: (x - datetime(1960, 1, 1)).days)\n",
    "    ds_table = conn.CASTable(dfs, caslib='CKLMSZ20', replace=True)\n",
    "    ds_table.table.dropTable(quiet=True)\n",
    "    conn.upload(data=tmp, casout={'caslib':'CKLMSZ20', 'name':dfs, 'promote':True}, \n",
    "                importoptions={'vars':{'date':{'format': 'MMDDYY', \"type\": \"double\", \"length\": 10}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_list = [i.replace(\"df_\", \"ts_results_\") for i in data_list]\n",
    "input_names = pd.DataFrame({\"tables\": data_list})\n",
    "output_names = pd.DataFrame({\"tables\": output_data_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table DS_COMB_INPUT_TABLE_NAMES in caslib CKLMSZ20.\n",
      "NOTE: The table DS_COMB_INPUT_TABLE_NAMES has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DS_COMB_OUTPUT_TABLE_NAMES in caslib CKLMSZ20.\n",
      "NOTE: The table DS_COMB_OUTPUT_TABLE_NAMES has been created in caslib CKLMSZ20 from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CKLMSZ20</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>DS_COMB_OUTPUT_TABLE_NAMES</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('DS_COMB_OUTPUT_TABLE_NAMES', caslib='CKLMSZ20')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.3s</span> &#183; <span class=\"cas-user\">user 0.032s</span> &#183; <span class=\"cas-sys\">sys 0.0498s</span> &#183; <span class=\"cas-memory\">mem 67.1MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'CKLMSZ20'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'DS_COMB_OUTPUT_TABLE_NAMES'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('DS_COMB_OUTPUT_TABLE_NAMES', caslib='CKLMSZ20')\n",
       "\n",
       "+ Elapsed: 0.3s, user: 0.032s, sys: 0.0498s, mem: 67.1mb"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_table = conn.CASTable(\"DS_COMB_INPUT_TABLE_NAMES\", caslib='CKLMSZ20', replace=True)\n",
    "ds_table.table.dropTable(quiet=True)\n",
    "\n",
    "ds_table = conn.CASTable(\"DS_COMB_OUTPUT_TABLE_NAMES\", caslib='CKLMSZ20', replace=True)\n",
    "ds_table.table.dropTable(quiet=True)\n",
    "\n",
    "conn.upload(data=input_names, casout={'caslib':'CKLMSZ20', 'name':'DS_COMB_INPUT_TABLE_NAMES', 'promote':True})\n",
    "conn.upload(data=output_names, casout={'caslib':'CKLMSZ20', 'name':'DS_COMB_OUTPUT_TABLE_NAMES', 'promote':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodun çalışma süresi: 0:25:45.956186\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_time - start_time))\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodun çalışma süresi: 1:13:49.891450\n"
     ]
    }
   ],
   "source": [
    "end_all_process = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_all_process - start_all_process))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
