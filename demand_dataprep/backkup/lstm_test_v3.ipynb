{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tunahan.aktas\\Anaconda3\\lib\\site-packages\\mpl_toolkits\\mplot3d\\__init__.py:1: MatplotlibDeprecationWarning: \n",
      "The deprecated function was deprecated in Matplotlib 3.4 and will be removed two minor releases later.\n",
      "  from .axes3d import Axes3D\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from pandas import read_excel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Prior libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "\n",
    "# To Get Combinatiobs\n",
    "import itertools\n",
    "\n",
    "# Datetime Libraries\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "# Trend Seasonality\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SAS Connection Library\n",
    "import swat\n",
    "\n",
    "# In Order To Read Config File\n",
    "import json\n",
    "\n",
    "# Model Preprocess Librarires\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Machine Learning Algorithm Libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Other Libraries\n",
    "import math\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiprocess as mp\n",
    "from functools import partial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max.columns\", 100)\n",
    "pd.set_option(\"display.max.rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../config_lstm.json\", \"r\")\n",
    "params_ = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_all_process = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Excel Files\n",
    "pas_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"pasifik_satis_path\"]) if doc_.startswith(\"Siparişe_göre_Sales_History\")])\n",
    "hor_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"horizon_satis_path\"]) if doc_.startswith(\"Horizon_Saha_\")])\n",
    "btt_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"btt_satis_path\"]) if doc_.startswith(\"Demand Sensing Sales History\") or doc_.startswith(\"Demand_Sensing_Sales_\")])\n",
    "\n",
    "saha_aktivite_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"horizon_aktivite_path\"]) if doc_.startswith(\"Demand_Sensing_Saha_Aktivit\") or doc_.startswith(\"Demand Sensing Saha Aktivit\")])\n",
    "pasifik_aktivite_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"pasifik_aktivite_path\"]) if doc_.startswith(\"Pasifik Aktivite Datası\")])\n",
    "\n",
    "portfoy_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"portfoy_path\"]) if doc_.startswith(\"Portföy\")])\n",
    "eslenik_kod_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"eslenik_kod_path\"]) if doc_.startswith(\"Ürün Eşlenik kodlar\")])\n",
    "kapsam_listeli = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"kapsam_path\"]) if doc_.startswith(\"Listeli Ürün\")])\n",
    "pas_siparis_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"pasifik_siparis_path\"]) if doc_.startswith(\"Siparişe_göre_Sales_History\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosya Listelerini Okuma İşlemi: 0:00:00.028993\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Dosya Listelerini Okuma İşlemi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Read Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chng_cols_beginning = {'Year': 'Yıl', 'Quarter': 'Çeyrek', 'Month': 'Ay', \n",
    "                       'Company Code': 'Şirket Kodu', 'Main Category Name': 'Ana Kategori Adı', \n",
    "                       'Category Name': 'Kategori Adı', 'Brand Name': 'Marka Adı', 'Product Code': 'Ürün Kodu', \n",
    "                       'Product Name': 'Ürün Adı', \"Ürün Adı (Mobis)\": 'Ürün Adı'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pas(paths_pas, new_col_names, loop_list):\n",
    "    import pandas as pd\n",
    "    print(\"Okumaya başladı.\", loop_list)\n",
    "    new_df = pd.read_excel(paths_pas+loop_list, sheet_name=\"Ürün Bazlı\", usecols=\"B:O\").rename(columns=new_col_names)\n",
    "    print(\"Okuma bitti ---->\", loop_list)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hor(paths_hor, new_col_names, loop_list):\n",
    "    import pandas as pd\n",
    "    print(\"Okumaya başladı.\", loop_list)\n",
    "    new_df = pd.read_excel(paths_hor+loop_list, skiprows=1, sheet_name=\"Horizon Saha Satış\", usecols=\"B:L\").rename(columns=new_col_names)\n",
    "    print(\"Okuma bitti ---->\", loop_list)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_btt(paths_btt, new_col_names, loop_list):\n",
    "    import pandas as pd\n",
    "    print(\"Okumaya başladı.\", loop_list)\n",
    "    new_df = pd.read_excel(paths_btt+loop_list, skiprows=1, sheet_name=\"BTT SAP Satış\", usecols=\"B:N\").rename(columns=new_col_names)\n",
    "    print(\"Okuma bitti ---->\", loop_list)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = []\n",
    "horizon_saha_df_all = []\n",
    "btt_df_all = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support()\n",
    "    available_cpu = mp.cpu_count() - 1\n",
    "    \n",
    "    paths_pas = params_[\"path\"][\"pasifik_satis_path\"]\n",
    "    paths_hor = params_[\"path\"][\"horizon_satis_path\"]\n",
    "    paths_btt = params_[\"path\"][\"btt_satis_path\"]\n",
    "\n",
    "    new_col_names = chng_cols_beginning\n",
    "\n",
    "    loop_pas = pas_lst\n",
    "    loop_hor = hor_lst\n",
    "    loop_btt = btt_lst\n",
    "    \n",
    "    func_pas = partial(read_pas, paths_pas, new_col_names)\n",
    "    func_hor = partial(read_hor, paths_hor, new_col_names)\n",
    "    func_btt = partial(read_btt, paths_btt, new_col_names)\n",
    "    \n",
    "    with mp.Pool(available_cpu) as p:\n",
    "        pasifik_df_all.append(p.map(func_pas, loop_pas))\n",
    "    with mp.Pool(available_cpu) as p:\n",
    "        horizon_saha_df_all.append(p.map(func_hor, loop_hor))\n",
    "    with mp.Pool(available_cpu) as p:\n",
    "        btt_df_all.append(p.map(func_btt, loop_btt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataların yüklenmesi: 0:01:57.076876\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Dataların yüklenmesi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pd.concat(pasifik_df_all[0], ignore_index=True)\n",
    "horizon_saha_df_all = pd.concat(horizon_saha_df_all[0], ignore_index=True)\n",
    "btt_df_all = pd.concat(btt_df_all[0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sadece Gerekli Sütunlar Tutuluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all.drop(columns=[\"Organizasyon Kodu\", \"Grup Kodu.\", \"Pladis-Non Pladis\"], axis=1, inplace=True)\n",
    "horizon_saha_df_all.drop(columns=[\"Çeyrek\"], axis=1, inplace=True)\n",
    "btt_df_all.drop(columns=[\"Çeyrek\", \"Şirket Kodu\"], axis=1, inplace=True)\n",
    "\n",
    "btt_df_all[\"Grup Adı\"] = \"BTT\"\n",
    "pasifik_df_all.rename(columns={\"Ana Kategori\": \"Ana Kategori Adı\", \"Kategori\": \"Kategori Adı\", \"Ürün Adı (Orjinal)\": \"Ürün Adı\", \"Sipariş Miktarı(Dönüş. Koli)\": \"Koli\", \n",
    "                               \"Sipariş Brüt Tutar\": \"KG\", \"Sipariş Brüt KG\": \"TL\"}, inplace=True)\n",
    "\n",
    "horizon_saha_df_all.rename(columns={\"Horizon müşteri grup\": \"Grup Adı\", \"Ürün Adı (Orjinal)\": \"Ürün Adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all.dropna(inplace=True, how=\"any\")\n",
    "horizon_saha_df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['Yıl', 'Ay', 'Grup Adı', 'Ana Kategori Adı', 'Kategori Adı', 'Marka Adı', 'Ürün Kodu', 'Ürün Adı', 'Koli', 'KG', 'TL']\n",
    "ltrs = list(string.ascii_letters) # Alfabede bulunan tüm harfleri tutan liste. Bunu, koli içi adet dataframe'deki harf içeren ürün kodlarını elemek için tutuyoruz.\n",
    "\n",
    "horizon_saha_df_all[\"Kategori Adı\"] = np.nan\n",
    "\n",
    "pasifik_df_all = pasifik_df_all[col_order]\n",
    "horizon_saha_df_all = horizon_saha_df_all[col_order]\n",
    "btt_df_all = btt_df_all[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_aktivite_detay(paths_detay, loop_list):\n",
    "    import pandas as pd\n",
    "    print(\"Okumaya başladı.\", loop_list)\n",
    "    new_df = pd.read_excel(paths_detay+loop_list, skiprows=1, sheet_name=\"Ürün Detay\", usecols=\"B:M\")\n",
    "    print(\"Okuma bitti ---->\", loop_list)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_aktivite_cat(paths_cat, loop_list):\n",
    "    import pandas as pd\n",
    "    print(\"Okumaya başladı.\", loop_list)\n",
    "    new_df = pd.read_excel(paths_cat+loop_list, skiprows=1, sheet_name=\"Kategori\", usecols=\"B:I\")\n",
    "    print(\"Okuma bitti ---->\", loop_list)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay = []\n",
    "saha_aktivite_cat = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support()\n",
    "    available_cpu = mp.cpu_count() - 1\n",
    "    \n",
    "    paths_detay = params_[\"path\"][\"horizon_aktivite_path\"]\n",
    "    paths_cat = params_[\"path\"][\"horizon_aktivite_path\"]\n",
    "\n",
    "    loop_detay = saha_aktivite_lst\n",
    "    loop_cat = saha_aktivite_lst\n",
    "    \n",
    "    func_detay = partial(read_aktivite_detay, paths_detay)\n",
    "    func_cat = partial(read_aktivite_cat, paths_cat)\n",
    "    \n",
    "    with mp.Pool(available_cpu) as p:\n",
    "        saha_aktivite_detay.append(p.map(func_detay, loop_detay))\n",
    "    with mp.Pool(available_cpu) as p:\n",
    "        saha_aktivite_cat.append(p.map(func_cat, loop_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataların yüklenmesi: 0:00:29.651125\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Dataların yüklenmesi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay = pd.concat(saha_aktivite_detay[0], ignore_index=True)\n",
    "saha_aktivite_cat = pd.concat(saha_aktivite_cat[0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay.drop_duplicates(subset=[\"Yıl\", \"Ay\", \"Saha Müşteri Grup\", \"Ürün Kodu\"], keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Pasifik Aktiviteleri\n",
    "pasifik_aktivite_df = pd.read_excel(params_[\"path\"][\"pasifik_aktivite_path\"]+pasifik_aktivite_lst[0])\n",
    "pasifik_aktivite_df.drop_duplicates(subset=[\"Yıl\", \"Ay\", \"Müşteri Grup\", \"Ürün Kodu\"], keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Fiyat Listesi\n",
    "fiyat_lst_pasifik = pd.read_excel(params_[\"path\"][\"fiyat_listesi_path\"]+params_[\"files\"][\"pasifik_fiyat_file\"])\n",
    "fiyat_lst_horizon = pd.read_excel(params_[\"path\"][\"fiyat_listesi_path\"]+params_[\"files\"][\"horizon_fiyat_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Portföy\n",
    "pasifik_portfoy_df = pd.read_excel(params_[\"path\"][\"portfoy_path\"]+portfoy_lst[0], sheet_name=\"Pasifik Portföy\", skiprows=3, usecols=\"D:H\")\n",
    "btt_portfoy_df = pd.read_excel(params_[\"path\"][\"portfoy_path\"]+portfoy_lst[0], sheet_name=\"BTT Portföy\", skiprows=2, usecols=\"D:H\")\n",
    "horizon_portfoy_df = pd.read_excel(params_[\"path\"][\"portfoy_path\"]+portfoy_lst[0], sheet_name=\"Horizon Portföy\", skiprows=2, usecols=\"E:I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Eşlenik Kodları\n",
    "eslenik_kod_df = pd.read_excel(params_[\"path\"][\"eslenik_kod_path\"]+eslenik_kod_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Calender\n",
    "calender_df = pd.read_excel(params_[\"path\"][\"calender_path\"]+params_[\"files\"][\"calender_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eslenik_kod_df[\"En Güncel Kod\"] = eslenik_kod_df[\"En Güncel Kod\"].apply(lambda x: int(x) if x not in ['delist ', \"delist\", \"Delist\"] else x.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a101_kapsam = pd.read_excel(params_[\"path\"][\"kapsam_path\"]+kapsam_listeli[0], sheet_name=\"A101 Portföy\")\n",
    "sok_kapsam = pd.read_excel(params_[\"path\"][\"kapsam_path\"]+kapsam_listeli[0], sheet_name=\"Şok Portföy\")\n",
    "bim_kapsam = pd.read_excel(params_[\"path\"][\"kapsam_path\"]+kapsam_listeli[0], sheet_name=\"Bim Portföy\")\n",
    "\n",
    "a101_kapsam[\"grup_adi\"] = \"A101\"\n",
    "sok_kapsam[\"grup_adi\"] = \"ŞOK\"\n",
    "bim_kapsam[\"grup_adi\"] = \"BİM\"\n",
    "\n",
    "kapsam_all = pd.concat([a101_kapsam, sok_kapsam, bim_kapsam], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Koli Birim Ağırlıkları\n",
    "koli_birim_agirlik = pd.read_excel(params_[\"files\"][\"koli_agirlik_birim_file\"])\n",
    "koli_birim_agirlik_pas = pd.read_excel(params_[\"files\"][\"koli_agirlik_birim_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "koli_birim_agirlik.rename(columns={\"Malzeme\": \"en_guncel_kod\", \n",
    "                                   \"Malzeme Açıklaması\": \"urun_adi\", \n",
    "                                   \"Ana Kategori\": \"ana_kategori_adi\",\n",
    "                                   \"Kategori\": \"kategori_adi\",\n",
    "                                   \"Alt Kategori\": \"alt_kategori_adi\",\n",
    "                                   \"Detay Kategori\": \"detay_kategori_adi\",\n",
    "                                   \"Marka\": \"marka_adi\", \"Marka Açıklaması\": \"marka_aciklamasi\",\n",
    "                                   \"Net Ağırlık\": \"kg\"}, inplace=True)\n",
    "\n",
    "koli_birim_agirlik_pas.rename(columns={\"Malzeme\": \"en_guncel_kod\", \n",
    "                                   \"Malzeme Açıklaması\": \"urun_adi\", \n",
    "                                   \"Ana Kategori\": \"ana_kategori_adi\",\n",
    "                                   \"Kategori\": \"kategori_adi\",\n",
    "                                   \"Alt Kategori\": \"alt_kategori_adi\",\n",
    "                                   \"Detay Kategori\": \"detay_kategori_adi\",\n",
    "                                   \"Marka\": \"marka_adi\", \"Marka Açıklaması\": \"marka_aciklamasi\",\n",
    "                                   \"Net Ağırlık\": \"kg\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "koli_birim_agirlik_pas[\"kanal\"] = \"pasifik\"\n",
    "koli_birim_agirlik_hor = koli_birim_agirlik.copy()\n",
    "koli_birim_agirlik_btt = koli_birim_agirlik.copy()\n",
    "koli_birim_agirlik_hor[\"kanal\"] = \"horizon\"\n",
    "koli_birim_agirlik_btt[\"kanal\"] = \"btt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "koli_birim_agirlik = pd.concat([koli_birim_agirlik_pas, koli_birim_agirlik_hor, koli_birim_agirlik_btt], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diğer dataların Jupytere yüklenme süresi: 0:00:33.579006\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Diğer dataların Jupytere yüklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifik 2016 aktivite verileri olmadığı için 2016 Sales dataları çıkartıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[pasifik_df_all[\"Yıl\"] != 2016].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all[\"Yıl\"] = horizon_saha_df_all[\"Yıl\"].astype(int)\n",
    "horizon_saha_df_all[\"Ay\"] = horizon_saha_df_all[\"Ay\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sales Datası İçin Ürün Kod Eşleme\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pd.merge(pasifik_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "horizon_saha_df_all = pd.merge(horizon_saha_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "btt_df_all = pd.merge(btt_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ürün Eşleme Kodları dosyasında yer almayan kodlar için mevcut ürün kodları verildi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_ = pasifik_df_all[pd.isnull(pasifik_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = pasifik_df_all[~pd.isnull(pasifik_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "pasifik_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "pasifik_df_all = pasifik_df_all.sort_values(pasifik_df_all.columns.to_list()).reset_index(drop=True)\n",
    "\n",
    "empty_ = btt_df_all[pd.isnull(btt_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = btt_df_all[~pd.isnull(btt_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "btt_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "btt_df_all = btt_df_all.sort_values(btt_df_all.columns.to_list()).reset_index(drop=True)\n",
    "\n",
    "empty_ = horizon_saha_df_all[pd.isnull(horizon_saha_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = horizon_saha_df_all[~pd.isnull(horizon_saha_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "horizon_saha_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all.sort_values(horizon_saha_df_all.columns.to_list()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adet adında yeni bir kolon oluşturuldu. Koli Sayısı 100'den az olanlara 0 yazıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all[\"Koli\"] = np.where(pasifik_df_all[\"Koli\"] < 100, 0, pasifik_df_all[\"Koli\"])\n",
    "btt_df_all[\"Koli\"] = np.where(btt_df_all[\"Koli\"] < 100, 0, btt_df_all[\"Koli\"])\n",
    "horizon_saha_df_all[\"Koli\"] = np.where(horizon_saha_df_all[\"Koli\"] < 100, 0, horizon_saha_df_all[\"Koli\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[pasifik_df_all[\"Koli\"] != 0].reset_index(drop=True)\n",
    "btt_df_all = btt_df_all[btt_df_all[\"Koli\"] != 0].reset_index(drop=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all[horizon_saha_df_all[\"Koli\"] != 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delist olan ürünler veriden çıkartıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[pasifik_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "btt_df_all = btt_df_all[btt_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all[horizon_saha_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aynı yıl, ay, grup adı, ana kategori adı, kategori adı, marka adı ve SKU kodundaki ürünler için toplam alındı. Sadece Koli İçi Adet için maksimum olan alındı."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marka adı dahil değil groupby'a\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_to_sum = {\"Koli\": \"sum\", \"KG\": \"sum\", \"TL\": \"sum\"}\n",
    "\n",
    "pasifik_df_all2 = pasifik_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()\n",
    "btt_df_all2 = btt_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()\n",
    "horizon_saha_df_all2 = horizon_saha_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2[\"Date\"] = pasifik_df_all2[\"Yıl\"].astype(str) + \"-\" +  pasifik_df_all2[\"Ay\"].astype(str) + \"-01\"\n",
    "btt_df_all2[\"Date\"] = btt_df_all2[\"Yıl\"].astype(int).astype(str) + \"-\" +  btt_df_all2[\"Ay\"].astype(int).astype(str) + \"-01\"\n",
    "horizon_saha_df_all2[\"Date\"] = horizon_saha_df_all2[\"Yıl\"].astype(int).astype(str) + \"-\" +  horizon_saha_df_all2[\"Ay\"].astype(int).astype(str) + \"-01\"\n",
    "\n",
    "pasifik_df_all2[\"Date\"] = pd.to_datetime(pasifik_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "btt_df_all2[\"Date\"] = pd.to_datetime(btt_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "horizon_saha_df_all2[\"Date\"] = pd.to_datetime(horizon_saha_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40539, 10), (166217, 9), (13775, 10))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pasifik_df_all2.shape, horizon_saha_df_all2.shape, btt_df_all2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon ve Pasifikte bulunan \"Diğer\"'lerin yanlarına \"_\" ile Diğer_Pasifik, Diğer_Horizon yazıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2[\"Grup Adı\"] = pasifik_df_all2[\"Grup Adı\"].apply(lambda x: \"Diğer_Pasifik\" if x == \"Diğer\" else x)\n",
    "horizon_saha_df_all2[\"Grup Adı\"] = horizon_saha_df_all2[\"Grup Adı\"].apply(lambda x: \"Diğer_Horizon\" if x == \"Diğer\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all2 = pd.concat([pasifik_df_all2, horizon_saha_df_all2, btt_df_all2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params_[\"time_info_for_debugging\"][\"ay\"] == 12:\n",
    "    ay_threshold = params_[\"time_info_for_debugging\"][\"ay\"] = 1\n",
    "    yil_threshold = params_[\"time_info_for_debugging\"][\"yil\"] + 1\n",
    "else:\n",
    "    ay_threshold = params_[\"time_info_for_debugging\"][\"ay\"] + 1\n",
    "    yil_threshold = params_[\"time_info_for_debugging\"][\"yil\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifik Filling Missing Dates\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_dates(df__, df_unique_list):\n",
    "    df_all_filled = []\n",
    "    for idx in df_unique_list.index:\n",
    "        tmp_df = df__[(df__[\"Grup Adı\"] == df_unique_list[\"Grup Adı\"][idx]) & (df__[\"En Güncel Kod\"] == df_unique_list[\"En Güncel Kod\"][idx])]\n",
    "        time_interval = []\n",
    "        dt = tmp_df.Date.min()\n",
    "        while dt <= datetime(params_[\"time_info_for_debugging\"][\"yil\"], params_[\"time_info_for_debugging\"][\"ay\"], 1):\n",
    "            time_interval.append(dt)\n",
    "            dt += relativedelta(months=1)\n",
    "        date_to_add = [i for i in time_interval if i not in tmp_df.Date.unique()]\n",
    "        if len(date_to_add) > 0:\n",
    "            add_df = pd.concat([pd.DataFrame(tmp_df.iloc[0]).T]*len(date_to_add))\n",
    "            add_df[\"Koli\"], add_df[\"KG\"], add_df[\"TL\"] = 0, 0, 0\n",
    "            add_df[\"Date\"] = date_to_add\n",
    "            add_df[\"Yıl\"] = add_df[\"Date\"].dt.year\n",
    "            add_df[\"Ay\"] = add_df[\"Date\"].dt.month\n",
    "            df_all_filled.append(pd.concat([tmp_df, add_df], ignore_index=True))\n",
    "        else: \n",
    "            df_all_filled.append(tmp_df)\n",
    "    return pd.concat(df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = pasifik_df_all2.drop_duplicates(subset=[\"Grup Adı\", \"En Güncel Kod\"], ignore_index=True)[[\"Grup Adı\", \"En Güncel Kod\"]]\n",
    "pasifik_df_all2 = fill_missing_dates(pasifik_df_all2, unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik missing value düzenlenmesi süresi: 0:00:23.076819\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Horizon Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = horizon_saha_df_all2.drop_duplicates(subset=[\"Grup Adı\", \"En Güncel Kod\"], ignore_index=True)[[\"Grup Adı\", \"En Güncel Kod\"]]\n",
    "horizon_saha_df_all2 = fill_missing_dates(horizon_saha_df_all2, unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon missing value düzenlenmesi süresi: 0:04:03.798419\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# BTT Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = btt_df_all2.drop_duplicates(subset=[\"Grup Adı\", \"En Güncel Kod\"], ignore_index=True)[[\"Grup Adı\", \"En Güncel Kod\"]]\n",
    "btt_df_all2 = fill_missing_dates(btt_df_all2, unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTT missing value düzenlenmesi süresi: 0:00:04.046004\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all22 = pd.concat([pasifik_df_all2, horizon_saha_df_all2, btt_df_all2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aktivite Datası İçin Ürün Kod Eşleme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik Aktivite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left join ile güncel kodlar getirildi. Delist olan ürünler listeden çıkartıldı. \"Çeyrek\" sütunu silindi. En güncel kod sütunnuda bulunamayan değerler Ürün Kodu sütunundan çekildi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_aktivite_df2 = pd.merge(pasifik_aktivite_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "pasifik_aktivite_df2 = pasifik_aktivite_df2[pasifik_aktivite_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "pasifik_aktivite_df2.drop(\"Çeyrek\", inplace=True, axis=1)\n",
    "pasifik_aktivite_df2['En Güncel Kod'] = pasifik_aktivite_df2['En Güncel Kod'].fillna(pasifik_aktivite_df2['Ürün Kodu'])\n",
    "pasifik_aktivite_df2.drop(columns=\"Ürün Kodu\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik Aktivite Ciro - Promosyon Tutarı ve İskonto Tekilleştirme (ORTALAMA ALARAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_pas = {\"Raf Tavsiye Satış Fiyatı\": \"mean\", \"İndirimli Raf Satış Fiyatı\": \"mean\", \"İndirim %\": \"mean\", \"Aktivite Tipi\": \"first\"}\n",
    "pasifik_aktivite_df3 = pasifik_aktivite_df2.groupby([\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Müşteri Grup\"]).agg(ort_pas).reset_index()\n",
    "pasifik_aktivite_df3 = pd.merge(pasifik_aktivite_df3, pasifik_aktivite_df2[[\"Yıl\", \"Ay\", \"Müşteri Grup\", \"En Güncel Kod\", \n",
    "                                                                            \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\"]],\n",
    "                                how=\"left\", \n",
    "                                on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Müşteri Grup\"])\n",
    "\n",
    "pasifik_aktivite_df3.drop_duplicates(subset=pasifik_aktivite_df3.columns.to_list(), inplace=True)\n",
    "pasifik_aktivite_df3.reset_index(drop=True, inplace=True)\n",
    "pasifik_aktivite_df3 = pasifik_aktivite_df3[pasifik_aktivite_df2.drop(\"Ürün Adı\", axis=1).columns.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon Aktivite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay2 = pd.merge(saha_aktivite_detay, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "saha_aktivite_detay2 = saha_aktivite_detay2[saha_aktivite_detay2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "saha_aktivite_detay2.drop(\"Çeyrek\", inplace=True, axis=1)\n",
    "saha_aktivite_detay2['En Güncel Kod'] = saha_aktivite_detay2['En Güncel Kod'].fillna(saha_aktivite_detay2['Ürün Kodu'])\n",
    "saha_aktivite_detay2.drop(columns=\"Ürün Kodu\", axis=1, inplace=True)\n",
    "saha_aktivite_detay2[\"İskonto %\"].replace(\"#DIV/0\", np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon Aktivite Ciro - Promosyon Tutarı ve İskonto Tekilleştirme (ORTALAMA ALARAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort = {\"Ciro (Kull. İade Düş.)\": \"mean\", \"Promosyon Tutarı\": \"mean\", \"İskonto %\": \"mean\"}\n",
    "saha_aktivite_detay3 = saha_aktivite_detay2.groupby([\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Saha Müşteri Grup\"]).agg(ort).reset_index()\n",
    "\n",
    "saha_aktivite_detay3 = pd.merge(saha_aktivite_detay3, saha_aktivite_detay2[[\"Yıl\", \"Ay\", \"Saha Müşteri Grup\", \"En Güncel Kod\", \n",
    "                                                 \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\"]],\n",
    "                           how=\"left\", \n",
    "                           on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Saha Müşteri Grup\"])\n",
    "\n",
    "saha_aktivite_detay3.drop_duplicates(subset=saha_aktivite_detay3.columns.to_list(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3 = saha_aktivite_detay3[saha_aktivite_detay2.drop(\"Ürün Adı (Mobis)\", axis=1).columns.to_list()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3.rename(columns={\"Saha Müşteri Grup\": \"Grup Adı\"}, inplace=True)\n",
    "saha_aktivite_detay3[\"Grup Adı\"] = saha_aktivite_detay3[\"Grup Adı\"].apply(lambda x: \"Diğer_Horizon\" if x == \"Diğer\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiyat Listesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Fiyatları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon.drop_duplicates(subset=fiyat_lst_horizon.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].str.replace(\"-\", \"\")\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].astype(int)*1\n",
    "except:\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].astype(int)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df = fiyat_lst_horizon.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df[\"Baslangic_Yıl\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.year)\n",
    "fiyat_lst_horizon_df[\"Baslangic_Ay\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.month)\n",
    "fiyat_lst_horizon_df[\"Baslangic_Gun\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.day)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.year)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Ay\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.month)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Gun\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df[\"Baslangic_Yıl\"] = fiyat_lst_horizon_df[\"Baslangic_Yıl\"].apply(lambda x: (horizon_saha_df_all2[\"Date\"].max().year)+1 if x > horizon_saha_df_all2[\"Date\"].max().year else x)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"] = fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"].apply(lambda x: (horizon_saha_df_all2[\"Date\"].max().year)+1 if x > horizon_saha_df_all2[\"Date\"].max().year else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = horizon_saha_df_all2[\"Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fiyat_unique = []\n",
    "\n",
    "for malzeme in fiyat_lst_horizon_df[\"Malzeme\"].unique():\n",
    "    temp_time_df = pd.DataFrame({\"Fiyat\": [np.nan]}, index=time_index)\n",
    "    temp_time_df = temp_time_df.reset_index().rename(columns={\"index\":\"date\"})    \n",
    "    temp_time_df[\"En Güncel Kod\"] = malzeme\n",
    "    temp_time_df[\"fiyat_gecisi\"] = 0\n",
    "    malzeme_df = fiyat_lst_horizon_df[fiyat_lst_horizon_df[\"Malzeme\"] == malzeme].reset_index(drop=True)\n",
    "    malzeme_df.drop(columns=[\"KşTü\", \"Koşul türü\", \"Tanım\", \"Ana Kategori\", \"Kategori\", \"ÖB\"], axis=1, inplace=True)\n",
    "    malzeme_df.drop_duplicates(subset=malzeme_df.columns.to_list(), inplace=True, ignore_index=True)\n",
    "    malzeme_df.sort_values(by=[\"Baslangic_Yıl\", \"Baslangic_Ay\", \"Baslangic_Gun\"], ignore_index=True, inplace=True)\n",
    "    check_idx1 = []\n",
    "    if len(malzeme_df) > 1:\n",
    "        for row1 in malzeme_df.index:\n",
    "            for row2 in malzeme_df[row1+1:].index:\n",
    "                if (malzeme_df.loc[row1][\"Gecerlilik_Yıl\"] == malzeme_df.loc[row2][\"Baslangic_Yıl\"]) and (malzeme_df.loc[row1][\"Gecerlilik_Ay\"] == malzeme_df.loc[row2][\"Baslangic_Ay\"]):\n",
    "                    num_days = calendar.monthrange(int(malzeme_df.loc[row2][\"Baslangic_Yıl\"]), int(malzeme_df.loc[row2][\"Baslangic_Ay\"]))[1]\n",
    "                    fyt=((int(malzeme_df.loc[row1][\"Gecerlilik_Gun\"])*malzeme_df.loc[row1][\"     Tutar\"]) + (num_days - int(malzeme_df.loc[row2][\"Baslangic_Gun\"]) + 1)*malzeme_df.loc[row2][\"     Tutar\"])/num_days\n",
    "\n",
    "                    end_idx1 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx1 = temp_time_df[temp_time_df[\"date\"] == end_idx1].index\n",
    "                    temp_time_df.loc[final_idx1, \"Fiyat\"] = fyt\n",
    "                    temp_time_df.loc[final_idx1,\"fiyat_gecisi\"] = 1\n",
    "\n",
    "                elif (malzeme_df.loc[row1, \"Gecerlilik_Gun\"] == calendar.monthrange(int(malzeme_df.loc[row1][\"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1][\"Gecerlilik_Ay\"]))[1] \\\n",
    "                     and malzeme_df.loc[row2, \"Baslangic_Gun\"] == 1):\n",
    "                    fyt5=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                    fyt6=malzeme_df.loc[row2][\"     Tutar\"]\n",
    "                    end_idx5 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    end_idx6 =  datetime(int(malzeme_df.loc[row2, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row2, \"Baslangic_Ay\"]), 1)\n",
    "                    final_idx5 = temp_time_df[temp_time_df[\"date\"] == end_idx5].index\n",
    "                    final_idx6 = temp_time_df[temp_time_df[\"date\"] == end_idx6].index\n",
    "                    temp_time_df.loc[final_idx5, \"Fiyat\"] = fyt5\n",
    "                    temp_time_df.loc[final_idx6, \"Fiyat\"] = fyt6\n",
    "\n",
    "                else:\n",
    "                    fyt2=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                    start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx2 = temp_time_df[(temp_time_df[\"date\"] > start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                    temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "            if (row1 == len(malzeme_df)-1) or (row1 == len(malzeme_df)-2):\n",
    "                fyt3=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                final_idx3 = temp_time_df[(temp_time_df[\"date\"] > start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "\n",
    "    else:\n",
    "        for row1 in malzeme_df.index:\n",
    "            fyt4=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "            start_idx4 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "            end_idx4 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "            final_idx4 = temp_time_df[(temp_time_df[\"date\"] >= start_idx4) & (temp_time_df[\"date\"] <= end_idx4)].index\n",
    "            temp_time_df.loc[final_idx4, \"Fiyat\"] = fyt4\n",
    "\n",
    "    if (malzeme_df.loc[0, \"Baslangic_Yıl\"] >= min(horizon_saha_df_all2[\"Yıl\"].unique())) and (len(malzeme_df) > 1):\n",
    "        temp_time_df.loc[temp_time_df[~pd.isnull(temp_time_df[\"Fiyat\"])].index[0]-1, \"Fiyat\"] = malzeme_df.loc[0, \"     Tutar\"]\n",
    "    temp_time_df = temp_time_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    h_fiyat_unique.append(temp_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fiyat_unique = pd.concat(h_fiyat_unique)\n",
    "h_fiyat_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "h_fiyat_unique.rename(columns={\"En Güncel Kod\": \"Ürün Kodu\", \"date\": \"Date\"}, inplace=True)\n",
    "h_fiyat_unique = h_fiyat_unique.merge(eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\")\n",
    "h_fiyat_unique[\"En Güncel Kod\"].fillna(h_fiyat_unique[\"Ürün Kodu\"], inplace=True)\n",
    "h_fiyat_unique = h_fiyat_unique[h_fiyat_unique[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "h_fiyat_unique = h_fiyat_unique.sort_values(by=[\"En Güncel Kod\", \"Date\"]).reset_index(drop=True)\n",
    "h_fiyat_unique = h_fiyat_unique.drop(columns=\"Ürün Kodu\", axis=1)\n",
    "# Aynı aya denk gelen ürünlerin fiyatlarının ortalaması alınıp, herhangi birinde fiyat geçişi varsa 1 alınır.\n",
    "h_fiyat_unique = h_fiyat_unique.groupby([\"Date\", \"En Güncel Kod\"]).agg({\"Fiyat\": \"mean\", \"fiyat_gecisi\": \"max\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon fiyatların düzenlenmesi süresi: 0:01:35.681047\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon fiyatların düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Fiyatları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik.drop_duplicates(subset=fiyat_lst_pasifik.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik[\"Malzeme\"] = fiyat_lst_pasifik[\"Malzeme\"].str.replace(\"-\", \"\")\n",
    "fiyat_lst_pasifik[\"Malzeme\"] = fiyat_lst_pasifik[\"Malzeme\"].astype(int)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df = fiyat_lst_pasifik.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df[\"Baslangic_Yıl\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.year)\n",
    "fiyat_lst_pasifik_df[\"Baslangic_Ay\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.month)\n",
    "fiyat_lst_pasifik_df[\"Baslangic_Gun\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.day)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.year)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Ay\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.month)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Gun\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df[\"Baslangic_Yıl\"] = fiyat_lst_pasifik_df[\"Baslangic_Yıl\"].apply(lambda x: (pasifik_df_all2[\"Date\"].max().year)+1 if x > pasifik_df_all2[\"Date\"].max().year else x)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"] = fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"].apply(lambda x: (pasifik_df_all2[\"Date\"].max().year)+1 if x > pasifik_df_all2[\"Date\"].max().year else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = pasifik_df_all2[\"Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique = []\n",
    "\n",
    "for malzeme in fiyat_lst_pasifik_df[\"Malzeme\"].unique():\n",
    "    temp_time_df = pd.DataFrame({\"Fiyat\": [np.nan]}, index=time_index)\n",
    "    temp_time_df = temp_time_df.reset_index().rename(columns={\"index\":\"date\"})    \n",
    "    temp_time_df[\"En Güncel Kod\"] = malzeme\n",
    "    temp_time_df[\"fiyat_gecisi\"] = 0\n",
    "    malzeme_df = fiyat_lst_pasifik_df[fiyat_lst_pasifik_df[\"Malzeme\"] == malzeme].reset_index(drop=True)\n",
    "    malzeme_df.drop(columns=[\"KşTü\", \"KşTü.1\", \"Malzeme Tanım\", \"Ana Kategori\", \"Kategori\"], axis=1, inplace=True)\n",
    "    malzeme_df.drop_duplicates(subset=malzeme_df.columns.to_list(), inplace=True, ignore_index=True)\n",
    "    malzeme_df.sort_values(by=[\"Baslangic_Yıl\", \"Baslangic_Ay\", \"Baslangic_Gun\"], ignore_index=True, inplace=True)\n",
    "    check_idx1 = []\n",
    "    if len(malzeme_df) > 1:\n",
    "        for row1 in malzeme_df.index:\n",
    "            for row2 in malzeme_df[row1+1:].index:\n",
    "                if (malzeme_df.loc[row1][\"Gecerlilik_Yıl\"] == malzeme_df.loc[row2][\"Baslangic_Yıl\"]) and (malzeme_df.loc[row1][\"Gecerlilik_Ay\"] == malzeme_df.loc[row2][\"Baslangic_Ay\"]):\n",
    "                    num_days = calendar.monthrange(int(malzeme_df.loc[row2][\"Baslangic_Yıl\"]), int(malzeme_df.loc[row2][\"Baslangic_Ay\"]))[1]\n",
    "                    fyt=((int(malzeme_df.loc[row1][\"Gecerlilik_Gun\"])*malzeme_df.loc[row1][\"Koli TL\"]) + (num_days - int(malzeme_df.loc[row2][\"Baslangic_Gun\"])+1)*malzeme_df.loc[row2][\"Koli TL\"])/num_days\n",
    "\n",
    "                    end_idx1 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx1 = temp_time_df[temp_time_df[\"date\"] == end_idx1].index\n",
    "                    temp_time_df.loc[final_idx1, \"Fiyat\"] = fyt\n",
    "                    temp_time_df.loc[final_idx1,\"fiyat_gecisi\"] = 1\n",
    "\n",
    "                elif (malzeme_df.loc[row1, \"Gecerlilik_Gun\"] == calendar.monthrange(int(malzeme_df.loc[row1][\"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1][\"Gecerlilik_Ay\"]))[1] \\\n",
    "                     and malzeme_df.loc[row2, \"Baslangic_Gun\"] == 1):\n",
    "                    fyt5=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    fyt6=malzeme_df.loc[row2][\"Koli TL\"]\n",
    "                    end_idx5 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    end_idx6 =  datetime(int(malzeme_df.loc[row2, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row2, \"Baslangic_Ay\"]), 1)\n",
    "                    final_idx5 = temp_time_df[temp_time_df[\"date\"] == end_idx5].index\n",
    "                    final_idx6 = temp_time_df[temp_time_df[\"date\"] == end_idx6].index\n",
    "                    temp_time_df.loc[final_idx5, \"Fiyat\"] = fyt5\n",
    "                    temp_time_df.loc[final_idx6, \"Fiyat\"] = fyt6\n",
    "\n",
    "\n",
    "                else:\n",
    "                    if malzeme_df.loc[row1, \"Baslangic_Gun\"] != 1:\n",
    "                        fyt2=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                        start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                        end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                        final_idx2 = temp_time_df[(temp_time_df[\"date\"] > start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                        temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "                    else:\n",
    "                        fyt2=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                        start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                        end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                        final_idx2 = temp_time_df[(temp_time_df[\"date\"] >= start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                        temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "                        \n",
    "            if (row1 == len(malzeme_df)-1) or (row1 == len(malzeme_df)-2):\n",
    "                if malzeme_df.loc[row1, \"Baslangic_Gun\"] != 1:\n",
    "                    fyt3=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx3 = temp_time_df[(temp_time_df[\"date\"] > start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                    temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "                else:\n",
    "                    fyt3=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx3 = temp_time_df[(temp_time_df[\"date\"] >= start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                    temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "    else:\n",
    "        for row1 in malzeme_df.index:\n",
    "            fyt4=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "            start_idx4 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "            end_idx4 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "            final_idx4 = temp_time_df[(temp_time_df[\"date\"] >= start_idx4) & (temp_time_df[\"date\"] <= end_idx4)].index\n",
    "            temp_time_df.loc[final_idx4, \"Fiyat\"] = fyt4\n",
    "    \n",
    "    if (malzeme_df.loc[0, \"Baslangic_Yıl\"] >= min(pasifik_df_all2[\"Yıl\"].unique())) and (len(malzeme_df) > 1):\n",
    "        temp_time_df.loc[temp_time_df[~pd.isnull(temp_time_df[\"Fiyat\"])].index[0]-1, \"Fiyat\"] = malzeme_df.loc[0, \"Koli TL\"]\n",
    "    temp_time_df = temp_time_df.dropna().reset_index(drop=True)\n",
    "        \n",
    "    p_fiyat_unique.append(temp_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique = pd.concat(p_fiyat_unique)\n",
    "p_fiyat_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "p_fiyat_unique.rename(columns={\"En Güncel Kod\": \"Ürün Kodu\", \"date\": \"Date\"}, inplace=True)\n",
    "p_fiyat_unique = p_fiyat_unique.merge(eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\")\n",
    "p_fiyat_unique[\"En Güncel Kod\"].fillna(p_fiyat_unique[\"Ürün Kodu\"], inplace=True)\n",
    "p_fiyat_unique = p_fiyat_unique[p_fiyat_unique[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "p_fiyat_unique = p_fiyat_unique.sort_values(by=[\"En Güncel Kod\", \"Date\"]).reset_index(drop=True)\n",
    "p_fiyat_unique = p_fiyat_unique.drop(columns=\"Ürün Kodu\", axis=1)\n",
    "# Aynı aya denk gelen ürünlerin fiyatlarının ortalaması alınıp, herhangi birinde fiyat geçişi varsa 1 alınır.\n",
    "p_fiyat_unique = p_fiyat_unique.groupby([\"Date\", \"En Güncel Kod\"]).agg({\"Fiyat\": \"mean\", \"fiyat_gecisi\": \"max\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik fiyatların düzenlenmesi süresi: 0:01:24.596799\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik fiyatların düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Portföy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_portfoy_df2 = pd.merge(pasifik_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "pasifik_portfoy_df2[\"En Güncel Kod\"] = pasifik_portfoy_df2[\"En Güncel Kod\"].fillna(pasifik_portfoy_df2[\"Kod\"])\n",
    "pasifik_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "pasifik_portfoy_df2 = pasifik_portfoy_df2[pasifik_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "pasifik_portfoy_df2[\"Portfoy\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_portfoy_df2 = pd.merge(horizon_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "horizon_portfoy_df2[\"En Güncel Kod\"] = horizon_portfoy_df2[\"En Güncel Kod\"].fillna(horizon_portfoy_df2[\"Kod\"])\n",
    "horizon_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "horizon_portfoy_df2 = horizon_portfoy_df2[horizon_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "horizon_portfoy_df2[\"Portfoy\"] = 1\n",
    "horizon_portfoy_df2 = horizon_portfoy_df2[~((horizon_portfoy_df2[\"Kod\"] == 135901))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTT Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_portfoy_df2 = pd.merge(btt_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "btt_portfoy_df2[\"En Güncel Kod\"] = btt_portfoy_df2[\"En Güncel Kod\"].fillna(btt_portfoy_df2[\"Kod\"])\n",
    "btt_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "btt_portfoy_df2 = btt_portfoy_df2[btt_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "btt_portfoy_df2[\"Portfoy\"] = 1\n",
    "btt_portfoy_df2 = btt_portfoy_df2[~((btt_portfoy_df2[\"Kod\"] == 135901))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portföy Kapsamındaki Sales Dataları"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pd.merge(pasifik_df_all2,pasifik_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "btt_df_all3 = pd.merge(btt_df_all2,btt_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "horizon_saha_df_all3 = pd.merge(horizon_saha_df_all2, horizon_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "pasifik_df_all3[\"Portfoy\"].fillna(0, inplace=True)\n",
    "btt_df_all3[\"Portfoy\"].fillna(0, inplace=True)\n",
    "horizon_saha_df_all3[\"Portfoy\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all3.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datalara Calender Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Jan 2016\n",
       "1     Feb 2016\n",
       "2     Mar 2016\n",
       "3     Apr 2016\n",
       "4     May 2016\n",
       "5     Jun 2016\n",
       "6     Jul 2016\n",
       "7     Aug 2016\n",
       "8     Sep 2016\n",
       "9     Oct 2016\n",
       "10    Nov 2016\n",
       "11    Dec 2016\n",
       "12    Jan 2017\n",
       "13    Feb 2017\n",
       "14    Mar 2017\n",
       "15    Apr 2017\n",
       "16    May 2017\n",
       "17    Jun 2017\n",
       "18    Jul 2017\n",
       "19    Aug 2017\n",
       "20    Sep 2017\n",
       "21    Oct 2017\n",
       "22    Nov 2017\n",
       "23    Dec 2017\n",
       "24    Jan 2018\n",
       "25    Feb 2018\n",
       "26    Mar 2018\n",
       "27    Apr 2018\n",
       "28    May 2018\n",
       "29    Jun 2018\n",
       "30    Jul 2018\n",
       "31    Aug 2018\n",
       "32    Sep 2018\n",
       "33    Oct 2018\n",
       "34    Nov 2018\n",
       "35    Dec 2018\n",
       "36    Jan 2019\n",
       "37    Feb 2019\n",
       "38    Mar 2019\n",
       "39    Apr 2019\n",
       "40    May 2019\n",
       "41    Jun 2019\n",
       "42    Jul 2019\n",
       "43    Aug 2019\n",
       "44    Sep 2019\n",
       "45    Oct 2019\n",
       "46    Nov 2019\n",
       "47    Dec 2019\n",
       "48    Jan 2020\n",
       "49    Feb 2020\n",
       "50    Mar 2020\n",
       "51    Apr 2020\n",
       "52    May 2020\n",
       "53    Jun 2020\n",
       "54    Jul 2020\n",
       "55    Aug 2020\n",
       "56    Sep 2020\n",
       "57    Oct 2020\n",
       "58    Nov 2020\n",
       "59    Dec 2020\n",
       "60    Jan 2021\n",
       "61    Feb 2021\n",
       "62    Mar 2021\n",
       "63    Apr 2021\n",
       "64    May 2021\n",
       "65    Jun 2021\n",
       "66    Jul 2021\n",
       "67    Aug 2021\n",
       "68    Sep 2021\n",
       "69    Oct 2021\n",
       "70    Nov 2021\n",
       "71    Dec 2021\n",
       "72    Jan 2022\n",
       "73    Feb 2022\n",
       "74    Mar 2022\n",
       "75    Apr 2022\n",
       "76    May 2022\n",
       "77    Jun 2022\n",
       "78    Jul 2022\n",
       "79    Aug 2022\n",
       "80    Sep 2022\n",
       "81    Oct 2022\n",
       "82    Nov 2022\n",
       "83    Dec 2022\n",
       "84    Jan 2023\n",
       "85    Feb 2023\n",
       "86    Mar 2023\n",
       "87    Apr 2023\n",
       "88    May 2023\n",
       "89    Jun 2023\n",
       "90    Jul 2023\n",
       "91    Aug 2023\n",
       "92    Sep 2023\n",
       "93    Oct 2023\n",
       "94    Nov 2023\n",
       "95    Dec 2023\n",
       "Name: DATE, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calender_df.pop(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pd.merge(pasifik_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")\n",
    "btt_df_all3 = pd.merge(btt_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")\n",
    "horizon_saha_df_all3 = pd.merge(horizon_saha_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataların Fiyat Ve Aktiviteler İle Birleştirilmesi\n",
    "---\n",
    "\n",
    "# Not:\n",
    "---\n",
    "### 1) BTT aktivite verisi için Horizon kısmındaki \"Geleneksel Kanal\" kullanılması istendi.\n",
    "### 2) BTT fiyat geçişleri için Horizon fiyat geçişleri baz alındı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60406, 28), (312746, 27), (16566, 28))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pasifik_df_all3.shape, horizon_saha_df_all3.shape, btt_df_all3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pasifik_df_all3.merge(p_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "pasifik_aktivite_df3.rename(columns={\"Müşteri Grup\": \"Grup Adı\", \"Grup adı\": \"Grup Adı\"}, inplace=True)\n",
    "pasifik_df_all3 = pd.merge(pasifik_df_all3, pasifik_aktivite_df3[[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\", \n",
    "                                                                  \"Raf Tavsiye Satış Fiyatı\", \"İndirimli Raf Satış Fiyatı\", \"İndirim %\",\n",
    "                                                                  \"Aktivite Tipi\"]], \n",
    "                           left_on=[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\"], \n",
    "                           right_on=[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3.rename(columns={\"Grup adı\": \"Grup Adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(h_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(saha_aktivite_detay3[['Ciro (Kull. İade Düş.)', 'Promosyon Tutarı', \n",
    "                                                                        'İskonto %', 'En Güncel Kod', \"Yıl\", \"Ay\", \"Grup Adı\"]],\n",
    "                                                  on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Grup Adı\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all3.drop_duplicates(ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_all3 = btt_df_all3.merge(h_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "btt_aktivite = saha_aktivite_detay3[saha_aktivite_detay3[\"Grup Adı\"] == \"GELENEKSEL KANAL\"].reset_index(drop=True)\n",
    "\n",
    "btt_df_all3 = btt_df_all3.merge(btt_aktivite[['Ciro (Kull. İade Düş.)', 'Promosyon Tutarı', \n",
    "                                              'İskonto %', 'En Güncel Kod', \"Yıl\", \"Ay\"]],\n",
    "                                on=[\"En Güncel Kod\", \"Yıl\", \"Ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_all3.drop_duplicates(ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sütun İsimlerini İngilizce Karaktere Çevirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_col_name(dff_):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    dff_: dataframe\n",
    "    Sütun ismini değiştirmek istediğiniz dataframe'i yazınız.\n",
    "    \n",
    "    Returns: Liste\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chng_letters = list(zip([\"ç\", \"ğ\", \"ı\", \"ö\", \"ş\", \"ü\", \" \", \"%\", \".\", \"(\", \")\", \"-\"], \n",
    "                            [\"c\", \"g\", \"i\", \"o\", \"s\", \"u\", \"_\", \"\", \"\", \"\", \"\", \"_\"]))\n",
    "    new_cols = []\n",
    "    for col in dff_.columns.str.lower():\n",
    "        for letter in range(len(chng_letters)):\n",
    "            col = col.replace(chng_letters[letter][0], chng_letters[letter][1])\n",
    "            if letter == len(chng_letters) - 1:\n",
    "                new_cols.append(col)\n",
    "            else:\n",
    "                pass\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3.columns = change_col_name(pasifik_df_all3)\n",
    "horizon_saha_df_all3.columns = change_col_name(horizon_saha_df_all3)\n",
    "btt_df_all3.columns = change_col_name(btt_df_all3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifikte Aktivite Tipi Verisi Eksik Olan Verilere \"Yok\" yazıldı\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3[\"aktivite_tipi\"].fillna(\"Yok\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df_all3.copy()\n",
    "horizon_saha_df_sorted = horizon_saha_df_all3.copy()\n",
    "btt_df_sorted = btt_df_all3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik = pasifik_df_sorted.copy()\n",
    "df_btt = btt_df_sorted.copy()\n",
    "df_horizon = horizon_saha_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sütun isim uzunluğunun 32'yi geçmemesi için\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik.columns = [i[:32] if len(i) > 32 else i for i in df_pasifik.columns]\n",
    "df_horizon.columns = [i[:32] if len(i) > 32 else i for i in df_horizon.columns]\n",
    "df_btt.columns = [i[:32] if len(i) > 32 else i for i in df_btt.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique.rename(columns={\"Date\": \"date\", \"En Güncel Kod\": \"en_guncel_kod\", \"Fiyat\": \"fiyat\"}, inplace=True)\n",
    "h_fiyat_unique.rename(columns={\"Date\": \"date\", \"En Güncel Kod\": \"en_guncel_kod\", \"Fiyat\": \"fiyat\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique[\"kanal\"] = \"pasifik\"\n",
    "h_fiyat_unique[\"kanal\"] = \"horizon\"\n",
    "fiyat_unique = pd.concat([p_fiyat_unique, h_fiyat_unique], axis=0, ignore_index=True)\n",
    "#fiyat_unique.to_csv(\"../data/fiyat_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik3 = df_pasifik.copy()\n",
    "df_btt3 = df_btt.copy()\n",
    "df_horizon3 = df_horizon.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik4 = df_pasifik3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_pasifik4 = df_pasifik4.merge(p_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])\n",
    "\n",
    "df_horizon4 = df_horizon3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_horizon4 = df_horizon4.merge(h_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])\n",
    "\n",
    "df_btt4 = df_btt3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_btt4 = df_btt4.merge(h_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_aktivite_df3.columns = change_col_name(pasifik_aktivite_df3)\n",
    "saha_aktivite_detay3.columns = change_col_name(saha_aktivite_detay3)\n",
    "pasifik_aktivite_df3.rename(columns={\"i̇ndirim_\": \"indirim_\"}, inplace=True)\n",
    "saha_aktivite_detay3.rename(columns={\"i̇i̇skonto_\": \"iskonto_\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik4 = df_pasifik4.drop(columns=['raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', \n",
    "                                        'i̇ndirim_', 'aktivite_tipi'], axis=1)\n",
    "\n",
    "df_horizon4 = df_horizon4.drop(columns=['ciro_kull_i̇ade_dus', 'promosyon_tutari', 'i̇skonto_'], axis=1)\n",
    "df_btt4 = df_btt4.drop(columns=['ciro_kull_i̇ade_dus', 'promosyon_tutari', 'i̇skonto_'], axis=1)\n",
    "\n",
    "pasifik_aktivite_df3.rename(columns={\"Müşteri Grup\": \"Grup adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik5 = pd.merge(df_pasifik4, pasifik_aktivite_df3[['yil', 'ay', 'grup_adi', 'en_guncel_kod', \n",
    "                                                          'raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', 'indirim_', \n",
    "                                                          'aktivite_tipi']], \n",
    "                           left_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                           right_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], how=\"left\")\n",
    "\n",
    "df_horizon5 = df_horizon4.merge(saha_aktivite_detay3[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                      'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], how=\"left\")\n",
    "\n",
    "btt_aktivite = saha_aktivite_detay3[saha_aktivite_detay3[\"grup_adi\"] == \"GELENEKSEL KANAL\"].reset_index(drop=True)\n",
    "btt_aktivite[\"grup_adi\"] = \"BTT\"\n",
    "\n",
    "df_btt5 = df_btt4.merge(btt_aktivite[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                      'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                        on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_horizon5.drop_duplicates(ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btt5.drop_duplicates(ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = df_pasifik5.copy()\n",
    "horizon_saha_df_sorted = df_horizon5.copy()\n",
    "btt_df_sorted = df_btt5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Kapsamı yeniden düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "kapsam_all.columns = change_col_name(kapsam_all)\n",
    "eslenik_kod_df.columns = change_col_name(eslenik_kod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "kapsam_all[\"urun_kodu\"] = kapsam_all[\"urun_kodu\"].apply(lambda x: int(x.split(\"-\")[0]+x.split(\"-\")[1]))\n",
    "kapsam_all = kapsam_all.merge(eslenik_kod_df[[\"urun_kodu\", \"en_guncel_kod\"]], how=\"left\", on=\"urun_kodu\")\n",
    "kapsam_all[\"en_guncel_kod\"].fillna(kapsam_all[\"urun_kodu\"], inplace=True)\n",
    "kapsam_all.drop_duplicates(subset=[\"en_guncel_kod\", \"grup_adi\"], inplace=True, ignore_index=True)\n",
    "kapsam_all = kapsam_all[kapsam_all[\"en_guncel_kod\"] != \"delist\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BACKUP_PASIFIK = pasifik_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df_sorted.merge(kapsam_all[[\"en_guncel_kod\", \"grup_adi\", \"durum\"]], how=\"left\", on=[\"en_guncel_kod\", \"grup_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_write_deneme = list(pasifik_df_sorted[(pasifik_df_sorted[\"durum\"].isna()) & (pasifik_df_sorted[\"grup_adi\"].isin([\"A101\", \"ŞOK\", \"BİM\"]))].index)\n",
    "check = pasifik_df_sorted[(pasifik_df_sorted[\"grup_adi\"] == \"BİM\") & (pasifik_df_sorted[\"portfoy\"] == 1)]\n",
    "check = check[[\"en_guncel_kod\", \"grup_adi\", \"portfoy\", \"durum\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.loc[idx_to_write_deneme, \"durum\"] = \"DENEME\"\n",
    "horizon_saha_df_sorted[\"durum\"] = np.nan\n",
    "btt_df_sorted[\"durum\"] = np.nan\n",
    "pasifik_df_sorted[\"Kanal\"] = \"pasifik\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_backup_kapsam = pasifik_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_scope_index = pasifik_df_sorted[(pasifik_df_sorted[\"portfoy\"] == 1) & \n",
    "                                       (pasifik_df_sorted[\"Kanal\"] == \"pasifik\") & \n",
    "                                       (pasifik_df_sorted[\"durum\"].isin(['DENEME+BÖLGESEL SATIŞ', 'DENEME'])) & \n",
    "                                       (~pasifik_df_sorted[\"grup_adi\"].isin([\"Diğer_Pasifik\", \"MİGROS\"]))].index\n",
    "pasifik_df_sorted.drop(\"Kanal\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "listeli_olmayan_sku = pasifik_df_sorted[(pasifik_df_sorted[\"grup_adi\"].isin([\"ŞOK\", \"A101\", \"BİM\"])) & \n",
    "                                        (~(pasifik_df_sorted[\"durum\"].isin([\"Listeli\", \"LİSTELİ\"]))) & \n",
    "                                        (pasifik_df_sorted[\"portfoy\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "listeli_olmayan_sku = listeli_olmayan_sku[[\"grup_adi\", \"en_guncel_kod\"]]\n",
    "listeli_olmayan_sku.drop_duplicates(inplace=True, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"indirim__\"] = [0 if akt < 0 else akt for akt in pasifik_df_sorted[\"indirim_\"]]\n",
    "pasifik_df_sorted.drop(\"indirim_\", axis=1, inplace=True)\n",
    "\n",
    "horizon_saha_df_sorted[\"i̇skonto__\"] = [0 if ((akt >= 0.35) or (akt <=0.01)) else akt for akt in horizon_saha_df_sorted[\"i̇skonto_\"]]\n",
    "horizon_saha_df_sorted.drop(\"i̇skonto_\", axis=1, inplace=True)\n",
    "\n",
    "btt_df_sorted[\"i̇skonto__\"] = [0 if ((akt >= 0.35) or (akt <=0.01)) else akt for akt in btt_df_sorted[\"i̇skonto_\"]]\n",
    "btt_df_sorted.drop(\"i̇skonto_\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "chng_cols = dict(zip(['Yıl', 'Ay', 'No_of_days', 'Weekdays_n', 'Weekdays_Ratio', 'Weekend_n',\n",
    "                      'Weekend_Ratio', 'Actual_Holiday_n', 'Actual_Holiday_Ratio',\n",
    "                      'Total_Holiday_n', 'Total_Holiday_Ratio', 'School_Day_n',\n",
    "                      'School_Day_Ratio', 'School_Day_brdg_n', 'School_Day_brdg_Ratio',\n",
    "                      'Ramadan_n', 'Ramadan_Ratio', 'Pandemic', 'Lockdown'],\n",
    "                     \n",
    "                     [\"yil\", \"ay\", \"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \n",
    "                      \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                      \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \n",
    "                      \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                      \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "calender_df.rename(columns=chng_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"indirim__\"].fillna(0, inplace=True)\n",
    "horizon_saha_df_sorted[\"i̇skonto__\"].fillna(0, inplace=True)\n",
    "btt_df_sorted[\"i̇skonto__\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted.rename(columns={\"i̇skonto__\": \"indirim__\"}, inplace=True)\n",
    "btt_df_sorted.rename(columns={\"i̇skonto__\": \"indirim__\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bir önceki aya yansımış aktiviteleri düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aktivite_regulation(df):\n",
    "    df_all = []\n",
    "    for sku in df[\"en_guncel_kod\"].unique():\n",
    "        for grup in df[\"grup_adi\"].unique():\n",
    "            test = df[(df[\"en_guncel_kod\"] == sku) & (df[\"grup_adi\"] == grup)]\n",
    "            for idx in test.index:\n",
    "                if (idx-2 not in test.index) and (idx-1 in test.index): # bir öncekine bakacak. ilk ve sonraki satıra bakacak. -1 götür\n",
    "                    if test.loc[idx-1, \"koli\"] > test.loc[idx, \"koli\"]:\n",
    "                        test.loc[idx-1, \"indirim__\"] += test.loc[idx, \"indirim__\"]\n",
    "                        test.loc[idx, \"indirim__\"] = 0\n",
    "                elif (idx-1 not in test.index): # ilk satırdayız. pass\n",
    "                    pass\n",
    "                else:\n",
    "                    dic = {}\n",
    "                    dic.update({idx-2: test.loc[idx-2, \"koli\"], \n",
    "                                idx-1: test.loc[idx-1, \"koli\"],\n",
    "                                idx: test.loc[idx, \"koli\"]})\n",
    "                    max_idx = max(dic, key=dic.get)\n",
    "                    if max_idx == idx:\n",
    "                        pass\n",
    "                    else:\n",
    "                        test.loc[max_idx, \"indirim__\"] += test.loc[idx, \"indirim__\"]\n",
    "                        test.loc[idx, \"indirim__\"] = 0\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasifik_df_sorted, horizon_saha_df_sorted, btt_df_sorted = aktivite_regulation(pasifik_df_sorted), aktivite_regulation(horizon_saha_df_sorted), aktivite_regulation(btt_df_sorted)\n",
    "#PASİFİK İÇİN İPTAL\n",
    "horizon_saha_df_sorted, btt_df_sorted = aktivite_regulation(horizon_saha_df_sorted), aktivite_regulation(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktivite Regulation süresi: 0:09:54.922050\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Aktivite Regulation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pasifik_aktivite_regulation(df):\n",
    "    df_all = []\n",
    "    for sku in df[\"en_guncel_kod\"].unique():\n",
    "        for grup in df[\"grup_adi\"].unique():\n",
    "            test = df[(df[\"en_guncel_kod\"] == sku) & (df[\"grup_adi\"] == grup)]\n",
    "            aktivite_tip_index = test[~test.aktivite_tipi.isna()].index.to_list()\n",
    "            for idx in aktivite_tip_index:\n",
    "                if idx == test.index[0]: # ilk satırsa atla\n",
    "                    pass\n",
    "                elif idx-1 in aktivite_tip_index: #bir önceki satırda aktivite varsa atla\n",
    "                    pass\n",
    "                else:\n",
    "                    if test.loc[idx, 'koli'] < test.loc[idx-1, 'koli']: # adet sayısı bir önceki satırdan küçükse \n",
    "                        test.loc[idx-1, 'aktivite_tipi'] = test.loc[idx, 'aktivite_tipi'] # aktiviteyi bir önceki satıra yaz\n",
    "                        test.loc[idx, 'aktivite_tipi'] = np.nan\n",
    "                        aktivite_tip_index.remove(idx) #listeden remove et ki bir alt satırda varsa önceki var mı kontrolüne takılmasın\n",
    "                        aktivite_tip_index.insert(0,0) #döngü listedeki elementlerin index'ine göre devam ettiği için en başa 0 insert et\n",
    "\n",
    "            indirim_index = test[test.indirim__ != 0].index.to_list()\n",
    "            for idx in indirim_index:\n",
    "                if idx == test.index[0]: # ilk satır\n",
    "                    pass \n",
    "                elif idx-1 in indirim_index: #bir önceki satırda indirim yüzdesi varsa atla\n",
    "                    pass \n",
    "                else:\n",
    "                    if test.loc[idx, 'koli'] < test.loc[idx-1, 'koli']:\n",
    "                        test.loc[idx-1, 'indirim__'] = test.loc[idx, 'indirim__'] #bir üste taşındı\n",
    "                        test.loc[idx, 'indirim__'] = 0  \n",
    "                        indirim_index.remove(idx)  #listeden remove et ki bir alt satırda varsa önceki var mı kontrolüne takılmasın\n",
    "                        indirim_index.insert(0, 0) #döngü listedeki elementlerin index'ine göre devam ettiği için en başa 0 insert et\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_aktivite_regulation(pasifik_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik Aktivite Regulation süresi: 0:00:34.191783\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik Aktivite Regulation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "horizon_saha_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "btt_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "\n",
    "btt_df_sorted.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\", \"date\"], keep=\"first\", ignore_index=True, inplace=True)\n",
    "horizon_saha_df_sorted.drop_duplicates(subset=[\"date\", \"grup_adi\", \"en_guncel_kod\"], keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted[\"aktivite_tipi\"] = np.nan\n",
    "btt_df_sorted[\"aktivite_tipi\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.rename(columns={'i̇ndirimli_raf_satis_fiyati': 'ciro_kull_i̇ade_dus',\n",
    "                                  'raf_tavsiye_satis_fiyati': 'promosyon_tutari'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"Kanal\"] = \"pasifik\"\n",
    "horizon_saha_df_sorted[\"Kanal\"] = \"horizon\"\n",
    "btt_df_sorted[\"Kanal\"] = \"btt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_df_eski = [] \n",
    "for docs_ in btt_lst:\n",
    "    horizon_df_eski.append(pd.read_excel(params_[\"path\"][\"horizon_koli_path\"]+docs_, skiprows=1, sheet_name=\"Horizon Saha Satış\", usecols=\"B:N\").rename(columns=chng_cols_beginning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_df_eski = pd.concat(horizon_df_eski, ignore_index=True)\n",
    "hor_kat_marka = horizon_df_eski.drop_duplicates(subset=[\"Kategori Adı\", \"Marka Adı\", \"Ürün Kodu\"], ignore_index=True)[[\"Kategori Adı\", \"Marka Adı\", \"Ürün Kodu\"]]\n",
    "hor_kat_marka.rename(columns={\"Kategori Adı\": \"kategori_adi\", \"Marka Adı\": \"marka_adi\", \"Ürün Kodu\": \"en_guncel_kod\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(hor_kat_marka, on=\"en_guncel_kod\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifik Marka İsmi Düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_marka = pasifik_df_all[[\"Grup Adı\", \"En Güncel Kod\", \"Marka Adı\"]].drop_duplicates(subset=[\"Grup Adı\", \"En Güncel Kod\", \"Marka Adı\"], ignore_index=True)\n",
    "pasifik_marka.rename(columns={\"Grup Adı\": \"grup_adi\", \"En Güncel Kod\": \"en_guncel_kod\", \"Marka Adı\": \"marka_adi\"}, inplace=True)\n",
    "pasifik_marka.grup_adi = pasifik_marka.grup_adi.apply(lambda x: \"Diğer_Pasifik\" if x==\"Diğer\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df_sorted.merge(pasifik_marka, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Horizon Marka İsmi Düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_marka = horizon_saha_df_all[[\"Grup Adı\", \"En Güncel Kod\", \"Marka Adı\"]].drop_duplicates(subset=[\"Grup Adı\", \"En Güncel Kod\", \"Marka Adı\"], ignore_index=True)\n",
    "horizon_marka.rename(columns={\"Grup Adı\": \"grup_adi\", \"En Güncel Kod\": \"en_guncel_kod\", \"Marka Adı\": \"marka_adi\"}, inplace=True)\n",
    "horizon_marka.grup_adi = horizon_marka.grup_adi.apply(lambda x: \"Diğer_Horizon\" if x==\"Diğer\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_marka[\"marka_adi\"] = horizon_marka[\"marka_adi\"].apply(lambda x: \"ALTINBAŞAK\" if x == \"ÜLKER GRİSSİNİ\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    horizon_saha_df_sorted.drop(columns=[\"marka_adi\"], axis=1, inplace=True)\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(horizon_marka, how=\"left\")\n",
    "horizon_saha_df_sorted.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# BTT Marka İsmi Düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_marka = btt_df_all[[\"Grup Adı\", \"En Güncel Kod\", \"Marka Adı\"]].drop_duplicates(subset=[\"Grup Adı\", \"En Güncel Kod\", \"Marka Adı\"], ignore_index=True)\n",
    "btt_marka.rename(columns={\"Grup Adı\": \"grup_adi\", \"En Güncel Kod\": \"en_guncel_kod\", \"Marka Adı\": \"marka_adi\"}, inplace=True)\n",
    "btt_marka.grup_adi = btt_marka.grup_adi.apply(lambda x: \"Diğer_Horizon\" if x==\"Diğer\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_marka[\"marka_adi\"] = btt_marka[\"marka_adi\"].apply(lambda x: \"ALTINBAŞAK\" if x == \"ÜLKER GRİSSİNİ\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"['marka_adi'] not found in axis\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    btt_df_sorted.drop(columns=[\"marka_adi\"], axis=1, inplace=True)\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_sorted = btt_df_sorted.merge(btt_marka, how=\"left\")\n",
    "btt_df_sorted.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([pasifik_df_sorted, horizon_saha_df_sorted, btt_df_sorted], axis=0, ignore_index=True)\n",
    "df_all.drop(columns=\"kanal\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[[\"yil\", \"ay\", \"Kanal\"]+df_all.drop(columns=[\"yil\", \"ay\", \"Kanal\"]).columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.aktivite_tipi.fillna(\"Yok\", inplace=True)\n",
    "try: df_all.drop(columns=[\"durum\", \"promosyon_tutari\", \"ciro_kull_i̇ade_dus\"], axis=1, inplace=True)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"koli_new\"] = df_all[\"koli\"]\n",
    "df_all[\"koli_new\"] = np.where(df_all[\"koli_new\"] == 0, np.nan, df_all[\"koli_new\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[df_all[\"portfoy\"] == 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Geçmiş ayları çalıştırmak için bu kısmı kullanıyoruz\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = df_all[df_all[\"date\"] < datetime(2021, 9, 1)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.to_csv(\"../../../2021_08_01_lstm_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all.to_csv(\"../../../lstm_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Delist'], dtype=object)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[df_all[\"kategori_adi\"].isna()].en_guncel_kod.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[~(df_all[\"en_guncel_kod\"].isin([\"Delist\", \" Delist\", \" Delist \", \"Delist \", \"delist\", \" delist\", \" delist \", \"delist \"]))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"../../../2021_11_01_lstm_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_kat_unique = df_all.drop_duplicates(subset=[\"Kanal\", \"kategori_adi\"], ignore_index=True)[[\"Kanal\", \"kategori_adi\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"back_to_school\"] = df_all[\"ay\"].apply(lambda x: 1 if x in [9, 2] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_kat_ = []\n",
    "corr_ = []\n",
    "dt_feature_importance_ = []\n",
    "xgb_feature_importance_ = []\n",
    "rf_feature_importance_ = []\n",
    "\n",
    "for idx in kanal_kat_unique.index:\n",
    "    tmp = df_all[(df_all[\"Kanal\"] == kanal_kat_unique[\"Kanal\"][idx]) & (df_all[\"kategori_adi\"] == kanal_kat_unique[\"kategori_adi\"][idx])]\n",
    "    tmp.fiyat.fillna(0, inplace=True)\n",
    "    tmp_corr = tmp[[\"koli\", \"no_of_days\", 'back_to_school', 'weekdays_ratio', 'weekend_ratio',\n",
    "                    'total_holiday_ratio', 'school_day_ratio', 'ramadan_ratio', 'indirim__', 'fiyat']].corr()\n",
    "    tmp_dt = tmp[[\"koli\", \"no_of_days\", 'back_to_school', 'weekdays_ratio', 'weekend_ratio',\n",
    "                  'total_holiday_ratio', 'school_day_ratio', 'ramadan_ratio', 'indirim__', 'fiyat']]\n",
    "    X = tmp_dt.drop(\"koli\", axis=1)\n",
    "    y = tmp_dt.koli\n",
    "    dt = DecisionTreeRegressor().fit(X, y)\n",
    "    xgb = XGBRegressor().fit(X, y)\n",
    "    rf = RandomForestRegressor().fit(X, y)\n",
    "    \n",
    "    dt_importance = dt.feature_importances_\n",
    "    xgb_importance = xgb.feature_importances_\n",
    "    rf_importance = rf.feature_importances_\n",
    "    dt_feat_imprtnce = {}\n",
    "    xgb_feat_imprtnce = {}\n",
    "    rf_feat_imprtnce = {}\n",
    "    for i, v in enumerate(dt_importance):\n",
    "        dt_feat_imprtnce.update({X.columns[i]: v})\n",
    "    for i, v in enumerate(xgb_importance):\n",
    "        xgb_feat_imprtnce.update({X.columns[i]: v})\n",
    "    for i, v in enumerate(rf_importance):\n",
    "        rf_feat_imprtnce.update({X.columns[i]: v})\n",
    "    dt_feat_imprtnce = {k: v for k, v in sorted(dt_feat_imprtnce.items(), key=lambda item: item[1], reverse=True)}\n",
    "    xgb_feat_imprtnce = {k: v for k, v in sorted(xgb_feat_imprtnce.items(), key=lambda item: item[1], reverse=True)}\n",
    "    rf_feat_imprtnce = {k: v for k, v in sorted(rf_feat_imprtnce.items(), key=lambda item: item[1], reverse=True)}\n",
    "    kanal_kat_.append(kanal_kat_unique.loc[idx, \"Kanal\"] + \"_\" + kanal_kat_unique.loc[idx, \"kategori_adi\"])\n",
    "    corr_.append(tmp_corr)\n",
    "    dt_feature_importance_.append(dt_feat_imprtnce)\n",
    "    xgb_feature_importance_.append(xgb_feat_imprtnce)\n",
    "    rf_feature_importance_.append(rf_feat_imprtnce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ = pd.DataFrame({\"hierarchy\": kanal_kat_,\n",
    "                         \"correlation\": corr_,\n",
    "                         \"dt_fi\": dt_feature_importance_,\n",
    "                         \"xgb_fi\": xgb_feature_importance_,\n",
    "                         \"rf_fi\": rf_feature_importance_})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for idx in summary_.index:\n",
    "    print(summary_.loc[idx, \"hierarchy\"])\n",
    "    print(\"-\"*50)\n",
    "    print()\n",
    "    print(\"correlation coefficients\")\n",
    "    print(\"-\"*50)\n",
    "    print(summary_.loc[idx, \"correlation\"][\"koli\"].sort_values(ascending=False))\n",
    "    print()\n",
    "    print(\"dt feature importance\")\n",
    "    print(\"=\"*50)\n",
    "    print(summary_.loc[idx, \"dt_fi\"])\n",
    "    print(\"\\n\")\n",
    "    print(\"xgb feature importance\")\n",
    "    print(\"=\"*50)\n",
    "    print(summary_.loc[idx, \"xgb_fi\"])\n",
    "    print(\"\\n\")\n",
    "    print(\"rf feature importance\")\n",
    "    print(\"=\"*50)\n",
    "    print(summary_.loc[idx, \"rf_fi\"])\n",
    "    print(\"\\n\")\n",
    "    print(\"*\"*200)\n",
    "    print(\"*\"*200)\n",
    "    print(\"*\"*200)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for idx in kanal_kat_unique.index:\n",
    "    data_plot = df_all[(df_all[\"Kanal\"] == kanal_kat_unique[\"Kanal\"][idx]) & (df_all[\"kategori_adi\"] == kanal_kat_unique[\"kategori_adi\"][idx])]\n",
    "    data_plot = data_plot.groupby([\"date\", \"Kanal\"]).agg({\"koli_new\": \"sum\"}).reset_index()\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(data_plot['date'], data_plot[\"koli_new\"], marker=\"o\", markersize=5)\n",
    "    plt.legend([\"Koli History\"])\n",
    "    plt.title(f\"{kanal_kat_unique['Kanal'][idx]} & {kanal_kat_unique['kategori_adi'][idx]}\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# !!!!!\n",
    "# Bu kısımda sadece pasifik için yapıyorum. Düzenledikten sonra horizon ve btt de dahil olacak\n",
    "# !!!!!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_backup = df_all.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Unique Fiyatların düzenlenmesi. Bazı kısımlarda na geldi çünkü önceki tarihlerin fiyatları yok. Data 2017 Ocak'ta başlıyor ama fiyat datası 2017 Aralıkta başlıyor. Bu durumdan dolayı missing imputation yaptım.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_unique.rename(columns={\"kanal\": \"Kanal\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v2 = df_all.copy()\n",
    "df_all_v2.en_guncel_kod = df_all_v2.en_guncel_kod.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_unique = fiyat_unique[~(fiyat_unique[\"en_guncel_kod\"].isin([\"Delist\", \" Delist\", \" Delist \", \"Delist \", \"delist\", \" delist\", \" delist \", \"delist \"]))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_unique.en_guncel_kod = fiyat_unique.en_guncel_kod.apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_all_v2.drop(columns=[\"fiyat\", \"fiyat_gecisi\"], axis=1, inplace=True)\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v2 = df_all_v2.merge(fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\", \"Kanal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_fiyat_ = df_all_v2[df_all_v2[\"fiyat\"].isna()][[\"grup_adi\", \"en_guncel_kod\"]].drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v3 = df_all_v2.copy()\n",
    "df_all_v3.fiyat.fillna(method=\"bfill\", inplace=True)\n",
    "df_all_v3.fiyat.fillna(0, inplace=True)\n",
    "df_all_v3.fiyat_gecisi.fillna(method=\"bfill\", inplace=True)\n",
    "df_all_v3.fiyat_gecisi.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Koli Missing Imputation (Mean)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_imputation(df__, unique_grup, unique_sku, loop):\n",
    "    print(\"Başla ------>\", unique_grup[loop], unique_sku[loop])\n",
    "    tmp = df__[(df__[\"grup_adi\"] == unique_grup[loop]) & (df__[\"en_guncel_kod\"] == unique_sku[loop])]\n",
    "    if tmp[\"koli_new\"].isna().sum() > 0:\n",
    "        mean = tmp[\"koli_new\"].mean()\n",
    "        tmp.koli_new.fillna(mean, inplace=True)\n",
    "    else: pass\n",
    "    print(\"Bitti ------>\", unique_grup[loop], unique_sku[loop])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = df_all_v3[[\"grup_adi\", \"en_guncel_kod\"]].drop_duplicates(ignore_index=True)\n",
    "unique_grup = unique_list[\"grup_adi\"].to_list()\n",
    "unique_sku = unique_list[\"en_guncel_kod\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v4 = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support()\n",
    "    available_cpu = mp.cpu_count() - 1\n",
    "    df__ = df_all_v3.copy()\n",
    "    func = partial(missing_imputation, df__, unique_grup, unique_sku)\n",
    "    loop = list(unique_list.index)\n",
    "    with mp.Pool(available_cpu) as p:\n",
    "        df_all_v4.append(p.map(func, loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Imputation: 0:00:23.571523\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Missing Imputation: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v4 = pd.concat(df_all_v4[0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v4[\"koli_new\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Trend Seasonality Decomposition with parallel processsing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = df_all_v4.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], ignore_index=True)[[\"grup_adi\", \"en_guncel_kod\"]]\n",
    "unique_grup = unique_list[\"grup_adi\"].to_list()\n",
    "unique_en_guncel_kod = unique_list[\"en_guncel_kod\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v4.drop_duplicates(ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_seasonality_decomp(df__, unique_grup, unique_en_guncel_kod, loop):\n",
    "    from statsmodels.tsa.seasonal import STL\n",
    "    import numpy as np\n",
    "    print(\"Başladı ---------->\", unique_grup[loop], unique_en_guncel_kod[loop])\n",
    "    temp_df = df__[(df__[\"en_guncel_kod\"] == unique_en_guncel_kod[loop]) & \n",
    "                   (df__[\"grup_adi\"] == unique_grup[loop])]\n",
    "    if len(temp_df) > 2:\n",
    "        df_ts = temp_df[['koli_new','date']]\n",
    "        df_ts.set_index('date',inplace=True)\n",
    "\n",
    "        result = STL(df_ts).fit()\n",
    "        temp_df['season'] = list(result.seasonal)\n",
    "        temp_df['trend']  = list(result.trend)\n",
    "        temp_df['residual']  = list(result.resid)\n",
    "    else:\n",
    "        temp_df['season'] = np.nan\n",
    "        temp_df['trend']  = np.nan\n",
    "        temp_df['residual']  = np.nan\n",
    "    print(\"Bitti ---------->\", unique_grup[loop], unique_en_guncel_kod[loop])\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v5 = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support()\n",
    "    available_cpu = mp.cpu_count() - 1\n",
    "    df__ = df_all_v4.copy()\n",
    "    func = partial(trend_seasonality_decomp, df__, unique_grup, unique_en_guncel_kod)\n",
    "    loop = list(unique_list.index)\n",
    "    with mp.Pool(available_cpu) as p:\n",
    "        df_all_v5.append(p.map(func, loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataların yüklenmesi: 0:00:26.712788\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Dataların yüklenmesi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v5 = pd.concat(df_all_v5[0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all_v5.to_csv(\"../data/2021_08_01_data_to_analysis_koli.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all_v5.to_csv(\"../data/2021_11_01_data_to_analysis_koli.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_next_four = df_all_v5[[\"grup_adi\", \"en_guncel_kod\", \"fiyat\", \"date\"]]\n",
    "fiyat_next_four.sort_values(by=[\"grup_adi\", \"en_guncel_kod\", \"date\"], ascending=[True, True, False], inplace=True, ignore_index=True)\n",
    "fiyat_next_four = fiyat_next_four.groupby([\"grup_adi\", \"en_guncel_kod\"]).agg({\"date\": \"max\", \"fiyat\": \"max\"}).reset_index()\n",
    "fiyat_next_four = fiyat_next_four[[\"grup_adi\", \"en_guncel_kod\", \"fiyat\"]].drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scope Belirleme (24 aylık veri varsa time series çalışsın. Yoksa moving average)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope(df__, unique_grup, unique_sku, threshold, loop):\n",
    "    print(\"Başla ----->\", loop)\n",
    "    tmp = df__[(df__[\"grup_adi\"] == unique_grup[loop]) & (df__[\"en_guncel_kod\"] == unique_sku[loop])]\n",
    "    if len(tmp[tmp[\"koli\"] != 0]) >= 24:\n",
    "        tmp[\"scope\"] = \"time_series\"\n",
    "    else:\n",
    "        tmp[\"scope\"] = \"moving_average\"\n",
    "    print(\"Bitti ----->\", loop)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = df_all_v5[[\"grup_adi\", \"en_guncel_kod\"]].drop_duplicates(ignore_index=True)\n",
    "unique_grup = unique_list[\"grup_adi\"].to_list()\n",
    "unique_sku = unique_list[\"en_guncel_kod\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v6 = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support()\n",
    "    available_cpu = mp.cpu_count() - 1\n",
    "    df__ = df_all_v5.copy()\n",
    "    func = partial(scope, df__, unique_grup, unique_sku, 24)\n",
    "    loop = list(unique_list.index)\n",
    "    with mp.Pool(available_cpu) as p:\n",
    "        df_all_v6.append(p.map(func, loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataların yüklenmesi: 0:00:24.110794\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Dataların yüklenmesi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v6 = pd.concat(df_all_v6[0], ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all_v6.to_csv(\"../data/2021_08_01_data_to_analysis_koli.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v6.to_csv(\"../data/2021_11_01_data_to_analysis_koli.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_all_v6 = pd.read_csv(\"../data/data_to_analysis_koli.csv\")\n",
    "df_all_v6[\"date\"] = pd.to_datetime(df_all_v6[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "grup_sku_unique = df_all_v6.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], ignore_index=True)[[\"grup_adi\", \"en_guncel_kod\"]]\n",
    "grup_akt_dict = {}\n",
    "for idx in grup_sku_unique.index:\n",
    "    tmp_df = df_all_v6[(df_all_v6[\"grup_adi\"] == grup_sku_unique[\"grup_adi\"][idx]) & \n",
    "                       (df_all_v6[\"en_guncel_kod\"] == grup_sku_unique[\"en_guncel_kod\"][idx])]\n",
    "    akt_dict = {}\n",
    "    for akt in tmp_df.aktivite_tipi.unique():\n",
    "        ratio = tmp_df[(tmp_df[\"koli\"] != 0) & (tmp_df[\"aktivite_tipi\"] == akt)][\"koli\"].mean() / tmp_df[(tmp_df[\"koli\"] != 0) & (tmp_df[\"aktivite_tipi\"] == \"Yok\")][\"koli\"].mean()\n",
    "        akt_dict.update({akt: ratio})\n",
    "    grup_akt_dict.update({grup_sku_unique.loc[idx, \"grup_adi\"] + \"_\" + str(grup_sku_unique.loc[idx, \"en_guncel_kod\"]): akt_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "grup_akt_dict2 = grup_akt_dict.copy()\n",
    "for idx in grup_akt_dict2.keys():\n",
    "    grup_akt_dict2[idx] = {key_: value_ for key_, value_ in sorted(grup_akt_dict2[idx].items(), key=lambda x: x[1], reverse=True)}\n",
    "    new_values = sorted([i for i in range(0, len(grup_akt_dict2[idx])*5, 5)], reverse=True)\n",
    "    for index_, value_ in enumerate(grup_akt_dict2[idx]):\n",
    "        grup_akt_dict2[idx][value_] = new_values[index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_akt_list = list(df_all_v6.aktivite_tipi.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_ in grup_akt_dict2.keys():\n",
    "    for i in unique_akt_list:\n",
    "        if i not in list(grup_akt_dict2[key_].keys()):\n",
    "            grup_akt_dict2[key_].update({i: 0})\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v7 = []\n",
    "unique_grup_sku = df_all_v6[[\"grup_adi\", \"en_guncel_kod\"]].drop_duplicates(ignore_index=True)\n",
    "for idx in unique_grup_sku.index:\n",
    "    tmp_df = df_all_v6[(df_all_v6[\"grup_adi\"] == unique_grup_sku.loc[idx, \"grup_adi\"]) & (df_all_v6[\"en_guncel_kod\"] == unique_grup_sku.loc[idx, \"en_guncel_kod\"])]\n",
    "    tmp_df.aktivite_tipi = tmp_df.aktivite_tipi.map(grup_akt_dict2[unique_grup_sku.loc[idx, \"grup_adi\"] + \"_\" + str(unique_grup_sku.loc[idx, \"en_guncel_kod\"])])\n",
    "    df_all_v7.append(tmp_df)\n",
    "df_all_v7 = pd.concat(df_all_v7, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, df_all_v7.indirim__.max()+1]\n",
    "len(indirim_bins) , df_all_v7.indirim__.value_counts(bins=indirim_bins).sort_index()\n",
    "df_all_v7['indirim__bins'] = pd.cut(df_all_v7.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v7[\"indirim__bins\"] = df_all_v7[\"indirim__\"].apply(lambda x: 0 if x <= 0 else \n",
    "                                                          (1 if x <= 0.01 else \n",
    "                                                           (2 if x <= 0.02 else \n",
    "                                                            (3 if x <= 0.03 else \n",
    "                                                             (4 if x <= 0.04 else \n",
    "                                                              (5 if x <= 0.05 else \n",
    "                                                               (6 if x <= 0.06 else \n",
    "                                                                (7 if x <= 0.07 else \n",
    "                                                                 (8 if x <= 0.08 else \n",
    "                                                                  (9 if x <= 0.09 else \n",
    "                                                                   (10 if x <= 0.10 else \n",
    "                                                                    (12.5 if x <= 0.15 else 15))))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "marka_df = df_all_v7.groupby([\"date\", \"Kanal\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\"]).agg({\"koli_new\": \"sum\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "marka_unique = marka_df[[\"Kanal\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\"]].drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "marka_trend_seasonality_df = []\n",
    "for idx in marka_unique.index:\n",
    "    temp_df = marka_df[(marka_df[\"Kanal\"] == marka_unique.loc[idx, \"Kanal\"]) &\n",
    "                       (marka_df[\"grup_adi\"] == marka_unique.loc[idx, \"grup_adi\"]) & \n",
    "                       (marka_df[\"ana_kategori_adi\"] == marka_unique.loc[idx, \"ana_kategori_adi\"]) & \n",
    "                       (marka_df[\"kategori_adi\"] == marka_unique.loc[idx, \"kategori_adi\"]) & \n",
    "                       (marka_df[\"marka_adi\"] == marka_unique.loc[idx, \"marka_adi\"])]\n",
    "    if len(temp_df) > 2:\n",
    "        df_ts = temp_df[['koli_new','date']]\n",
    "        df_ts.set_index('date',inplace=True)\n",
    "\n",
    "        result = STL(df_ts).fit()\n",
    "        temp_df['season_marka'] = list(result.seasonal)\n",
    "        temp_df['trend_marka']  = list(result.trend)\n",
    "        temp_df['residual_marka']  = list(result.resid)\n",
    "    else:\n",
    "        temp_df['season_marka'] = np.nan\n",
    "        temp_df['trend_marka']  = np.nan\n",
    "        temp_df['residual_marka']  = np.nan\n",
    "    marka_trend_seasonality_df.append(temp_df)\n",
    "marka_trend_seasonality_df = pd.concat(marka_trend_seasonality_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    marka_trend_seasonality_df.drop(\"koli_new\", axis=1, inplace=True)\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "    pass\n",
    "df_all_v8 = df_all_v7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v8 = df_all_v8.merge(marka_trend_seasonality_df, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_v8[\"model_input_info\"] = df_all_v8[\"Kanal\"] + \"_\" + df_all_v8[\"kategori_adi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    calender_df[\"date\"] = pd.to_datetime(calender_df[\"Yıl\"].astype(str) + \"-\" + calender_df[\"Ay\"].astype(str) + \"-01\", format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "except:\n",
    "    calender_df[\"date\"] = pd.to_datetime(calender_df[\"yil\"].astype(str) + \"-\" + calender_df[\"ay\"].astype(str) + \"-01\", format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "calender_df.rename(columns=chng_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "calender_df[\"back_to_school\"] = calender_df[\"ay\"].apply(lambda x: 1 if x in [9, 2] else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modelleme Kısmı\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modellemede hangi kategoride hangi sütunlar kullanılacak kısmı için model_input_info şeklinde yeni bir sütun açtık."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# Time Series (LSTM)\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_future(X, time_steps=1):\n",
    "    Xs = []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "    return np.array(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset2(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)        \n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1234\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BİSKÜVİ', 'KREM ÇİKOLATA/EZME', 'ÇİKOLATA KAPLAMA', 'ÇİKOLATA',\n",
       "       'KEK', 'GOFRET', 'ŞEKER', 'PASTACILIK ÜRÜNÜ', 'SAKIZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_v8.kategori_adi.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['moving_average'], dtype=object)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_v8[(df_all_v8[\"en_guncel_kod\"] == 136306) & (df_all_v8[\"Kanal\"] == \"pasifik\")].scope.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = df_all_v8[df_all_v8[\"scope\"] == \"time_series\"]\n",
    "ts_df = ts_df[(ts_df[\"Kanal\"] == \"pasifik\")]\n",
    "grup_sku_unique = ts_df[[\"grup_adi\", \"en_guncel_kod\"]].drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def lstm_predictions(ts_df, df_all_v8, grup_sku_unique, loop_list):\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Başladı -->\",grup_sku_unique.loc[loop_list, \"grup_adi\"], str(grup_sku_unique.loc[loop_list, \"en_guncel_kod\"]))\n",
    "    #for idx in tqdm(grup_sku_unique.index):\n",
    "    max_date = ts_df.date.max()\n",
    "    iterated_results = []\n",
    "    iteration_ = 0\n",
    "    while iteration_ < 5:\n",
    "    #    for idx in [9, 10, 16, 17]:\n",
    "        print(\"=\"*len(\"Iteration --> 0\"))\n",
    "        print(\"Iteration -->\", iteration_+1)\n",
    "        print(\"=\"*len(\"Iteration --> 0\"))\n",
    "        print()\n",
    "#    for idx in tqdm([0]):\n",
    "        for learn_rate in [0.05, 0.1, 0.2]:\n",
    "#            print(\"iteration:\",iteration_,\"--->\",grup_sku_unique.loc[idx, \"grup_adi\"], grup_sku_unique.loc[idx, \"en_guncel_kod\"], learn_rate)\n",
    "            ts_df_sku = ts_df[(ts_df[\"grup_adi\"] == grup_sku_unique.loc[loop_list, \"grup_adi\"]) & (ts_df[\"en_guncel_kod\"] == grup_sku_unique.loc[loop_list, \"en_guncel_kod\"])]\n",
    "            ts_df_sku.set_index(\"date\", inplace=True)\n",
    "            model_input_info = {\n",
    "\n",
    "                \"pasifik_BİSKÜVİ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\", \"ramadan_ratio\"], # \"pasifik_BİSKÜVİ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_KREM ÇİKOLATA/EZME\": [\"indirim__\", \"fiyat\", \"school_day_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_ÇİKOLATA KAPLAMA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"], #\"pasifik_ÇİKOLATA KAPLAMA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"ramadan_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_ÇİKOLATA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"ramadan_ratio\", \"aktivite_tipi\", \"total_holiday_ratio\"],\n",
    "                \"pasifik_KEK\": [\"fiyat\", \"school_day_ratio\", \"ramadan_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_GOFRET\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_ŞEKER\": [\"fiyat\", \"indirim__\", \"total_holiday_ratio\", \"ramadan_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_PASTACILIK ÜRÜNÜ\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\"],\n",
    "                \"pasifik_SAKIZ\": [\"fiyat\", \"indirim__\", \"weekdays_ratio\", \"ramadan_ratio\", \"aktivite_tipi\"],\n",
    "\n",
    "                \"horizon_BİSKÜVİ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_ÇİKOLATA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_ÇİKOLATA KAPLAMA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_KEK\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"back_to_school\", \"ramadan_ratio\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_GOFRET\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_ŞEKER\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_PASTACILIK ÜRÜNÜ\": [\"fiyat\", \"indirim__\", \"ramadan_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_KREM ÇİKOLATA/EZME\": [\"fiyat\", \"indirim__\", \"ramadan_ratio\", \"total_holiday_ratio\", \"school_day_ratio\", \"aktivite_tipi\"], \n",
    "                \"horizon_SAKIZ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"], \n",
    "\n",
    "                \"btt_BİSKÜVİ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"btt_ÇİKOLATA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\", \"ramadan_ratio\"],\n",
    "                \"btt_ÇİKOLATA KAPLAMA\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"ramadan_ratio\", \"back_to_school\", \"school_day_ratio\"],\n",
    "                \"btt_GOFRET\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"ramadan_ratio\", \"total_holiday_ratio\"],\n",
    "                \"btt_ŞEKER\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"school_day_ratio\", \"total_holiday_ratio\", \"weekdays_ratio\"],\n",
    "                \"btt_PASTACILIK ÜRÜNÜ\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"school_day_ratio\", \"total_holiday_ratio\", \"no_of_days\"],\n",
    "                \"btt_KEK\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"school_day_ratio\", \"total_holiday_ratio\"],\n",
    "                \"btt_KREM ÇİKOLATA/EZME\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"school_day_ratio\"],\n",
    "                \"btt_SAKIZ\": [\"indirim__\", \"aktivite_tipi\", \"total_holiday_ratio\", \"school_day_ratio\"]\n",
    "\n",
    "            }\n",
    "\n",
    "            for key_ in model_input_info.keys():\n",
    "                model_input_info[key_].extend([\"trend\", \"season\", \"indirim__bins\", \"trend_marka\", \"season_marka\", \"koli_new\"])\n",
    "                try:\n",
    "                    model_input_info[key_].remove(\"indirim__\")\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            f_columns = model_input_info[ts_df_sku[\"model_input_info\"].unique()[0]]\n",
    "            ts_df_sku = ts_df_sku[f_columns]\n",
    "        #    print(ts_df_sku.shape)\n",
    "            if (ts_df_sku[\"trend\"].values==ts_df_sku[\"trend_marka\"].values).any():\n",
    "                f_columns.remove(\"trend_marka\")\n",
    "                f_columns.remove(\"season_marka\")\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            ts_df_sku = ts_df_sku[f_columns]\n",
    "            #print(ts_df_sku)\n",
    "\n",
    "            f_transformer = RobustScaler()\n",
    "            koli_transformer = RobustScaler()\n",
    "\n",
    "            y_col = [i for i in f_columns if i == \"koli_new\"]\n",
    "            f_columns.remove(\"koli_new\")\n",
    "\n",
    "            f_transformer = f_transformer.fit(ts_df_sku[f_columns].to_numpy())\n",
    "            koli_transformer = koli_transformer.fit(ts_df_sku[y_col])\n",
    "\n",
    "            ts_df_sku.loc[:, f_columns] = f_transformer.transform(ts_df_sku[f_columns].to_numpy())\n",
    "            ts_df_sku['koli_new'] = koli_transformer.transform(ts_df_sku[y_col])\n",
    "\n",
    "\n",
    "            time_steps = 3\n",
    "            # reshape to [samples, time_steps, n_features]\n",
    "            X_train, y_train = create_dataset2(ts_df_sku.iloc[:,:-1], ts_df_sku.iloc[:, -1:], time_steps)\n",
    "        #    print(X_train.shape, y_train.shape)\n",
    "\n",
    "            model = keras.Sequential()\n",
    "            model.add(\n",
    "              keras.layers.Bidirectional(\n",
    "                keras.layers.LSTM(\n",
    "                  units=128, \n",
    "                  input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "                )\n",
    "              )\n",
    "            )\n",
    "            model.add(keras.layers.Dropout(rate=learn_rate))\n",
    "            model.add(keras.layers.Dense(units=1))\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            \n",
    "            all_dates = [max_date + relativedelta(months=i) for i in range(1, 5)]\n",
    "\n",
    "            history = model.fit(X_train, y_train, validation_split=0.1, epochs=30, batch_size=32, verbose=0, shuffle=False)\n",
    "            kntrl = df_all_v8[(df_all_v8[\"grup_adi\"] == grup_sku_unique.loc[idx, \"grup_adi\"]) & (df_all_v8[\"en_guncel_kod\"] == grup_sku_unique.loc[idx, \"en_guncel_kod\"])]\n",
    "            season_trend_add = kntrl[(kntrl[\"date\"] <= max(all_dates) - relativedelta(years=1)) & (kntrl[\"date\"] >= min(all_dates) - relativedelta(years=1))][[\"date\", \"trend\", \"season\", \"trend_marka\", \"season_marka\"]]\n",
    "\n",
    "            future_X_part1 = ts_df_sku.iloc[-3:, :-1]\n",
    "            future_X = ts_df_sku.iloc[-4:, :-1]\n",
    "            future_X.reset_index(inplace=True)\n",
    "\n",
    "            all_cols_unique = []\n",
    "            for i,j in model_input_info.items():\n",
    "                all_cols_unique.extend(j)\n",
    "\n",
    "            all_cols_unique = list(set(all_cols_unique))\n",
    "            all_cols_unique.remove(\"koli_new\")\n",
    "            all_cols_unique.remove(\"indirim__bins\")\n",
    "            all_cols_unique.remove(\"aktivite_tipi\")\n",
    "            calender_cols = [i for i in future_X.columns if i in calender_df and i != \"date\"]\n",
    "\n",
    "            future_X[\"date\"] = all_dates\n",
    "            if 'season' in future_X.columns:\n",
    "                future_X.season = season_trend_add.season.values\n",
    "            if 'season_marka' in future_X.columns:\n",
    "                future_X.season_marka = season_trend_add.season_marka.values\n",
    "            if 'total_holiday_ratio' in future_X.columns:\n",
    "                future_X.drop(columns='total_holiday_ratio', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'total_holiday_ratio']], how=\"left\", on=['date'])\n",
    "            if 'back_to_school' in future_X.columns:\n",
    "                future_X.drop(columns='back_to_school', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'back_to_school']], how=\"left\", on=['date'])\n",
    "            if 'ramadan_ratio' in future_X.columns:\n",
    "                future_X.drop(columns='ramadan_ratio', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'ramadan_ratio']], how=\"left\", on=['date'])\n",
    "            if 'weekdays_ratio' in future_X.columns:\n",
    "                future_X.drop(columns='weekdays_ratio', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'weekdays_ratio']], how=\"left\", on=['date'])\n",
    "            if 'school_day_ratio' in future_X.columns:\n",
    "                future_X.drop(columns='school_day_ratio', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'school_day_ratio']], how=\"left\", on=['date'])\n",
    "            if 'no_of_days' in future_X.columns:\n",
    "                future_X.drop(columns='no_of_days', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'no_of_days']], how=\"left\", on=['date'])\n",
    "            if 'fiyat' in future_X.columns:\n",
    "                future_X.fiyat = [kntrl.tail()[[\"date\", \"fiyat\", \"trend\", \"trend_marka\"]].iloc[-1:, 1].values[0]]*4\n",
    "            if 'trend' in future_X.columns:\n",
    "                future_X.trend = [kntrl.tail()[[\"date\", \"fiyat\", \"trend\", \"trend_marka\"]].iloc[-1:, 2].values[0]]*4\n",
    "            if 'trend_marka' in future_X.columns:\n",
    "                future_X.trend_marka = [kntrl.tail()[[\"date\", \"fiyat\", \"trend\", \"trend_marka\"]].iloc[-1:, 3].values[0]]*4\n",
    "\n",
    "            future_X.loc[1, \"aktivite_tipi\"] = 10\n",
    "            future_X.loc[1, \"indirim__bins\"] = 5\n",
    "            #print(future_X)\n",
    "            future_X.set_index(\"date\", inplace=True)\n",
    "            future_X = future_X[f_columns]\n",
    "\n",
    "            future_X = f_transformer.inverse_transform(future_X)\n",
    "            future_X = np.vstack((future_X_part1.values, future_X))\n",
    "            X = create_dataset_future(future_X, time_steps)\n",
    "            X.shape\n",
    "\n",
    "            y_predT = model.predict(X)\n",
    "\n",
    "            y_predT_inv = koli_transformer.inverse_transform(y_predT)\n",
    "            y_predT_inv2 = [i[0] for i in y_predT_inv]\n",
    "            if y_predT_inv2[0] < 0 or y_predT_inv2[1] < 0 or y_predT_inv2[2] < 0 or y_predT_inv2[3] < 0:\n",
    "                pass\n",
    "            else:\n",
    "                pred_df = pd.DataFrame({\"date\": all_dates,\n",
    "                                        \"grup_adi\": [grup_sku_unique.loc[idx, \"grup_adi\"]]*4,\n",
    "                                        \"en_guncel_kod\": [grup_sku_unique.loc[idx, \"en_guncel_kod\"]]*4,\n",
    "                                        \"pred\": y_predT_inv2})\n",
    "                iterated_results.append(pred_df)\n",
    "        iteration_+= 1\n",
    "    iterated_results = pd.concat(iterated_results, ignore_index=True)\n",
    "    pred_df = iterated_results.groupby([\"date\", \"grup_adi\", \"en_guncel_kod\"]).agg({\"pred\": \"mean\"}).reset_index()\n",
    "    print(\"Bitti -->\",grup_sku_unique.loc[loop_list, \"grup_adi\"], str(grup_sku_unique.loc[loop_list, \"en_guncel_kod\"]))\n",
    "    print(\"\\n\")\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#lstm_predictions(ts_df, df_all_v8, grup_sku_unique, 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_final_preds = []\n",
    "if __name__ == \"__main__\" or __name__ == \"__parents_main__\":\n",
    "    mp.freeze_support()\n",
    "    available_cpu = mp.cpu_count() - 1\n",
    "    func = partial(lstm_predictions, ts_df, df_all_v8, grup_sku_unique)\n",
    "    loop = list(grup_sku_unique.index)\n",
    "    loop = loop[:10]\n",
    "    with mp.Pool(available_cpu) as p:\n",
    "        all_final_preds.append(p.map(func, loop))\n",
    "        #all_final_preds.append(p.imap_unordered(func, loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_final_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Iteration --> 1\n",
      "===============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                               | 1/295 [00:17<1:24:04, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000205C37F58B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002058A198940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 295/295 [1:43:29<00:00, 21.05s/it]\n",
      "  0%|                                                                                          | 0/295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Iteration --> 2\n",
      "===============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 295/295 [1:41:01<00:00, 20.55s/it]\n",
      "  0%|                                                                                          | 0/295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Iteration --> 3\n",
      "===============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 295/295 [1:42:51<00:00, 20.92s/it]\n",
      "  0%|                                                                                          | 0/295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Iteration --> 4\n",
      "===============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 295/295 [1:44:33<00:00, 21.27s/it]\n",
      "  0%|                                                                                          | 0/295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Iteration --> 5\n",
      "===============\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 295/295 [1:44:24<00:00, 21.24s/it]\n"
     ]
    }
   ],
   "source": [
    "#for idx in tqdm(grup_sku_unique.index):\n",
    "max_date = ts_df.date.max()\n",
    "iterated_results = []\n",
    "iteration_ = 0\n",
    "while iteration_ < 5:\n",
    "#    for idx in [9, 10, 16, 17]:\n",
    "    print(\"=\"*len(\"Iteration --> 0\"))\n",
    "    print(\"Iteration -->\", iteration_+1)\n",
    "    print(\"=\"*len(\"Iteration --> 0\"))\n",
    "    print()\n",
    "    for idx in tqdm(grup_sku_unique.index):\n",
    "#    for idx in tqdm([0]):\n",
    "        for learn_rate in [0.05, 0.1, 0.2]:\n",
    "#            print(\"iteration:\",iteration_,\"--->\",grup_sku_unique.loc[idx, \"grup_adi\"], grup_sku_unique.loc[idx, \"en_guncel_kod\"], learn_rate)\n",
    "            ts_df_sku = ts_df[(ts_df[\"grup_adi\"] == grup_sku_unique.loc[idx, \"grup_adi\"]) & (ts_df[\"en_guncel_kod\"] == grup_sku_unique.loc[idx, \"en_guncel_kod\"])]\n",
    "            ts_df_sku.set_index(\"date\", inplace=True)\n",
    "            model_input_info = {\n",
    "                \n",
    "                \"pasifik_BİSKÜVİ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\", \"ramadan_ratio\"], # \"pasifik_BİSKÜVİ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_KREM ÇİKOLATA/EZME\": [\"indirim__\", \"fiyat\", \"school_day_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_ÇİKOLATA KAPLAMA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"], #\"pasifik_ÇİKOLATA KAPLAMA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"ramadan_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_ÇİKOLATA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"ramadan_ratio\", \"aktivite_tipi\", \"total_holiday_ratio\"],\n",
    "                \"pasifik_KEK\": [\"fiyat\", \"school_day_ratio\", \"ramadan_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_GOFRET\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_ŞEKER\": [\"fiyat\", \"indirim__\", \"total_holiday_ratio\", \"ramadan_ratio\", \"aktivite_tipi\"],\n",
    "                \"pasifik_PASTACILIK ÜRÜNÜ\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\"],\n",
    "                \"pasifik_SAKIZ\": [\"fiyat\", \"indirim__\", \"weekdays_ratio\", \"ramadan_ratio\", \"aktivite_tipi\"],\n",
    "\n",
    "                \"horizon_BİSKÜVİ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_ÇİKOLATA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_ÇİKOLATA KAPLAMA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_KEK\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"back_to_school\", \"ramadan_ratio\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_GOFRET\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_ŞEKER\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_PASTACILIK ÜRÜNÜ\": [\"fiyat\", \"indirim__\", \"ramadan_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"],\n",
    "                \"horizon_KREM ÇİKOLATA/EZME\": [\"fiyat\", \"indirim__\", \"ramadan_ratio\", \"total_holiday_ratio\", \"school_day_ratio\", \"aktivite_tipi\"], \n",
    "                \"horizon_SAKIZ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"total_holiday_ratio\", \"aktivite_tipi\"], \n",
    "\n",
    "                \"btt_BİSKÜVİ\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\"],\n",
    "                \"btt_ÇİKOLATA\": [\"fiyat\", \"indirim__\", \"school_day_ratio\", \"aktivite_tipi\", \"ramadan_ratio\"],\n",
    "                \"btt_ÇİKOLATA KAPLAMA\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"ramadan_ratio\", \"back_to_school\", \"school_day_ratio\"],\n",
    "                \"btt_GOFRET\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"ramadan_ratio\", \"total_holiday_ratio\"],\n",
    "                \"btt_ŞEKER\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"school_day_ratio\", \"total_holiday_ratio\", \"weekdays_ratio\"],\n",
    "                \"btt_PASTACILIK ÜRÜNÜ\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"school_day_ratio\", \"total_holiday_ratio\", \"no_of_days\"],\n",
    "                \"btt_KEK\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"school_day_ratio\", \"total_holiday_ratio\"],\n",
    "                \"btt_KREM ÇİKOLATA/EZME\": [\"fiyat\", \"indirim__\", \"aktivite_tipi\", \"school_day_ratio\"],\n",
    "                \"btt_SAKIZ\": [\"indirim__\", \"aktivite_tipi\", \"total_holiday_ratio\", \"school_day_ratio\"]\n",
    "\n",
    "            }\n",
    "\n",
    "            for key_ in model_input_info.keys():\n",
    "                model_input_info[key_].extend([\"trend\", \"season\", \"indirim__bins\", \"trend_marka\", \"season_marka\", \"koli_new\"])\n",
    "                try:\n",
    "                    model_input_info[key_].remove(\"indirim__\")\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            f_columns = model_input_info[ts_df_sku[\"model_input_info\"].unique()[0]]\n",
    "            ts_df_sku = ts_df_sku[f_columns]\n",
    "        #    print(ts_df_sku.shape)\n",
    "            if (ts_df_sku[\"trend\"].values==ts_df_sku[\"trend_marka\"].values).any():\n",
    "                f_columns.remove(\"trend_marka\")\n",
    "                f_columns.remove(\"season_marka\")\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            ts_df_sku = ts_df_sku[f_columns]\n",
    "            #print(ts_df_sku)\n",
    "\n",
    "            f_transformer = RobustScaler()\n",
    "            koli_transformer = RobustScaler()\n",
    "\n",
    "            y_col = [i for i in f_columns if i == \"koli_new\"]\n",
    "            f_columns.remove(\"koli_new\")\n",
    "\n",
    "            f_transformer = f_transformer.fit(ts_df_sku[f_columns].to_numpy())\n",
    "            koli_transformer = koli_transformer.fit(ts_df_sku[y_col])\n",
    "\n",
    "            ts_df_sku.loc[:, f_columns] = f_transformer.transform(ts_df_sku[f_columns].to_numpy())\n",
    "            ts_df_sku['koli_new'] = koli_transformer.transform(ts_df_sku[y_col])\n",
    "\n",
    "            all_dates = [max_date + relativedelta(months=i) for i in range(1, 5)]\n",
    "            \n",
    "            \n",
    "            time_steps = 3\n",
    "            # reshape to [samples, time_steps, n_features]\n",
    "            X_train, y_train = create_dataset2(ts_df_sku.iloc[:,:-1], ts_df_sku.iloc[:, -1:], time_steps)\n",
    "        #    print(X_train.shape, y_train.shape)\n",
    "\n",
    "            model = keras.Sequential()\n",
    "            model.add(\n",
    "              keras.layers.Bidirectional(\n",
    "                keras.layers.LSTM(\n",
    "                  units=128, \n",
    "                  input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "                )\n",
    "              )\n",
    "            )\n",
    "            model.add(keras.layers.Dropout(rate=learn_rate))\n",
    "            model.add(keras.layers.Dense(units=1))\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "            history = model.fit(X_train, y_train, validation_split=0.1, epochs=30, batch_size=32, verbose=0, shuffle=False)\n",
    "            kntrl = df_all_v8[(df_all_v8[\"grup_adi\"] == grup_sku_unique.loc[idx, \"grup_adi\"]) & (df_all_v8[\"en_guncel_kod\"] == grup_sku_unique.loc[idx, \"en_guncel_kod\"])]\n",
    "            season_trend_add = kntrl[(kntrl[\"date\"] <= max(all_dates) - relativedelta(years=1)) & (kntrl[\"date\"] >= min(all_dates) - relativedelta(years=1))][[\"date\", \"trend\", \"season\", \"trend_marka\", \"season_marka\"]]\n",
    "\n",
    "            future_X_part1 = ts_df_sku.iloc[-3:, :-1]\n",
    "            future_X = ts_df_sku.iloc[-4:, :-1]\n",
    "            future_X.reset_index(inplace=True)\n",
    "\n",
    "            all_cols_unique = []\n",
    "            for i,j in model_input_info.items():\n",
    "                all_cols_unique.extend(j)\n",
    "\n",
    "            all_cols_unique = list(set(all_cols_unique))\n",
    "            all_cols_unique.remove(\"koli_new\")\n",
    "            all_cols_unique.remove(\"indirim__bins\")\n",
    "            all_cols_unique.remove(\"aktivite_tipi\")\n",
    "            calender_cols = [i for i in future_X.columns if i in calender_df and i != \"date\"]\n",
    "\n",
    "            all_dates = [max_date + relativedelta(months=i) for i in range(1, 5)]\n",
    "\n",
    "            future_X[\"date\"] = all_dates\n",
    "            if 'season' in future_X.columns:\n",
    "                future_X.season = season_trend_add.season.values\n",
    "            if 'season_marka' in future_X.columns:\n",
    "                future_X.season_marka = season_trend_add.season_marka.values\n",
    "            if 'total_holiday_ratio' in future_X.columns:\n",
    "                future_X.drop(columns='total_holiday_ratio', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'total_holiday_ratio']], how=\"left\", on=['date'])\n",
    "            if 'back_to_school' in future_X.columns:\n",
    "                future_X.drop(columns='back_to_school', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'back_to_school']], how=\"left\", on=['date'])\n",
    "            if 'ramadan_ratio' in future_X.columns:\n",
    "                future_X.drop(columns='ramadan_ratio', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'ramadan_ratio']], how=\"left\", on=['date'])\n",
    "            if 'weekdays_ratio' in future_X.columns:\n",
    "                future_X.drop(columns='weekdays_ratio', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'weekdays_ratio']], how=\"left\", on=['date'])\n",
    "            if 'school_day_ratio' in future_X.columns:\n",
    "                future_X.drop(columns='school_day_ratio', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'school_day_ratio']], how=\"left\", on=['date'])\n",
    "            if 'no_of_days' in future_X.columns:\n",
    "                future_X.drop(columns='no_of_days', axis=1, inplace=True)\n",
    "                future_X = future_X.merge(calender_df[['date', 'no_of_days']], how=\"left\", on=['date'])\n",
    "            if 'fiyat' in future_X.columns:\n",
    "                future_X.fiyat = [kntrl.tail()[[\"date\", \"fiyat\", \"trend\", \"trend_marka\"]].iloc[-1:, 1].values[0]]*4\n",
    "            if 'trend' in future_X.columns:\n",
    "                future_X.trend = [kntrl.tail()[[\"date\", \"fiyat\", \"trend\", \"trend_marka\"]].iloc[-1:, 2].values[0]]*4\n",
    "            if 'trend_marka' in future_X.columns:\n",
    "                future_X.trend_marka = [kntrl.tail()[[\"date\", \"fiyat\", \"trend\", \"trend_marka\"]].iloc[-1:, 3].values[0]]*4\n",
    "\n",
    "            future_X.loc[1, \"aktivite_tipi\"] = 10\n",
    "            future_X.loc[1, \"indirim__bins\"] = 5\n",
    "            #print(future_X)\n",
    "            future_X.set_index(\"date\", inplace=True)\n",
    "            future_X = future_X[f_columns]\n",
    "\n",
    "            future_X = f_transformer.inverse_transform(future_X)\n",
    "            future_X = np.vstack((future_X_part1.values, future_X))\n",
    "            X = create_dataset_future(future_X, time_steps)\n",
    "            X.shape\n",
    "\n",
    "            y_predT = model.predict(X)\n",
    "\n",
    "            y_predT_inv = koli_transformer.inverse_transform(y_predT)\n",
    "            y_predT_inv2 = [i[0] for i in y_predT_inv]\n",
    "            if y_predT_inv2[0] < 0 or y_predT_inv2[1] < 0 or y_predT_inv2[2] < 0 or y_predT_inv2[3] < 0:\n",
    "                pass\n",
    "            else:\n",
    "                pred_df = pd.DataFrame({\"date\": all_dates,\n",
    "                                        \"grup_adi\": [grup_sku_unique.loc[idx, \"grup_adi\"]]*4,\n",
    "                                        \"en_guncel_kod\": [grup_sku_unique.loc[idx, \"en_guncel_kod\"]]*4,\n",
    "                                        \"pred\": y_predT_inv2})\n",
    "                iterated_results.append(pred_df)\n",
    "    iteration_+= 1\n",
    "iterated_results = pd.concat(iterated_results, ignore_index=True)\n",
    "pred_df = iterated_results.groupby([\"date\", \"grup_adi\", \"en_guncel_kod\"]).agg({\"pred\": \"mean\"}).reset_index()\n",
    "all_final_preds.append(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-32a539d7fdc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_final_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_final_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mall_final_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"grup_adi\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"en_guncel_kod\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \"\"\"\n\u001b[1;32m--> 274\u001b[1;33m     op = _Concatenator(\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    307\u001b[0m     ):\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mABCSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m             raise TypeError(\n\u001b[0m\u001b[0;32m    310\u001b[0m                 \u001b[1;34m\"first argument must be an iterable of pandas \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[1;34mf'objects, you passed an object of type \"{type(objs).__name__}\"'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""
     ]
    }
   ],
   "source": [
    "all_final_preds = pd.concat(all_final_preds, ignore_index=True)\n",
    "all_final_preds.sort_values(by=[\"grup_adi\", \"en_guncel_kod\", \"date\"], ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_final_preds.to_excel(\"../reports/2021_08_01_lstm_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_final_preds.to_excel(\"../reports/lstm_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_final_preds.to_excel(\"../reports/pasifik_2021_11_01_lstm_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_ts = all_final_preds.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], ignore_index=True)[[\"grup_adi\", \"en_guncel_kod\"]]\n",
    "grup_sku_to_add_ma = pd.concat([grup_sku_unique, after_ts]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# Moving Average\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bu kısım önceki versiyonda hazır var kullanıp devam edilecek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_df = df_all_v8[df_all_v8[\"scope\"] == \"moving_average\"]\n",
    "grup_sku_unique_ma = ma_df[[\"grup_adi\", \"en_guncel_kod\"]].drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "grup_sku_unique_ma = pd.concat([grup_sku_unique_ma, grup_sku_to_add_ma], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "grup_akt_dict2_reverse = {}\n",
    "for key_, value_ in grup_akt_dict2.items():\n",
    "    if sum([i for i in grup_akt_dict2[key_].values()]) == 0:\n",
    "        grup_akt_dict2_reverse.update({key_: {0: \"Yok\"}})\n",
    "    else:\n",
    "        tmp_dict = {}\n",
    "        for key_2, value_2 in grup_akt_dict2[key_].items():\n",
    "            tmp_dict.update({value_2: key_2})\n",
    "        grup_akt_dict2_reverse.update({key_: tmp_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_, value_ in grup_akt_dict2_reverse.items():\n",
    "    if \"Yok\" not in grup_akt_dict2_reverse[key_].values():\n",
    "        grup_akt_dict2_reverse[key_].update({0: \"Yok\"})\n",
    "    for idx in range(0, (len(grup_akt_dict2_reverse[key_]))*5, 5):\n",
    "        if idx != 0:\n",
    "            if len(grup_akt_dict2_reverse[key_].values()) > 1 and grup_akt_dict2_reverse[key_][idx] == \"Yok\":\n",
    "                grup_akt_dict2_reverse[key_].update({0: \"Buradakilerin disindaki herhangi bir sey\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_df2 = []\n",
    "for idx in grup_sku_unique_ma.index:\n",
    "    tmp_df = ma_df[(ma_df[\"grup_adi\"] == grup_sku_unique_ma.loc[idx, \"grup_adi\"]) & \n",
    "                   (ma_df[\"en_guncel_kod\"] == grup_sku_unique_ma.loc[idx, \"en_guncel_kod\"])]\n",
    "    tmp_df[\"aktivite_tipi\"] = tmp_df[\"aktivite_tipi\"].map(grup_akt_dict2_reverse[grup_sku_unique_ma.loc[idx, \"grup_adi\"]+\"_\"+str(grup_sku_unique_ma.loc[idx, \"en_guncel_kod\"])])\n",
    "    ma_df2.append(tmp_df)\n",
    "ma_df2 = pd.concat(ma_df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yok                   7416\n",
       "İn&out                 312\n",
       "Mağaza içi/Dağılım      75\n",
       "CRM                     10\n",
       "Kasiyer                  9\n",
       "Mutluluk                 3\n",
       "Çoklu Alım               2\n",
       "Name: aktivite_tipi, dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_aktivite_map = df_all_v6[df_all_v6[\"scope\"] == \"moving_average\"][[\"date\", \"grup_adi\", \"en_guncel_kod\", \"aktivite_tipi\"]].drop_duplicates(ignore_index=True)\n",
    "ma_df2.drop(columns=\"aktivite_tipi\", axis=1, inplace=True)\n",
    "ma_df2 = ma_df2.merge(ma_aktivite_map, how=\"left\")\n",
    "ma_df2.aktivite_tipi.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "_date = []\n",
    "_grup_adi = []\n",
    "_en_guncel_kod = []\n",
    "_aktivite_tipi = []\n",
    "_pred = []\n",
    "\n",
    "date_list = [ma_df2.date.max()+relativedelta(months=i) for i in range(1, 5)]\n",
    "for idx in grup_sku_unique_ma.index:\n",
    "#for idx in [2]:\n",
    "    tmp_df = ma_df2[(ma_df2[\"grup_adi\"] == grup_sku_unique_ma.loc[idx, \"grup_adi\"]) & \n",
    "                    (ma_df2[\"en_guncel_kod\"] == grup_sku_unique_ma.loc[idx, \"en_guncel_kod\"])]\n",
    "    for akt in tmp_df.aktivite_tipi.unique():\n",
    "        if akt == \"Yok\": # aktivite yoksa\n",
    "            if len(tmp_df) > 13:\n",
    "                add_yakin_hist = []\n",
    "                near_hist = []\n",
    "                date_counter = 1\n",
    "                while date_counter < 5:\n",
    "                    date_ = tmp_df.date.max() + relativedelta(months=date_counter)\n",
    "                    \n",
    "                    _date.append(date_)\n",
    "                    _grup_adi.append(grup_sku_unique_ma.loc[idx, \"grup_adi\"])\n",
    "                    _en_guncel_kod.append(grup_sku_unique_ma.loc[idx, \"en_guncel_kod\"])\n",
    "                    _aktivite_tipi.append(akt)\n",
    "                    \n",
    "                    uzak_hist = list(tmp_df[(tmp_df[\"date\"] == date_ - relativedelta(months=2, years=1)) | \n",
    "                                            (tmp_df[\"date\"] == date_ - relativedelta(months=3, years=1)) | \n",
    "                                            (tmp_df[\"date\"] == date_ - relativedelta(months=4, years=1))].koli_new.values)\n",
    "                    \n",
    "                    check_date = date_ - relativedelta(years=1)\n",
    "                    check_koli = tmp_df[tmp_df[\"date\"] == check_date].koli_new.values[0]\n",
    "                    \n",
    "                    multiply_digit = check_koli / np.mean(uzak_hist)\n",
    "                    if multiply_digit < 0.5 or multiply_digit > 2:\n",
    "                        multiply_digit = 1\n",
    "                    \n",
    "                    if date_counter == 1:\n",
    "                        yakin_hist = list(tmp_df[(tmp_df[\"date\"] == date_ - relativedelta(months=2)) | \n",
    "                                                 (tmp_df[\"date\"] == date_ - relativedelta(months=3)) | \n",
    "                                                 (tmp_df[\"date\"] == date_ - relativedelta(months=4))].koli_new.values)\n",
    "                        new_pred_value = np.mean(yakin_hist)*multiply_digit\n",
    "                        _pred.append(new_pred_value)\n",
    "                    elif date_counter == 2:\n",
    "                        yakin_hist = list(tmp_df[(tmp_df[\"date\"] == date_ - relativedelta(months=2)) | \n",
    "                                                 (tmp_df[\"date\"] == date_ - relativedelta(months=3)) | \n",
    "                                                 (tmp_df[\"date\"] == date_ - relativedelta(months=4))].koli_new.values)\n",
    "                        new_pred_value = np.mean(yakin_hist)*multiply_digit\n",
    "                        _pred.append(new_pred_value)\n",
    "                        add_yakin_hist.append(new_pred_value)\n",
    "                    else:\n",
    "                        yakin_hist = list(tmp_df[(tmp_df[\"date\"] == date_ - relativedelta(months=2)) | \n",
    "                                                 (tmp_df[\"date\"] == date_ - relativedelta(months=3)) | \n",
    "                                                 (tmp_df[\"date\"] == date_ - relativedelta(months=4))].koli_new.values)\n",
    "                        yakin_hist.extend(add_yakin_hist)\n",
    "                        new_pred_value = np.mean(yakin_hist)*multiply_digit\n",
    "                        add_yakin_hist.append(new_pred_value)\n",
    "                        _pred.append(new_pred_value)\n",
    "                    \n",
    "                    date_counter+=1\n",
    "                    \n",
    "            else:\n",
    "                date_counter = 1\n",
    "                koli_ort_list = tmp_df.iloc[-3:, :].koli_new.to_list()\n",
    "                while date_counter < 5:\n",
    "                    if len(tmp_df) > 2:\n",
    "                        date_ = tmp_df.date.max() + relativedelta(months=date_counter)\n",
    "                        _date.append(date_)\n",
    "                        _grup_adi.append(grup_sku_unique_ma.loc[idx, \"grup_adi\"])\n",
    "                        _en_guncel_kod.append(grup_sku_unique_ma.loc[idx, \"en_guncel_kod\"])\n",
    "                        _aktivite_tipi.append(akt)\n",
    "\n",
    "                        new_pred = np.mean(koli_ort_list)\n",
    "                        _pred.append(new_pred)\n",
    "                        koli_ort_list.pop(2)\n",
    "                        koli_ort_list.append(new_pred)\n",
    "                        date_counter += 1\n",
    "                    else:\n",
    "                        if len(tmp) == 1:\n",
    "                            date_ = tmp_df.date.max() + relativedelta(months=date_counter)\n",
    "                            _date.append(date_)\n",
    "                            _grup_adi.append(grup_sku_unique_ma.loc[idx, \"grup_adi\"])\n",
    "                            _en_guncel_kod.append(grup_sku_unique_ma.loc[idx, \"en_guncel_kod\"])\n",
    "                            _aktivite_tipi.append(akt)\n",
    "\n",
    "                            \n",
    "                            new_pred = np.mean(koli_ort_list)\n",
    "                            _pred.append(new_pred)\n",
    "                            if date_counter > 2:\n",
    "                                koli_ort_list.pop(2)\n",
    "                            else:\n",
    "                                pass\n",
    "                            koli_ort_list.append(new_pred)\n",
    "                            date_counter += 1\n",
    "                        else:\n",
    "                            date_ = tmp_df.date.max() + relativedelta(months=date_counter)\n",
    "                            _date.append(date_)\n",
    "                            _grup_adi.append(grup_sku_unique_ma.loc[idx, \"grup_adi\"])\n",
    "                            _en_guncel_kod.append(grup_sku_unique_ma.loc[idx, \"en_guncel_kod\"])\n",
    "                            _aktivite_tipi.append(akt)\n",
    "\n",
    "                            \n",
    "                            new_pred = np.mean(koli_ort_list)\n",
    "                            _pred.append(new_pred)\n",
    "                            if date_counter > 1:\n",
    "                                koli_ort_list.pop(2)\n",
    "                            else:\n",
    "                                pass\n",
    "                            koli_ort_list.append(new_pred)\n",
    "                            date_counter += 1\n",
    "\n",
    "        else: # aktivite varsa\n",
    "            tmp_date = tmp_df.date.max()\n",
    "            another_df = tmp_df[tmp_df[\"aktivite_tipi\"] == akt]\n",
    "            mean_with_akt = []\n",
    "            date_counter = 1\n",
    "            while date_counter < 5:\n",
    "                date_ = tmp_date + relativedelta(months=date_counter)\n",
    "                tmp_mean = another_df.koli_new.to_list()\n",
    "                tmp_mean = tmp_mean[date_counter-1:]\n",
    "                tmp_mean.extend(mean_with_akt)\n",
    "                new_pred_value = np.mean(tmp_mean)\n",
    "                mean_with_akt.append(new_pred_value)\n",
    "                \n",
    "                _date.append(date_)\n",
    "                _grup_adi.append(grup_sku_unique_ma.loc[idx, \"grup_adi\"])\n",
    "                _en_guncel_kod.append(grup_sku_unique_ma.loc[idx, \"en_guncel_kod\"])\n",
    "                _aktivite_tipi.append(akt)\n",
    "                _pred.append(new_pred_value)\n",
    "                date_counter+=1\n",
    "                \n",
    "            \n",
    "#        if len(tmp_df[tmp_df[\"aktivite_tipi\"] == akt]) > 0: # aktivite filtrelendiğinde geçmişte varsa\n",
    "#            continue\n",
    "#        else:\n",
    "#            ma_results.date.extend(date_list)\n",
    "#            ma_results.grup_adi.extend([grup_sku_unique_ma.loc[idx, \"grup_adi\"]]*4)\n",
    "#            ma_results.en_guncel_kod.extend([grup_sku_unique_ma.loc[idx, \"grup_adi\"]]*4)\n",
    "#            ma_results.en_guncel_kod.extend([akt]*4)\n",
    "#            ma_results.pred.extend([0]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik fiyatların düzenlenmesi süresi: 0:00:03.079427\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik fiyatların düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_results = pd.DataFrame({\"date\": _date, \n",
    "                           \"grup_adi\": _grup_adi, \n",
    "                           \"en_guncel_kod\": _en_guncel_kod, \n",
    "                           \"aktivite_tipi\": _aktivite_tipi, \n",
    "                           \"pred\": _pred})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ma_results.to_excel(\"../reports/2021_08_01_ort_sonuclari.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_results.to_excel(\"../reports/pasifik_2021_11_01_ort_sonuclari.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
