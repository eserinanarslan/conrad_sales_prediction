{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tunahan.aktas\\Anaconda3\\lib\\site-packages\\mpl_toolkits\\mplot3d\\__init__.py:1: MatplotlibDeprecationWarning: \n",
      "The deprecated function was deprecated in Matplotlib 3.4 and will be removed two minor releases later.\n",
      "  from .axes3d import Axes3D\n"
     ]
    }
   ],
   "source": [
    "# Prior libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "\n",
    "# To Get Combinatiobs\n",
    "import itertools\n",
    "\n",
    "# Datetime Libraries\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "# Trend Seasonality\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SAS Connection Library\n",
    "import swat\n",
    "\n",
    "# In Order To Read Config File\n",
    "import json\n",
    "\n",
    "# Model Preprocess Librarires\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Machine Learning Algorithm Libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Other Libraries\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max.columns\", 100)\n",
    "pd.set_option(\"display.max.rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../config.json\", \"r\")\n",
    "params_ = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_all_process = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Excel Files\n",
    "pas_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"pasifik_satis_path\"]) if doc_.startswith(\"Siparişe_göre_Sales_History\")])\n",
    "hor_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"horizon_satis_path\"]) if doc_.startswith(\"Horizon_Saha_\")])\n",
    "btt_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"btt_satis_path\"]) if doc_.startswith(\"Demand Sensing Sales History\") or doc_.startswith(\"Demand_Sensing_Sales_\")])\n",
    "\n",
    "saha_aktivite_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"horizon_aktivite_path\"]) if doc_.startswith(\"Demand_Sensing_Saha_Aktivit\") or doc_.startswith(\"Demand Sensing Saha Aktivit\")])\n",
    "pasifik_aktivite_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"pasifik_aktivite_path\"]) if doc_.startswith(\"Pasifik Aktivite Datası\")])\n",
    "\n",
    "portfoy_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"portfoy_path\"]) if doc_.startswith(\"Portföy\")])\n",
    "eslenik_kod_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"eslenik_kod_path\"]) if doc_.startswith(\"Ürün Eşlenik kodlar\")])\n",
    "kapsam_listeli = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"kapsam_path\"]) if doc_.startswith(\"Listeli Ürün\")])\n",
    "pas_siparis_lst = sorted([doc_ for doc_ in os.listdir(params_[\"path\"][\"pasifik_siparis_path\"]) if doc_.startswith(\"Siparişe_göre_Sales_History\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosya Listelerini Okuma İşlemi: 0:00:00.043001\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Dosya Listelerini Okuma İşlemi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Read Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historik data ve koli içi adetlerini okuyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales Data\n",
    "pasifik_df_all = []\n",
    "btt_df_all = []\n",
    "horizon_saha_df_all = []\n",
    "chng_cols_beginning = {'Year': 'Yıl', 'Quarter': 'Çeyrek', 'Month': 'Ay', \n",
    "                       'Company Code': 'Şirket Kodu', 'Main Category Name': 'Ana Kategori Adı', \n",
    "                       'Category Name': 'Kategori Adı', 'Brand Name': 'Marka Adı', 'Product Code': 'Ürün Kodu', \n",
    "                       'Product Name': 'Ürün Adı', \"Ürün Adı (Mobis)\": 'Ürün Adı'}\n",
    "for docs_ in btt_lst:\n",
    "    btt_df_all.append(pd.read_excel(params_[\"path\"][\"btt_satis_path\"]+docs_, skiprows=1, sheet_name=\"BTT SAP Satış\", usecols=\"B:N\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "for docs_ in hor_lst:\n",
    "    horizon_saha_df_all.append(pd.read_excel(params_[\"path\"][\"horizon_satis_path\"]+docs_, skiprows=1, sheet_name=\"Horizon Saha Satış\", usecols=\"B:L\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "for docs_ in pas_lst:\n",
    "    pasifik_df_all.append(pd.read_excel(params_[\"path\"][\"pasifik_satis_path\"]+docs_, sheet_name=\"Ürün Bazlı\", usecols=\"B:O\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "# Pasifik ve Horizon historic datasında koli içi adetini bir önceki versiyon olan excelin içerisinden okuyoruz. Bu yüzden siparis klasörünün içindeki dosyalarda gezeceğiz.\n",
    "pasifik_df_koli_ici_adet = [] \n",
    "for docs_ in pas_siparis_lst:\n",
    "    pasifik_df_koli_ici_adet.append(pd.read_excel(params_[\"path\"][\"pasifik_koli_path\"]+docs_, skiprows=1, sheet_name=\"Koli içi adet\", usecols=\"B:D\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "horizon_df_eski = [] \n",
    "for docs_ in btt_lst:\n",
    "    horizon_df_eski.append(pd.read_excel(params_[\"path\"][\"horizon_koli_path\"]+docs_, skiprows=1, sheet_name=\"Horizon Saha Satış\", usecols=\"B:N\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "pasifik_df_all = pd.concat(pasifik_df_all)\n",
    "btt_df_all = pd.concat(btt_df_all)\n",
    "horizon_saha_df_all = pd.concat(horizon_saha_df_all)\n",
    "\n",
    "pasifik_df_koli_ici_adet = pd.concat(pasifik_df_koli_ici_adet)\n",
    "horizon_df_eski = pd.concat(horizon_df_eski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historik datanın Jupytere yüklenme süresi: 0:08:44.131058\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Historik datanın Jupytere yüklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sadece Gerekli Sütunlar Tutuluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all.drop(columns=[\"Organizasyon Kodu\", \"Grup Kodu.\", \"Pladis-Non Pladis\"], axis=1, inplace=True)\n",
    "horizon_saha_df_all.drop(columns=[\"Çeyrek\"], axis=1, inplace=True)\n",
    "btt_df_all.drop(columns=[\"Çeyrek\", \"Şirket Kodu\"], axis=1, inplace=True)\n",
    "\n",
    "btt_df_all[\"Grup Adı\"] = \"BTT\"\n",
    "pasifik_df_all.rename(columns={\"Ana Kategori\": \"Ana Kategori Adı\", \"Kategori\": \"Kategori Adı\", \"Ürün Adı (Orjinal)\": \"Ürün Adı\", \"Sipariş Miktarı(Dönüş. Koli)\": \"Koli\", \n",
    "                               \"Sipariş Brüt Tutar\": \"KG\", \"Sipariş Brüt KG\": \"TL\"}, inplace=True)\n",
    "\n",
    "horizon_saha_df_all.rename(columns={\"Horizon müşteri grup\": \"Grup Adı\", \"Ürün Adı (Orjinal)\": \"Ürün Adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik için koli içi adetler \"Koli içi adet\" sheetinde tutuluyor. Buradan alıyoruz fakat historik datada bulunmayan ürün kodları da var. Bu ürün kodlarından bazıları alfabetik harfler içeriyor. Bu durumu ortadan kaldırmak için aşağıdaki işlemi uyguluyoruz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['Yıl', 'Ay', 'Grup Adı', 'Ana Kategori Adı', 'Kategori Adı', 'Marka Adı', 'Ürün Kodu', 'Ürün Adı', 'Koli İçi Adet', 'Koli', 'KG', 'TL']\n",
    "ltrs = list(string.ascii_letters) # Alfabede bulunan tüm harfleri tutan liste. Bunu, koli içi adet dataframe'deki harf içeren ürün kodlarını elemek için tutuyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_koli_ici_adet.drop_duplicates(subset=pasifik_df_koli_ici_adet.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True) # Her dosyada 300k'ya yakın satır var. Yaklaşık 2M kadar satır geliyor çünkü 5-6 yıllık data okuyoruz. Bu yüzden drop duplicates ile satır sayısını azaltıyoruz.\n",
    "pasifik_df_koli_ici_adet = pasifik_df_koli_ici_adet[~(pasifik_df_koli_ici_adet[\"Ürün Kodu\"].str.contains(\"|\".join(ltrs), regex=True))] # Alfabetik harflerle başlayan ürün kodlarını elemine ediyoruz.\n",
    "\n",
    "pasifik_df_koli_ici_adet[\"Ürün Kodu\"] = pasifik_df_koli_ici_adet[\"Ürün Kodu\"].str.replace(\" \", \"\") # Bazı ürün kodları 0015 01 şeklinde gelmiş. Yani nümerik gözükse de arada boşluk var. O yüzden boşlukları kaldırıyoruz.\n",
    "pasifik_df_koli_ici_adet[\"Ürün Kodu\"] = pasifik_df_koli_ici_adet[\"Ürün Kodu\"].astype(\"int64\") # Bir üst satırda boşlukları kaldırdıktan sonra integer hale getiriyoruz.\n",
    "pasifik_df_koli_ici_adet = pasifik_df_koli_ici_adet[[\"Ürün Kodu\", \"Koli İçi Adet\"]] # Bu iki satır kalabilir. Left join ile historik dataya ekleyeceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all.merge(pasifik_df_koli_ici_adet, on=\"Ürün Kodu\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her dosyada; Ürün Adı sütununda \"sum:\" diye bir gözlem bulunuyor. Dip toplam yapıp datayı atmışlar. Bu satırları sildim.\n",
    "horizon_saha_df_all = horizon_saha_df_all[~(horizon_saha_df_all[\"Ürün Adı\"].str.contains(\"Sum:|sum:\"))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_kategori = horizon_df_eski[['Ürün Kodu','Kategori Adı']]\n",
    "horizon_kategori.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "horizon_df_koli_ici_adet = horizon_df_eski[['Yıl','Ay','Ürün Kodu','Koli İçi Adet']]\n",
    "horizon_df_koli_ici_adet.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all = horizon_saha_df_all.merge(horizon_kategori, how='left', on='Ürün Kodu')\n",
    "horizon_saha_df_all = horizon_saha_df_all.merge(horizon_df_koli_ici_adet, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[col_order]\n",
    "horizon_saha_df_all = horizon_saha_df_all[col_order]\n",
    "btt_df_all = btt_df_all[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saha Aktiviteleri\n",
    "saha_aktivite_detay = []\n",
    "saha_aktivite_cat = []\n",
    "\n",
    "for docs_ in saha_aktivite_lst:\n",
    "    saha_aktivite_detay.append(pd.read_excel(params_[\"path\"][\"horizon_aktivite_path\"]+docs_, skiprows=1, sheet_name=\"Ürün Detay\", usecols=\"B:M\"))\n",
    "    saha_aktivite_cat.append(pd.read_excel(params_[\"path\"][\"horizon_aktivite_path\"]+docs_, skiprows=1, sheet_name=\"Kategori\", usecols=\"B:I\"))\n",
    "saha_aktivite_detay = pd.concat(saha_aktivite_detay)\n",
    "saha_aktivite_cat = pd.concat(saha_aktivite_cat)\n",
    "\n",
    "saha_aktivite_detay.drop_duplicates(subset=[\"Yıl\", \"Ay\", \"Saha Müşteri Grup\", \"Ürün Kodu\"], keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Pasifik Aktiviteleri\n",
    "pasifik_aktivite_df = pd.read_excel(params_[\"path\"][\"pasifik_aktivite_path\"]+pasifik_aktivite_lst[0])\n",
    "\n",
    "pasifik_aktivite_df.drop_duplicates(subset=[\"Yıl\", \"Ay\", \"Müşteri Grup\", \"Ürün Kodu\"], keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Fiyat Listesi\n",
    "fiyat_lst_pasifik = pd.read_excel(params_[\"path\"][\"fiyat_listesi_path\"]+params_[\"files\"][\"pasifik_fiyat_file\"])\n",
    "fiyat_lst_horizon = pd.read_excel(params_[\"path\"][\"fiyat_listesi_path\"]+params_[\"files\"][\"horizon_fiyat_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Portföy\n",
    "pasifik_portfoy_df = pd.read_excel(params_[\"path\"][\"portfoy_path\"]+portfoy_lst[0], sheet_name=\"Pasifik Portföy\", skiprows=3, usecols=\"D:H\")\n",
    "btt_portfoy_df = pd.read_excel(params_[\"path\"][\"portfoy_path\"]+portfoy_lst[0], sheet_name=\"BTT Portföy\", skiprows=2, usecols=\"D:H\")\n",
    "horizon_portfoy_df = pd.read_excel(params_[\"path\"][\"portfoy_path\"]+portfoy_lst[0], sheet_name=\"Horizon Portföy\", skiprows=2, usecols=\"E:I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Eşlenik Kodları\n",
    "eslenik_kod_df = pd.read_excel(params_[\"path\"][\"eslenik_kod_path\"]+eslenik_kod_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Calender\n",
    "calender_df = pd.read_excel(params_[\"path\"][\"calender_path\"]+params_[\"files\"][\"calender_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Plasiyer Yarışma\n",
    "yarisma_df = pd.read_excel(params_[\"path\"][\"yarisma_path\"]+params_[\"files\"][\"yarisma_file\"], sheet_name=\"yarisma_historik_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eslenik_kod_df[\"En Güncel Kod\"] = eslenik_kod_df[\"En Güncel Kod\"].apply(lambda x: int(x) if x not in ['delist ', \"delist\", \"Delist\"] else x.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a101_kapsam = pd.read_excel(params_[\"path\"][\"kapsam_path\"]+kapsam_listeli[0], sheet_name=\"A101 Portföy\")\n",
    "sok_kapsam = pd.read_excel(params_[\"path\"][\"kapsam_path\"]+kapsam_listeli[0], sheet_name=\"Şok Portföy\")\n",
    "bim_kapsam = pd.read_excel(params_[\"path\"][\"kapsam_path\"]+kapsam_listeli[0], sheet_name=\"Bim Portföy\")\n",
    "\n",
    "a101_kapsam[\"grup_adi\"] = \"A101\"\n",
    "sok_kapsam[\"grup_adi\"] = \"ŞOK\"\n",
    "bim_kapsam[\"grup_adi\"] = \"BİM\"\n",
    "\n",
    "kapsam_all = pd.concat([a101_kapsam, sok_kapsam, bim_kapsam], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Enflasyon\n",
    "enflasyon_df = pd.read_html(params_[\"path\"][\"enflasyon_path\"])[0]\n",
    "enflasyon_df.columns=[\"date\", \"enflasyon_etkisi\", \"degisim\"]\n",
    "enflasyon_df.drop(\"degisim\", axis=1, inplace=True)\n",
    "enflasyon_df[\"date\"] = enflasyon_df[\"date\"].apply(lambda x: \"01-\"+x)\n",
    "enflasyon_df[\"date\"] = pd.to_datetime(enflasyon_df[\"date\"], format=\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Koli Birim Ağırlıkları\n",
    "koli_birim_agirlik = pd.read_excel(params_[\"files\"][\"koli_agirlik_birim_file\"])\n",
    "koli_birim_agirlik_pas = pd.read_excel(params_[\"files\"][\"koli_agirlik_birim_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "koli_birim_agirlik.rename(columns={\"Malzeme\": \"en_guncel_kod\", \n",
    "                                   \"Malzeme Açıklaması\": \"urun_adi\", \n",
    "                                   \"Ana Kategori\": \"ana_kategori_adi\",\n",
    "                                   \"Kategori\": \"kategori_adi\",\n",
    "                                   \"Alt Kategori\": \"alt_kategori_adi\",\n",
    "                                   \"Detay Kategori\": \"detay_kategori_adi\",\n",
    "                                   \"Marka\": \"marka_adi\", \"Marka Açıklaması\": \"marka_aciklamasi\",\n",
    "                                   \"Net Ağırlık\": \"kg\"}, inplace=True)\n",
    "\n",
    "koli_birim_agirlik_pas.rename(columns={\"Malzeme\": \"en_guncel_kod\", \n",
    "                                   \"Malzeme Açıklaması\": \"urun_adi\", \n",
    "                                   \"Ana Kategori\": \"ana_kategori_adi\",\n",
    "                                   \"Kategori\": \"kategori_adi\",\n",
    "                                   \"Alt Kategori\": \"alt_kategori_adi\",\n",
    "                                   \"Detay Kategori\": \"detay_kategori_adi\",\n",
    "                                   \"Marka\": \"marka_adi\", \"Marka Açıklaması\": \"marka_aciklamasi\",\n",
    "                                   \"Net Ağırlık\": \"kg\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "koli_birim_agirlik_pas[\"kanal\"] = \"pasifik\"\n",
    "koli_birim_agirlik_hor = koli_birim_agirlik.copy()\n",
    "koli_birim_agirlik_btt = koli_birim_agirlik.copy()\n",
    "koli_birim_agirlik_hor[\"kanal\"] = \"horizon\"\n",
    "koli_birim_agirlik_btt[\"kanal\"] = \"btt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "koli_birim_agirlik = pd.concat([koli_birim_agirlik_pas, koli_birim_agirlik_hor, koli_birim_agirlik_btt], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diğer dataların Jupytere yüklenme süresi: 0:01:03.046544\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Diğer dataların Jupytere yüklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifik 2016 aktivite verileri olmadığı için 2016 Sales dataları çıkartıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[pasifik_df_all[\"Yıl\"] != 2016].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasifik_df_all = pasifik_df_all[~((pasifik_df_all[\"Yıl\"] == 2021) & (pasifik_df_all[\"Ay\"].isin([6, 7, 8, 9])))].reset_index(drop=True)\n",
    "#horizon_saha_df_all = horizon_saha_df_all[~((horizon_saha_df_all[\"Yıl\"] == 2021) & (horizon_saha_df_all[\"Ay\"].isin([6, 7, 8, 9])))].reset_index(drop=True)\n",
    "#btt_df_all = btt_df_all[~((btt_df_all[\"Yıl\"] == 2021) & (btt_df_all[\"Ay\"].isin([6, 7, 8, 9])))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all[\"Yıl\"] = horizon_saha_df_all[\"Yıl\"].astype(int)\n",
    "horizon_saha_df_all[\"Ay\"] = horizon_saha_df_all[\"Ay\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Datası İçin Ürün Kod Eşleme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "kategori_adi = btt_df_all.drop_duplicates(subset=[\"Marka Adı\", \"Kategori Adı\"], keep=\"first\")[[\"Marka Adı\", \"Kategori Adı\"]]\n",
    "kategori_adi.sort_values(by=[\"Marka Adı\", \"Kategori Adı\"], ignore_index=True, inplace=True)\n",
    "kategori_adi = dict(kategori_adi.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all[\"Kategori Adı\"] = horizon_saha_df_all[\"Kategori Adı\"].fillna(horizon_saha_df_all[\"Marka Adı\"].map(kategori_adi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pd.merge(pasifik_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "horizon_saha_df_all = pd.merge(horizon_saha_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "btt_df_all = pd.merge(btt_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ürün Eşleme Kodları dosyasında yer almayan kodlar için mevcut ürün kodları verildi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_ = pasifik_df_all[pd.isnull(pasifik_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = pasifik_df_all[~pd.isnull(pasifik_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "pasifik_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "pasifik_df_all = pasifik_df_all.sort_values(pasifik_df_all.columns.to_list()).reset_index(drop=True)\n",
    "\n",
    "empty_ = btt_df_all[pd.isnull(btt_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = btt_df_all[~pd.isnull(btt_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "btt_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "btt_df_all = btt_df_all.sort_values(btt_df_all.columns.to_list()).reset_index(drop=True)\n",
    "\n",
    "empty_ = horizon_saha_df_all[pd.isnull(horizon_saha_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = horizon_saha_df_all[~pd.isnull(horizon_saha_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "horizon_saha_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all.sort_values(horizon_saha_df_all.columns.to_list()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adet adında yeni bir kolon oluşturuldu. Koli Sayısı 100'den az olanlara 0 yazıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all[\"Adet\"] = pasifik_df_all[\"Koli İçi Adet\"] * pasifik_df_all[\"Koli\"]\n",
    "btt_df_all[\"Adet\"] = btt_df_all[\"Koli İçi Adet\"] * btt_df_all[\"Koli\"]\n",
    "horizon_saha_df_all[\"Adet\"] = horizon_saha_df_all[\"Koli İçi Adet\"] * horizon_saha_df_all[\"Koli\"]\n",
    "\n",
    "pasifik_df_all[\"Adet\"] = pasifik_df_all[\"Adet\"] = np.where(pasifik_df_all[\"Koli\"] < 100, 1, pasifik_df_all[\"Adet\"])\n",
    "btt_df_all[\"Adet\"] = btt_df_all[\"Adet\"] = np.where(btt_df_all[\"Koli\"] < 100, 1, btt_df_all[\"Adet\"])\n",
    "horizon_saha_df_all[\"Adet\"] = horizon_saha_df_all[\"Adet\"] = np.where(horizon_saha_df_all[\"Koli\"] < 100, 1, horizon_saha_df_all[\"Adet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delist olan ürünler veriden çıkartıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[pasifik_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "btt_df_all = btt_df_all[btt_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all[horizon_saha_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aynı yıl, ay, grup adı, ana kategori adı, kategori adı, marka adı ve SKU kodundaki ürünler için toplam alındı. Sadece Koli İçi Adet için maksimum olan alındı."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marka adı dahil değil groupby'a\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_to_sum = {\"Koli İçi Adet\": \"sum\", \"Koli\": \"sum\", \"KG\": \"sum\", \"TL\": \"sum\", \"Adet\": \"sum\"}\n",
    "\n",
    "pasifik_df_all2 = pasifik_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()\n",
    "btt_df_all2 = btt_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()\n",
    "horizon_saha_df_all2 = horizon_saha_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2[\"Date\"] = pasifik_df_all2[\"Yıl\"].astype(str) + \"-\" +  pasifik_df_all2[\"Ay\"].astype(str) + \"-01\"\n",
    "btt_df_all2[\"Date\"] = btt_df_all2[\"Yıl\"].astype(int).astype(str) + \"-\" +  btt_df_all2[\"Ay\"].astype(int).astype(str) + \"-01\"\n",
    "horizon_saha_df_all2[\"Date\"] = horizon_saha_df_all2[\"Yıl\"].astype(int).astype(str) + \"-\" +  horizon_saha_df_all2[\"Ay\"].astype(int).astype(str) + \"-01\"\n",
    "\n",
    "pasifik_df_all2[\"Date\"] = pd.to_datetime(pasifik_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "btt_df_all2[\"Date\"] = pd.to_datetime(btt_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "horizon_saha_df_all2[\"Date\"] = pd.to_datetime(horizon_saha_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon ve Pasifikte bulunan \"Diğer\"'lerin yanlarına \"_\" ile Diğer_Pasifik, Diğer_Horizon yazıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2[\"Grup Adı\"] = pasifik_df_all2[\"Grup Adı\"].apply(lambda x: \"Diğer_Pasifik\" if x == \"Diğer\" else x)\n",
    "horizon_saha_df_all2[\"Grup Adı\"] = horizon_saha_df_all2[\"Grup Adı\"].apply(lambda x: \"Diğer_Horizon\" if x == \"Diğer\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all2 = pd.concat([pasifik_df_all2, horizon_saha_df_all2, btt_df_all2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifik Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_urun_isimleri = pasifik_df_all[[\"Marka Adı\", \"Ürün Adı\", \"En Güncel Kod\"]].drop_duplicates(subset=[\"Marka Adı\", \"En Güncel Kod\"],ignore_index=True,keep=\"first\")\n",
    "pasifik_urun_isimleri = pasifik_urun_isimleri[~((pasifik_urun_isimleri[\"Marka Adı\"] == \"DANKEK BATON\") & (pasifik_urun_isimleri[\"En Güncel Kod\"] == 80403))]\n",
    "pasifik_df_all2 = pd.merge(pasifik_df_all2, pasifik_urun_isimleri[[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"]].drop_duplicates(subset=[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"], keep=\"first\"), on=\"En Güncel Kod\", how=\"left\")\n",
    "pasifik_df_all2 = pasifik_df_all2[pasifik_df_all2.columns.to_list()[:5]+pasifik_df_all2.columns.to_list()[-2:]+[pasifik_df_all2.columns.to_list()[5]]+pasifik_df_all2.columns.to_list()[6:12]]\n",
    "\n",
    "\n",
    "\n",
    "horizon_urun_isimleri = horizon_saha_df_all[[\"Marka Adı\", \"Ürün Adı\", \"En Güncel Kod\"]].drop_duplicates(subset=[\"Marka Adı\", \"En Güncel Kod\"],ignore_index=True,keep=\"first\")\n",
    "horizon_urun_isimleri = horizon_urun_isimleri[~(((horizon_urun_isimleri[\"Marka Adı\"] == \"DANKEK BATON\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 80403)) | \n",
    "                                                ((horizon_urun_isimleri[\"Marka Adı\"] == \"MAVİ YEŞİL\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 11802)) |\n",
    "                                                ((horizon_urun_isimleri[\"Marka Adı\"] == \"MAVİ YEŞİL\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 74306)) |\n",
    "                                                ((horizon_urun_isimleri[\"Marka Adı\"] == \"AS KRAKER\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 190502)))]\n",
    "horizon_saha_df_all2 = pd.merge(horizon_saha_df_all2, horizon_urun_isimleri[[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "horizon_saha_df_all2 = horizon_saha_df_all2[horizon_saha_df_all2.columns.to_list()[:5]+horizon_saha_df_all2.columns.to_list()[-2:]+[horizon_saha_df_all2.columns.to_list()[5]]+horizon_saha_df_all2.columns.to_list()[6:12]]\n",
    "\n",
    "\n",
    "\n",
    "btt_urun_isimleri = btt_df_all[[\"Marka Adı\", \"Ürün Adı\", \"En Güncel Kod\"]].drop_duplicates(subset=[\"Marka Adı\", \"En Güncel Kod\"],ignore_index=True,keep=\"first\")\n",
    "btt_urun_isimleri = btt_urun_isimleri[~((btt_urun_isimleri[\"Marka Adı\"] == \"DANKEK BATON\") & (btt_urun_isimleri[\"En Güncel Kod\"] == 80403))]\n",
    "btt_df_all2 = pd.merge(btt_df_all2, btt_urun_isimleri[[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"]].drop_duplicates(subset=[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"], keep=\"first\"), on=\"En Güncel Kod\", how=\"left\")\n",
    "btt_df_all2 = btt_df_all2[btt_df_all2.columns.to_list()[:5]+btt_df_all2.columns.to_list()[-2:]+[btt_df_all2.columns.to_list()[5]]+btt_df_all2.columns.to_list()[6:12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all_filled = []\n",
    "for idx, test in pasifik_df_all2.groupby([\"En Güncel Kod\", \"Yıl\", \"Grup Adı\"]):\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, 13):\n",
    "        try:\n",
    "            if i == test.loc[i-1, \"Ay\"]:\n",
    "                if i == 13:\n",
    "                    break\n",
    "            else:\n",
    "                test.loc[-1] = test.loc[0]\n",
    "                test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "                test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "                test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        except:\n",
    "            test.loc[-1] = test.loc[0]\n",
    "            test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "            test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "            test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        test[\"Date\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    rows_to_drop = []\n",
    "    start = test.index[0]\n",
    "    length = 1\n",
    "    while (test.loc[start, \"Adet\"] == 1) and (length < len(test)):\n",
    "        rows_to_drop.append(start)\n",
    "        length+=1\n",
    "        start+=1\n",
    "    test.drop(index=rows_to_drop, inplace=True)\n",
    "    pasifik_df_all_filled.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2 = pd.concat(pasifik_df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik missing value düzenlenmesi süresi: 0:04:04.553430\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Horizon Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all_filled = []\n",
    "for idx, test in horizon_saha_df_all2.groupby([\"En Güncel Kod\", \"Yıl\", \"Grup Adı\"]):\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, 13):\n",
    "        try:\n",
    "            if i == test.loc[i-1, \"Ay\"]:\n",
    "                if i == 13:\n",
    "                    break\n",
    "            else:\n",
    "                test.loc[-1] = test.loc[0]\n",
    "                test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "                test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "                test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        except:\n",
    "            test.loc[-1] = test.loc[0]\n",
    "            test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "            test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "            test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        test[\"Date\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    \n",
    "    rows_to_drop = []\n",
    "    start = test.index[0]\n",
    "    length = 1\n",
    "    while (test.loc[start, \"Adet\"] == 1) and (length < len(test)):\n",
    "        rows_to_drop.append(start)\n",
    "        length+=1\n",
    "        start+=1\n",
    "    test.drop(index=rows_to_drop, inplace=True)\n",
    "    horizon_saha_df_all_filled.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all2 = pd.concat(horizon_saha_df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon missing value düzenlenmesi süresi: 0:12:20.034902\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# BTT Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_all_filled = []\n",
    "for idx, test in btt_df_all2.groupby([\"En Güncel Kod\", \"Yıl\", \"Grup Adı\"]):\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, 13):\n",
    "        try:\n",
    "            if i == test.loc[i-1, \"Ay\"]:\n",
    "                if i == 13:\n",
    "                    break\n",
    "            else:\n",
    "                test.loc[-1] = test.loc[0]\n",
    "                test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "                test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "                test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        except:\n",
    "            test.loc[-1] = test.loc[0]\n",
    "            test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "            test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "            test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        test[\"Date\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "    rows_to_drop = []\n",
    "    start = test.index[0]\n",
    "    length = 1\n",
    "    while (test.loc[start, \"Adet\"] == 1) and (length < len(test)):\n",
    "        rows_to_drop.append(start)\n",
    "        length+=1\n",
    "        start+=1\n",
    "    test.drop(index=rows_to_drop, inplace=True)\n",
    "    btt_df_all_filled.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_all2 = pd.concat(btt_df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTT missing value düzenlenmesi süresi: 0:01:29.922050\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_horizon_saha_df_all2 = horizon_saha_df_all2.copy()\n",
    "new_pasifik_df_all2 = pasifik_df_all2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_horizon_saha_df_all2['Yıl'] = new_horizon_saha_df_all2['Yıl'].astype(int)\n",
    "new_horizon_saha_df_all2['Ay'] = new_horizon_saha_df_all2['Ay'].astype(int)\n",
    "new_horizon_saha_df_all2['En Güncel Kod'] = new_horizon_saha_df_all2['En Güncel Kod'].astype(int)\n",
    "new_horizon_saha_df_all2['Date'] = new_horizon_saha_df_all2['Yıl'].astype(str) + '-' + new_horizon_saha_df_all2['Ay'].astype(str) + '-01'\n",
    "new_horizon_saha_df_all2[\"Date\"] = pd.to_datetime(new_horizon_saha_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all22 = pd.concat([new_pasifik_df_all2, new_horizon_saha_df_all2, btt_df_all2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aktivite Datası İçin Ürün Kod Eşleme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik Aktivite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left join ile güncel kodlar getirildi. Delist olan ürünler listeden çıkartıldı. \"Çeyrek\" sütunu silindi. En güncel kod sütunnuda bulunamayan değerler Ürün Kodu sütunundan çekildi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_aktivite_df2 = pd.merge(pasifik_aktivite_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "pasifik_aktivite_df2 = pasifik_aktivite_df2[pasifik_aktivite_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "pasifik_aktivite_df2.drop(\"Çeyrek\", inplace=True, axis=1)\n",
    "pasifik_aktivite_df2['En Güncel Kod'] = pasifik_aktivite_df2['En Güncel Kod'].fillna(pasifik_aktivite_df2['Ürün Kodu'])\n",
    "pasifik_aktivite_df2.drop(columns=\"Ürün Kodu\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik Aktivite Ciro - Promosyon Tutarı ve İskonto Tekilleştirme (ORTALAMA ALARAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_pas = {\"Raf Tavsiye Satış Fiyatı\": \"mean\", \"İndirimli Raf Satış Fiyatı\": \"mean\", \"İndirim %\": \"mean\", \"Aktivite Tipi\": \"first\"}\n",
    "pasifik_aktivite_df3 = pasifik_aktivite_df2.groupby([\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Müşteri Grup\"]).agg(ort_pas).reset_index()\n",
    "pasifik_aktivite_df3 = pd.merge(pasifik_aktivite_df3, pasifik_aktivite_df2[[\"Yıl\", \"Ay\", \"Müşteri Grup\", \"En Güncel Kod\", \n",
    "                                                                            \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\"]],\n",
    "                                how=\"left\", \n",
    "                                on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Müşteri Grup\"])\n",
    "\n",
    "pasifik_aktivite_df3.drop_duplicates(subset=pasifik_aktivite_df3.columns.to_list(), inplace=True)\n",
    "pasifik_aktivite_df3.reset_index(drop=True, inplace=True)\n",
    "pasifik_aktivite_df3 = pasifik_aktivite_df3[pasifik_aktivite_df2.drop(\"Ürün Adı\", axis=1).columns.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon Aktivite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay2 = pd.merge(saha_aktivite_detay, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "saha_aktivite_detay2 = saha_aktivite_detay2[saha_aktivite_detay2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "saha_aktivite_detay2.drop(\"Çeyrek\", inplace=True, axis=1)\n",
    "saha_aktivite_detay2['En Güncel Kod'] = saha_aktivite_detay2['En Güncel Kod'].fillna(saha_aktivite_detay2['Ürün Kodu'])\n",
    "saha_aktivite_detay2.drop(columns=\"Ürün Kodu\", axis=1, inplace=True)\n",
    "saha_aktivite_detay2[\"İskonto %\"].replace(\"#DIV/0\", np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon Aktivite Ciro - Promosyon Tutarı ve İskonto Tekilleştirme (ORTALAMA ALARAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort = {\"Ciro (Kull. İade Düş.)\": \"mean\", \"Promosyon Tutarı\": \"mean\", \"İskonto %\": \"mean\"}\n",
    "saha_aktivite_detay3 = saha_aktivite_detay2.groupby([\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Saha Müşteri Grup\"]).agg(ort).reset_index()\n",
    "\n",
    "saha_aktivite_detay3 = pd.merge(saha_aktivite_detay3, saha_aktivite_detay2[[\"Yıl\", \"Ay\", \"Saha Müşteri Grup\", \"En Güncel Kod\", \n",
    "                                                 \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\"]],\n",
    "                           how=\"left\", \n",
    "                           on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Saha Müşteri Grup\"])\n",
    "\n",
    "saha_aktivite_detay3.drop_duplicates(subset=saha_aktivite_detay3.columns.to_list(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3 = saha_aktivite_detay3[saha_aktivite_detay2.drop(\"Ürün Adı (Mobis)\", axis=1).columns.to_list()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3.rename(columns={\"Saha Müşteri Grup\": \"Grup Adı\"}, inplace=True)\n",
    "saha_aktivite_detay3[\"Grup Adı\"] = saha_aktivite_detay3[\"Grup Adı\"].apply(lambda x: \"Diğer_Horizon\" if x == \"Diğer\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiyat Listesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Fiyatları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon.drop_duplicates(subset=fiyat_lst_horizon.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].str.replace(\"-\", \"\")\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].astype(int)*1\n",
    "except:\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].astype(int)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df = fiyat_lst_horizon.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df[\"Baslangic_Yıl\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.year)\n",
    "fiyat_lst_horizon_df[\"Baslangic_Ay\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.month)\n",
    "fiyat_lst_horizon_df[\"Baslangic_Gun\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.day)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.year)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Ay\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.month)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Gun\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df[\"Baslangic_Yıl\"] = fiyat_lst_horizon_df[\"Baslangic_Yıl\"].apply(lambda x: (horizon_saha_df_all2[\"Date\"].max().year)+1 if x > horizon_saha_df_all2[\"Date\"].max().year else x)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"] = fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"].apply(lambda x: (horizon_saha_df_all2[\"Date\"].max().year)+1 if x > horizon_saha_df_all2[\"Date\"].max().year else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = horizon_saha_df_all2[\"Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fiyat_unique = []\n",
    "\n",
    "for malzeme in fiyat_lst_horizon_df[\"Malzeme\"].unique():\n",
    "    temp_time_df = pd.DataFrame({\"Fiyat\": [np.nan]}, index=time_index)\n",
    "    temp_time_df = temp_time_df.reset_index().rename(columns={\"index\":\"date\"})    \n",
    "    temp_time_df[\"En Güncel Kod\"] = malzeme\n",
    "    temp_time_df[\"fiyat_gecisi\"] = 0\n",
    "    malzeme_df = fiyat_lst_horizon_df[fiyat_lst_horizon_df[\"Malzeme\"] == malzeme].reset_index(drop=True)\n",
    "    malzeme_df.drop(columns=[\"KşTü\", \"Koşul türü\", \"Tanım\", \"Ana Kategori\", \"Kategori\", \"ÖB\"], axis=1, inplace=True)\n",
    "    malzeme_df.drop_duplicates(subset=malzeme_df.columns.to_list(), inplace=True, ignore_index=True)\n",
    "    malzeme_df.sort_values(by=[\"Baslangic_Yıl\", \"Baslangic_Ay\", \"Baslangic_Gun\"], ignore_index=True, inplace=True)\n",
    "    check_idx1 = []\n",
    "    if len(malzeme_df) > 1:\n",
    "        for row1 in malzeme_df.index:\n",
    "            for row2 in malzeme_df[row1+1:].index:\n",
    "                if (malzeme_df.loc[row1][\"Gecerlilik_Yıl\"] == malzeme_df.loc[row2][\"Baslangic_Yıl\"]) and (malzeme_df.loc[row1][\"Gecerlilik_Ay\"] == malzeme_df.loc[row2][\"Baslangic_Ay\"]):\n",
    "                    num_days = calendar.monthrange(int(malzeme_df.loc[row2][\"Baslangic_Yıl\"]), int(malzeme_df.loc[row2][\"Baslangic_Ay\"]))[1]\n",
    "                    fyt=((int(malzeme_df.loc[row1][\"Gecerlilik_Gun\"])*malzeme_df.loc[row1][\"     Tutar\"]) + (num_days - int(malzeme_df.loc[row2][\"Baslangic_Gun\"]) + 1)*malzeme_df.loc[row2][\"     Tutar\"])/num_days\n",
    "\n",
    "                    end_idx1 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx1 = temp_time_df[temp_time_df[\"date\"] == end_idx1].index\n",
    "                    temp_time_df.loc[final_idx1, \"Fiyat\"] = fyt\n",
    "                    temp_time_df.loc[final_idx1,\"fiyat_gecisi\"] = 1\n",
    "\n",
    "                elif (malzeme_df.loc[row1, \"Gecerlilik_Gun\"] == calendar.monthrange(int(malzeme_df.loc[row1][\"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1][\"Gecerlilik_Ay\"]))[1] \\\n",
    "                     and malzeme_df.loc[row2, \"Baslangic_Gun\"] == 1):\n",
    "                    fyt5=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                    fyt6=malzeme_df.loc[row2][\"     Tutar\"]\n",
    "                    end_idx5 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    end_idx6 =  datetime(int(malzeme_df.loc[row2, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row2, \"Baslangic_Ay\"]), 1)\n",
    "                    final_idx5 = temp_time_df[temp_time_df[\"date\"] == end_idx5].index\n",
    "                    final_idx6 = temp_time_df[temp_time_df[\"date\"] == end_idx6].index\n",
    "                    temp_time_df.loc[final_idx5, \"Fiyat\"] = fyt5\n",
    "                    temp_time_df.loc[final_idx6, \"Fiyat\"] = fyt6\n",
    "\n",
    "                else:\n",
    "                    fyt2=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                    start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx2 = temp_time_df[(temp_time_df[\"date\"] > start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                    temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "            if (row1 == len(malzeme_df)-1) or (row1 == len(malzeme_df)-2):\n",
    "                fyt3=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                final_idx3 = temp_time_df[(temp_time_df[\"date\"] > start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "\n",
    "    else:\n",
    "        for row1 in malzeme_df.index:\n",
    "            fyt4=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "            start_idx4 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "            end_idx4 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "            final_idx4 = temp_time_df[(temp_time_df[\"date\"] >= start_idx4) & (temp_time_df[\"date\"] <= end_idx4)].index\n",
    "            temp_time_df.loc[final_idx4, \"Fiyat\"] = fyt4\n",
    "\n",
    "    if (malzeme_df.loc[0, \"Baslangic_Yıl\"] >= min(horizon_saha_df_all2[\"Yıl\"].unique())) and (len(malzeme_df) > 1):\n",
    "        temp_time_df.loc[temp_time_df[~pd.isnull(temp_time_df[\"Fiyat\"])].index[0]-1, \"Fiyat\"] = malzeme_df.loc[0, \"     Tutar\"]\n",
    "    temp_time_df = temp_time_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    h_fiyat_unique.append(temp_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fiyat_unique = pd.concat(h_fiyat_unique)\n",
    "h_fiyat_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "h_fiyat_unique.rename(columns={\"En Güncel Kod\": \"Ürün Kodu\", \"date\": \"Date\"}, inplace=True)\n",
    "h_fiyat_unique = h_fiyat_unique.merge(eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\")\n",
    "h_fiyat_unique[\"En Güncel Kod\"].fillna(h_fiyat_unique[\"Ürün Kodu\"], inplace=True)\n",
    "h_fiyat_unique = h_fiyat_unique[h_fiyat_unique[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "h_fiyat_unique = h_fiyat_unique.sort_values(by=[\"En Güncel Kod\", \"Date\"]).reset_index(drop=True)\n",
    "h_fiyat_unique = h_fiyat_unique.drop(columns=\"Ürün Kodu\", axis=1)\n",
    "# Aynı aya denk gelen ürünlerin fiyatlarının ortalaması alınıp, herhangi birinde fiyat geçişi varsa 1 alınır.\n",
    "h_fiyat_unique = h_fiyat_unique.groupby([\"Date\", \"En Güncel Kod\"]).agg({\"Fiyat\": \"mean\", \"fiyat_gecisi\": \"max\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon fiyatların düzenlenmesi süresi: 0:02:27.367240\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon fiyatların düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Fiyatları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik.drop_duplicates(subset=fiyat_lst_pasifik.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik[\"Malzeme\"] = fiyat_lst_pasifik[\"Malzeme\"].str.replace(\"-\", \"\")\n",
    "fiyat_lst_pasifik[\"Malzeme\"] = fiyat_lst_pasifik[\"Malzeme\"].astype(int)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df = fiyat_lst_pasifik.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df[\"Baslangic_Yıl\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.year)\n",
    "fiyat_lst_pasifik_df[\"Baslangic_Ay\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.month)\n",
    "fiyat_lst_pasifik_df[\"Baslangic_Gun\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.day)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.year)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Ay\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.month)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Gun\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df[\"Baslangic_Yıl\"] = fiyat_lst_pasifik_df[\"Baslangic_Yıl\"].apply(lambda x: (pasifik_df_all2[\"Date\"].max().year)+1 if x > pasifik_df_all2[\"Date\"].max().year else x)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"] = fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"].apply(lambda x: (pasifik_df_all2[\"Date\"].max().year)+1 if x > pasifik_df_all2[\"Date\"].max().year else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = pasifik_df_all2[\"Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique = []\n",
    "\n",
    "for malzeme in fiyat_lst_pasifik_df[\"Malzeme\"].unique():\n",
    "    temp_time_df = pd.DataFrame({\"Fiyat\": [np.nan]}, index=time_index)\n",
    "    temp_time_df = temp_time_df.reset_index().rename(columns={\"index\":\"date\"})    \n",
    "    temp_time_df[\"En Güncel Kod\"] = malzeme\n",
    "    temp_time_df[\"fiyat_gecisi\"] = 0\n",
    "    malzeme_df = fiyat_lst_pasifik_df[fiyat_lst_pasifik_df[\"Malzeme\"] == malzeme].reset_index(drop=True)\n",
    "    malzeme_df.drop(columns=[\"KşTü\", \"KşTü.1\", \"Malzeme Tanım\", \"Ana Kategori\", \"Kategori\"], axis=1, inplace=True)\n",
    "    malzeme_df.drop_duplicates(subset=malzeme_df.columns.to_list(), inplace=True, ignore_index=True)\n",
    "    malzeme_df.sort_values(by=[\"Baslangic_Yıl\", \"Baslangic_Ay\", \"Baslangic_Gun\"], ignore_index=True, inplace=True)\n",
    "    check_idx1 = []\n",
    "    if len(malzeme_df) > 1:\n",
    "        for row1 in malzeme_df.index:\n",
    "            for row2 in malzeme_df[row1+1:].index:\n",
    "                if (malzeme_df.loc[row1][\"Gecerlilik_Yıl\"] == malzeme_df.loc[row2][\"Baslangic_Yıl\"]) and (malzeme_df.loc[row1][\"Gecerlilik_Ay\"] == malzeme_df.loc[row2][\"Baslangic_Ay\"]):\n",
    "                    num_days = calendar.monthrange(int(malzeme_df.loc[row2][\"Baslangic_Yıl\"]), int(malzeme_df.loc[row2][\"Baslangic_Ay\"]))[1]\n",
    "                    fyt=((int(malzeme_df.loc[row1][\"Gecerlilik_Gun\"])*malzeme_df.loc[row1][\"Koli TL\"]) + (num_days - int(malzeme_df.loc[row2][\"Baslangic_Gun\"])+1)*malzeme_df.loc[row2][\"Koli TL\"])/num_days\n",
    "\n",
    "                    end_idx1 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx1 = temp_time_df[temp_time_df[\"date\"] == end_idx1].index\n",
    "                    temp_time_df.loc[final_idx1, \"Fiyat\"] = fyt\n",
    "                    temp_time_df.loc[final_idx1,\"fiyat_gecisi\"] = 1\n",
    "\n",
    "                elif (malzeme_df.loc[row1, \"Gecerlilik_Gun\"] == calendar.monthrange(int(malzeme_df.loc[row1][\"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1][\"Gecerlilik_Ay\"]))[1] \\\n",
    "                     and malzeme_df.loc[row2, \"Baslangic_Gun\"] == 1):\n",
    "                    fyt5=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    fyt6=malzeme_df.loc[row2][\"Koli TL\"]\n",
    "                    end_idx5 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    end_idx6 =  datetime(int(malzeme_df.loc[row2, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row2, \"Baslangic_Ay\"]), 1)\n",
    "                    final_idx5 = temp_time_df[temp_time_df[\"date\"] == end_idx5].index\n",
    "                    final_idx6 = temp_time_df[temp_time_df[\"date\"] == end_idx6].index\n",
    "                    temp_time_df.loc[final_idx5, \"Fiyat\"] = fyt5\n",
    "                    temp_time_df.loc[final_idx6, \"Fiyat\"] = fyt6\n",
    "\n",
    "\n",
    "                else:\n",
    "                    if malzeme_df.loc[row1, \"Baslangic_Gun\"] != 1:\n",
    "                        fyt2=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                        start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                        end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                        final_idx2 = temp_time_df[(temp_time_df[\"date\"] > start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                        temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "                    else:\n",
    "                        fyt2=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                        start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                        end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                        final_idx2 = temp_time_df[(temp_time_df[\"date\"] >= start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                        temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "                        \n",
    "            if (row1 == len(malzeme_df)-1) or (row1 == len(malzeme_df)-2):\n",
    "                if malzeme_df.loc[row1, \"Baslangic_Gun\"] != 1:\n",
    "                    fyt3=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx3 = temp_time_df[(temp_time_df[\"date\"] > start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                    temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "                else:\n",
    "                    fyt3=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx3 = temp_time_df[(temp_time_df[\"date\"] >= start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                    temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "    else:\n",
    "        for row1 in malzeme_df.index:\n",
    "            fyt4=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "            start_idx4 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "            end_idx4 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "            final_idx4 = temp_time_df[(temp_time_df[\"date\"] >= start_idx4) & (temp_time_df[\"date\"] <= end_idx4)].index\n",
    "            temp_time_df.loc[final_idx4, \"Fiyat\"] = fyt4\n",
    "    \n",
    "    if (malzeme_df.loc[0, \"Baslangic_Yıl\"] >= min(pasifik_df_all2[\"Yıl\"].unique())) and (len(malzeme_df) > 1):\n",
    "        temp_time_df.loc[temp_time_df[~pd.isnull(temp_time_df[\"Fiyat\"])].index[0]-1, \"Fiyat\"] = malzeme_df.loc[0, \"Koli TL\"]\n",
    "    temp_time_df = temp_time_df.dropna().reset_index(drop=True)\n",
    "        \n",
    "    p_fiyat_unique.append(temp_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique = pd.concat(p_fiyat_unique)\n",
    "p_fiyat_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "p_fiyat_unique.rename(columns={\"En Güncel Kod\": \"Ürün Kodu\", \"date\": \"Date\"}, inplace=True)\n",
    "p_fiyat_unique = p_fiyat_unique.merge(eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\")\n",
    "p_fiyat_unique[\"En Güncel Kod\"].fillna(p_fiyat_unique[\"Ürün Kodu\"], inplace=True)\n",
    "p_fiyat_unique = p_fiyat_unique[p_fiyat_unique[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "p_fiyat_unique = p_fiyat_unique.sort_values(by=[\"En Güncel Kod\", \"Date\"]).reset_index(drop=True)\n",
    "p_fiyat_unique = p_fiyat_unique.drop(columns=\"Ürün Kodu\", axis=1)\n",
    "# Aynı aya denk gelen ürünlerin fiyatlarının ortalaması alınıp, herhangi birinde fiyat geçişi varsa 1 alınır.\n",
    "p_fiyat_unique = p_fiyat_unique.groupby([\"Date\", \"En Güncel Kod\"]).agg({\"Fiyat\": \"mean\", \"fiyat_gecisi\": \"max\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik fiyatların düzenlenmesi süresi: 0:03:15.952545\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik fiyatların düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 0'ları doldururken en son yıl ve ayın ötesi de 0 ile dolduruyor. (Örn: Sales datasında 2021'in 3. ayına kadar data olduğu durumda 0'lar ile doldururken 2021 12. aya kadar 0 atıyor.\n",
    "# Bu durumun önüne geçmek için aşağıdaki işlemler yapılmaktadır.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_max_year = pasifik_df_all[\"Yıl\"].max()\n",
    "pas_max_month = pasifik_df_all[pasifik_df_all[\"Yıl\"] == pasifik_df_all[\"Yıl\"].max()][\"Ay\"].max()\n",
    "hor_max_year = horizon_saha_df_all[\"Yıl\"].max()\n",
    "hor_max_month = horizon_saha_df_all[horizon_saha_df_all[\"Yıl\"] == horizon_saha_df_all[\"Yıl\"].max()][\"Ay\"].max()\n",
    "btt_max_year = btt_df_all[\"Yıl\"].max()\n",
    "btt_max_month = btt_df_all[btt_df_all[\"Yıl\"] == btt_df_all[\"Yıl\"].max()][\"Ay\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2 = pasifik_df_all2[~((pasifik_df_all2[\"Yıl\"] == pas_max_year) & (pasifik_df_all2[\"Ay\"] > pas_max_month))].reset_index(drop=True)\n",
    "horizon_saha_df_all2 = horizon_saha_df_all2[~((horizon_saha_df_all2[\"Yıl\"] == hor_max_year) & \n",
    "                                            (horizon_saha_df_all2[\"Ay\"] > hor_max_month))].reset_index(drop=True)\n",
    "btt_df_all2 = btt_df_all2[~((btt_df_all2[\"Yıl\"] == btt_max_year) & (btt_df_all2[\"Ay\"] > btt_max_month))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portföy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_portfoy_df2 = pd.merge(pasifik_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "pasifik_portfoy_df2[\"En Güncel Kod\"] = pasifik_portfoy_df2[\"En Güncel Kod\"].fillna(pasifik_portfoy_df2[\"Kod\"])\n",
    "pasifik_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "pasifik_portfoy_df2 = pasifik_portfoy_df2[pasifik_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "pasifik_portfoy_df2[\"Portfoy\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_portfoy_df2 = pd.merge(horizon_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "horizon_portfoy_df2[\"En Güncel Kod\"] = horizon_portfoy_df2[\"En Güncel Kod\"].fillna(horizon_portfoy_df2[\"Kod\"])\n",
    "horizon_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "horizon_portfoy_df2 = horizon_portfoy_df2[horizon_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "horizon_portfoy_df2[\"Portfoy\"] = 1\n",
    "horizon_portfoy_df2 = horizon_portfoy_df2[~((horizon_portfoy_df2[\"Kod\"] == 135901))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTT Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_portfoy_df2 = pd.merge(btt_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "btt_portfoy_df2[\"En Güncel Kod\"] = btt_portfoy_df2[\"En Güncel Kod\"].fillna(btt_portfoy_df2[\"Kod\"])\n",
    "btt_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "btt_portfoy_df2 = btt_portfoy_df2[btt_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "btt_portfoy_df2[\"Portfoy\"] = 1\n",
    "btt_portfoy_df2 = btt_portfoy_df2[~((btt_portfoy_df2[\"Kod\"] == 135901))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portföy Kapsamındaki Sales Dataları"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pd.merge(pasifik_df_all2,pasifik_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "btt_df_all3 = pd.merge(btt_df_all2,btt_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "horizon_saha_df_all3 = pd.merge(horizon_saha_df_all2,horizon_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "pasifik_df_all3[\"Portfoy\"].fillna(0, inplace=True)\n",
    "btt_df_all3[\"Portfoy\"].fillna(0, inplace=True)\n",
    "horizon_saha_df_all3[\"Portfoy\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datalara Calender Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Jan 2016\n",
       "1     Feb 2016\n",
       "2     Mar 2016\n",
       "3     Apr 2016\n",
       "4     May 2016\n",
       "5     Jun 2016\n",
       "6     Jul 2016\n",
       "7     Aug 2016\n",
       "8     Sep 2016\n",
       "9     Oct 2016\n",
       "10    Nov 2016\n",
       "11    Dec 2016\n",
       "12    Jan 2017\n",
       "13    Feb 2017\n",
       "14    Mar 2017\n",
       "15    Apr 2017\n",
       "16    May 2017\n",
       "17    Jun 2017\n",
       "18    Jul 2017\n",
       "19    Aug 2017\n",
       "20    Sep 2017\n",
       "21    Oct 2017\n",
       "22    Nov 2017\n",
       "23    Dec 2017\n",
       "24    Jan 2018\n",
       "25    Feb 2018\n",
       "26    Mar 2018\n",
       "27    Apr 2018\n",
       "28    May 2018\n",
       "29    Jun 2018\n",
       "30    Jul 2018\n",
       "31    Aug 2018\n",
       "32    Sep 2018\n",
       "33    Oct 2018\n",
       "34    Nov 2018\n",
       "35    Dec 2018\n",
       "36    Jan 2019\n",
       "37    Feb 2019\n",
       "38    Mar 2019\n",
       "39    Apr 2019\n",
       "40    May 2019\n",
       "41    Jun 2019\n",
       "42    Jul 2019\n",
       "43    Aug 2019\n",
       "44    Sep 2019\n",
       "45    Oct 2019\n",
       "46    Nov 2019\n",
       "47    Dec 2019\n",
       "48    Jan 2020\n",
       "49    Feb 2020\n",
       "50    Mar 2020\n",
       "51    Apr 2020\n",
       "52    May 2020\n",
       "53    Jun 2020\n",
       "54    Jul 2020\n",
       "55    Aug 2020\n",
       "56    Sep 2020\n",
       "57    Oct 2020\n",
       "58    Nov 2020\n",
       "59    Dec 2020\n",
       "60    Jan 2021\n",
       "61    Feb 2021\n",
       "62    Mar 2021\n",
       "63    Apr 2021\n",
       "64    May 2021\n",
       "65    Jun 2021\n",
       "66    Jul 2021\n",
       "67    Aug 2021\n",
       "68    Sep 2021\n",
       "69    Oct 2021\n",
       "70    Nov 2021\n",
       "71    Dec 2021\n",
       "72    Jan 2022\n",
       "73    Feb 2022\n",
       "74    Mar 2022\n",
       "75    Apr 2022\n",
       "76    May 2022\n",
       "77    Jun 2022\n",
       "78    Jul 2022\n",
       "79    Aug 2022\n",
       "80    Sep 2022\n",
       "81    Oct 2022\n",
       "82    Nov 2022\n",
       "83    Dec 2022\n",
       "84    Jan 2023\n",
       "85    Feb 2023\n",
       "86    Mar 2023\n",
       "87    Apr 2023\n",
       "88    May 2023\n",
       "89    Jun 2023\n",
       "90    Jul 2023\n",
       "91    Aug 2023\n",
       "92    Sep 2023\n",
       "93    Oct 2023\n",
       "94    Nov 2023\n",
       "95    Dec 2023\n",
       "Name: DATE, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calender_df.pop(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pd.merge(pasifik_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")\n",
    "btt_df_all3 = pd.merge(btt_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")\n",
    "horizon_saha_df_all3 = pd.merge(horizon_saha_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataların Fiyat Ve Aktiviteler İle Birleştirilmesi\n",
    "---\n",
    "\n",
    "# Not:\n",
    "---\n",
    "### 1) BTT aktivite verisi için Horizon kısmındaki \"Geleneksel Kanal\" kullanılması istendi.\n",
    "### 2) BTT fiyat geçişleri için Horizon fiyat geçişleri baz alındı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pasifik_df_all3.merge(p_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "pasifik_aktivite_df3.rename(columns={\"Müşteri Grup\": \"Grup Adı\", \"Grup adı\": \"Grup Adı\"}, inplace=True)\n",
    "pasifik_df_all3 = pd.merge(pasifik_df_all3, pasifik_aktivite_df3[[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\", \n",
    "                                                                  \"Raf Tavsiye Satış Fiyatı\", \"İndirimli Raf Satış Fiyatı\", \"İndirim %\",\n",
    "                                                                  \"Aktivite Tipi\"]], \n",
    "                           left_on=[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\"], \n",
    "                           right_on=[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3.rename(columns={\"Grup adı\": \"Grup Adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(h_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(saha_aktivite_detay3[['Ciro (Kull. İade Düş.)', 'Promosyon Tutarı', \n",
    "                                                                        'İskonto %', 'En Güncel Kod', \"Yıl\", \"Ay\", \"Grup Adı\"]],\n",
    "                                                  on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Grup Adı\"], how=\"left\")\n",
    "btt_df_all3 = btt_df_all3.merge(h_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "btt_aktivite = saha_aktivite_detay3[saha_aktivite_detay3[\"Grup Adı\"] == \"GELENEKSEL KANAL\"].reset_index(drop=True)\n",
    "\n",
    "btt_df_all3 = btt_df_all3.merge(btt_aktivite[['Ciro (Kull. İade Düş.)', 'Promosyon Tutarı', \n",
    "                                              'İskonto %', 'En Güncel Kod', \"Yıl\", \"Ay\"]],\n",
    "                                on=[\"En Güncel Kod\", \"Yıl\", \"Ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sütun İsimlerini İngilizce Karaktere Çevirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_col_name(dff_):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    dff_: dataframe\n",
    "    Sütun ismini değiştirmek istediğiniz dataframe'i yazınız.\n",
    "    \n",
    "    Returns: Liste\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chng_letters = list(zip([\"ç\", \"ğ\", \"ı\", \"ö\", \"ş\", \"ü\", \" \", \"%\", \".\", \"(\", \")\", \"-\"], \n",
    "                            [\"c\", \"g\", \"i\", \"o\", \"s\", \"u\", \"_\", \"\", \"\", \"\", \"\", \"_\"]))\n",
    "    new_cols = []\n",
    "    for col in dff_.columns.str.lower():\n",
    "        for letter in range(len(chng_letters)):\n",
    "            col = col.replace(chng_letters[letter][0], chng_letters[letter][1])\n",
    "            if letter == len(chng_letters) - 1:\n",
    "                new_cols.append(col)\n",
    "            else:\n",
    "                pass\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3.columns = change_col_name(pasifik_df_all3)\n",
    "horizon_saha_df_all3.columns = change_col_name(horizon_saha_df_all3)\n",
    "btt_df_all3.columns = change_col_name(btt_df_all3)\n",
    "enflasyon_df.columns = change_col_name(enflasyon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifikte Aktivite Tipi Verisi Eksik Olan Verilere \"Yok\" yazıldı\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3[\"aktivite_tipi\"].fillna(\"Yok\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Enflasyon Verilerinin Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pasifik_df_all3.merge(enflasyon_df, on=[\"date\"], how=\"left\")\n",
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(enflasyon_df, on=[\"date\"], how=\"left\")\n",
    "btt_df_all3 = btt_df_all3.merge(enflasyon_df, on=[\"date\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df_all3.copy()\n",
    "horizon_saha_df_sorted = horizon_saha_df_all3.copy()\n",
    "btt_df_sorted = btt_df_all3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Satış Olmayan Aylar Flaglendi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"satis_var\"] = [0 if adet <= 1 else 1 for adet in pasifik_df_sorted[\"adet\"]]\n",
    "horizon_saha_df_sorted[\"satis_var\"] = [0 if adet <= 1 else 1 for adet in horizon_saha_df_sorted[\"adet\"]]\n",
    "btt_df_sorted[\"satis_var\"] = [0 if adet <= 1 else 1 for adet in btt_df_sorted[\"adet\"]]\n",
    "\n",
    "df_pasifik = pasifik_df_sorted.copy()\n",
    "df_btt = btt_df_sorted.copy()\n",
    "df_horizon = horizon_saha_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sütun isim uzunluğunun 32'yi geçmemesi için\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik.columns = [i[:32] if len(i) > 32 else i for i in df_pasifik.columns]\n",
    "df_horizon.columns = [i[:32] if len(i) > 32 else i for i in df_horizon.columns]\n",
    "df_btt.columns = [i[:32] if len(i) > 32 else i for i in df_btt.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sales datasındaki son yıl ve aya kadar ulaşmayan, yarıda kesilen verilerin, son tarihe kadar NaN ile doldurulması\n",
    "## Sales verisindeki en son tarih Haziran 2021 ise, herhangi bir SKU'nun son gözlemi Şubat 2019'da olsa dahi Haizran 2021'e kadar devam ettiriliyor\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_and_horizon_cols_to_drop = [\"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\",\n",
    "                                \"urun_adi\", \"en_guncel_kod\", \"portfoy\", \"koli_i̇ci_adet\", \"koli\", \"kg\", \"tl\", \"adet\", \n",
    "                                \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\",\n",
    "                                \"i̇skonto_\", \"satis_var\"]\n",
    "\n",
    "pasifik_cols_to_drop = [\"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\",\n",
    "                        \"urun_adi\", \"en_guncel_kod\", \"portfoy\", \"koli_i̇ci_adet\", \"koli\", \"kg\", \"tl\", \"adet\",\n",
    "                        \"fiyat\", \"fiyat_gecisi\", \"raf_tavsiye_satis_fiyati\", \"i̇ndirimli_raf_satis_fiyati\",\n",
    "                        \"aktivite_tipi\", \"i̇ndirim_\", \"satis_var\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bu kısmı düzenle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#year_ = datetime.now().year\n",
    "#month_ = datetime.now().month\n",
    "\n",
    "month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "if month_ == 12:\n",
    "    month_ = 1\n",
    "    year_+=1\n",
    "else:\n",
    "    month_+=1\n",
    "\n",
    "df_pasifik = df_pasifik[df_pasifik[\"date\"] < datetime(year_,\n",
    "                                                      month_,\n",
    "                                                      1)]\n",
    "df_horizon = df_horizon[df_horizon[\"date\"] < datetime(year_,\n",
    "                                                      month_,\n",
    "                                                      1)]\n",
    "df_btt = df_btt[df_btt[\"date\"] < datetime(year_,\n",
    "                                          month_,\n",
    "                                          1)]\n",
    "\n",
    "#df_pasifik = df_pasifik[df_pasifik[\"date\"] < datetime.now()]\n",
    "#df_horizon = df_horizon[df_horizon[\"date\"] < datetime.now()]\n",
    "#df_btt = df_btt[df_btt[\"date\"] < datetime.now()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_backup_for_null_date = df_horizon.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_template = df_pasifik.drop(columns=pasifik_cols_to_drop, axis=1)\n",
    "pasifik_template = pasifik_template.drop_duplicates(subset=pasifik_template.columns.to_list()).sort_values(by=[\"yil\", \"ay\"])\n",
    "\n",
    "btt_template = df_btt.drop(columns=btt_and_horizon_cols_to_drop, axis=1)\n",
    "btt_template = btt_template.drop_duplicates(subset=btt_template.columns.to_list()).sort_values(by=[\"yil\", \"ay\"])\n",
    "\n",
    "horizon_template = df_horizon.drop(columns=btt_and_horizon_cols_to_drop, axis=1)\n",
    "horizon_template = horizon_template.drop_duplicates(subset=horizon_template.columns.to_list()).sort_values(by=[\"yil\", \"ay\"])\n",
    "\n",
    "pas_date = pasifik_df_sorted.date.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "hor_date = horizon_saha_df_sorted.date.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "btt_date = btt_df_sorted.date.drop_duplicates().sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_sku_fill = []\n",
    "pasifik_grup_fill = []\n",
    "for sku in df_pasifik[\"en_guncel_kod\"].unique():\n",
    "    for grup in df_pasifik[\"grup_adi\"].unique():\n",
    "        temp = df_pasifik[(df_pasifik[\"en_guncel_kod\"] == sku) & (df_pasifik[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "        if temp[\"date\"].max() < pasifik_template[\"date\"].max():\n",
    "            pasifik_sku_fill.append(sku)\n",
    "            pasifik_grup_fill.append(grup)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        temp_date = pd.DataFrame(pas_date)[pd.DataFrame(pas_date)[\"date\"] >= temp[\"date\"].min()]\n",
    "        \n",
    "        if len(temp_date) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            pasifik_sku_fill.append(sku)\n",
    "            pasifik_grup_fill.append(grup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik missing imputation düzenlenmesi süresi: 0:00:50.198656\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing imputation düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_sku_fill = []\n",
    "btt_grup_fill = []\n",
    "for sku in df_btt[\"en_guncel_kod\"].unique():\n",
    "    for grup in df_btt[\"grup_adi\"].unique():\n",
    "        temp = df_btt[(df_btt[\"en_guncel_kod\"] == sku) & (df_btt[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "        if temp[\"date\"].max() < btt_template[\"date\"].max():\n",
    "            btt_sku_fill.append(sku)\n",
    "            btt_grup_fill.append(grup)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        temp_date = pd.DataFrame(btt_date)[pd.DataFrame(btt_date)[\"date\"] >= temp[\"date\"].min()]\n",
    "        \n",
    "        if len(temp_date) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            btt_sku_fill.append(sku)\n",
    "            btt_grup_fill.append(grup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTT missing imputation süresi: 0:00:04.616899\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing imputation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_sku_fill = []\n",
    "horizon_grup_fill = []\n",
    "for sku in df_horizon[\"en_guncel_kod\"].unique():\n",
    "    for grup in df_horizon[\"grup_adi\"].unique():\n",
    "        temp = df_horizon[(df_horizon[\"en_guncel_kod\"] == sku) & (df_horizon[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "        if temp[\"date\"].max() < horizon_template[\"date\"].max():\n",
    "            horizon_sku_fill.append(sku)\n",
    "            horizon_grup_fill.append(grup)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        temp_date = pd.DataFrame(hor_date)[pd.DataFrame(hor_date)[\"date\"] >= temp[\"date\"].min()]\n",
    "        \n",
    "        if len(temp_date) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            horizon_sku_fill.append(sku)\n",
    "            horizon_grup_fill.append(grup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon missing imputation düzenlenmesi süresi: 0:02:14.517903\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing imputation düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pasifik_to_append = []\n",
    "\n",
    "ffill_cols = ['grup_adi', 'ana_kategori_adi', 'kategori_adi',\n",
    "             'marka_adi', 'urun_adi', 'en_guncel_kod', \"portfoy\"]\n",
    "\n",
    "zero_fill_cols = ['koli_i̇ci_adet', 'koli', 'kg', 'tl', 'satis_var']\n",
    "\n",
    "one_fill_cols = [\"adet\"]\n",
    "\n",
    "for row in range(len(pasifik_sku_fill)):\n",
    "    temp = df_pasifik[(df_pasifik[\"en_guncel_kod\"] == pasifik_sku_fill[row]) & \n",
    "                      (df_pasifik[\"grup_adi\"] == pasifik_grup_fill[row])].reset_index(drop=True)\n",
    "    \n",
    "    pasifik_template2 = pasifik_template[pasifik_template[\"date\"] > temp.date.max()].reset_index(drop=True)    \n",
    "    df_to_append = temp.merge(pasifik_template2, how=\"outer\", on=list(set(temp).intersection(set(pasifik_template2))))\n",
    "    \n",
    "    temp_date = pd.DataFrame(pas_date)[pd.DataFrame(pas_date)[\"date\"] >= df_to_append[\"date\"].min()][\"date\"]\n",
    "    date_df = pd.DataFrame(list(set(temp_date) - set(df_to_append[\"date\"])), columns=[\"date\"])\n",
    "    \n",
    "    if len(date_df) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_to_append = df_to_append.merge(date_df, how=\"outer\", on=\"date\")\n",
    "        df_to_append[\"yil\"] = df_to_append[\"date\"].apply(lambda x: x.date().year)\n",
    "        df_to_append[\"ay\"] = df_to_append[\"date\"].apply(lambda x: x.date().month)\n",
    "        df_to_append = df_to_append.merge(pasifik_template2, how=\"outer\", on=list(set(df_to_append).intersection(set(pasifik_template2))))\n",
    "\n",
    "    for f in ffill_cols:\n",
    "        df_to_append[f].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    for z in zero_fill_cols:\n",
    "        df_to_append[z].fillna(0, inplace=True)\n",
    "    \n",
    "    for o in one_fill_cols:\n",
    "        df_to_append[o].fillna(1, inplace=True)\n",
    "    dfs_pasifik_to_append.append(df_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik missing append süresi: 0:02:55.508148\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing append süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_horizon_to_append = []\n",
    "\n",
    "ffill_cols = ['grup_adi', 'ana_kategori_adi', 'kategori_adi',\n",
    "             'marka_adi', 'urun_adi', 'en_guncel_kod', \"portfoy\"]\n",
    "\n",
    "zero_fill_cols = ['koli_i̇ci_adet', 'koli', 'kg', 'tl', 'satis_var']\n",
    "\n",
    "one_fill_cols = [\"adet\"]\n",
    "\n",
    "for row in range(len(horizon_sku_fill)):\n",
    "    temp = df_horizon[(df_horizon[\"en_guncel_kod\"] == horizon_sku_fill[row]) & \n",
    "                      (df_horizon[\"grup_adi\"] == horizon_grup_fill[row])].reset_index(drop=True)\n",
    "    \n",
    "    horizon_template2 = horizon_template[horizon_template[\"date\"] > temp.date.max()].reset_index(drop=True)\n",
    "    df_to_append = temp.merge(horizon_template2, how=\"outer\", on=list(set(temp).intersection(set(horizon_template2))))\n",
    "    \n",
    "    temp_date = pd.DataFrame(hor_date)[pd.DataFrame(hor_date)[\"date\"] >= df_to_append[\"date\"].min()][\"date\"]\n",
    "    date_df = pd.DataFrame(list(set(temp_date) - set(df_to_append[\"date\"])), columns=[\"date\"])\n",
    "    \n",
    "    if len(date_df) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_to_append = df_to_append.merge(date_df, how=\"outer\", on=\"date\")\n",
    "        df_to_append[\"yil\"] = df_to_append[\"date\"].apply(lambda x: x.date().year)\n",
    "        df_to_append[\"ay\"] = df_to_append[\"date\"].apply(lambda x: x.date().month)\n",
    "        df_to_append = df_to_append.merge(horizon_template2, how=\"outer\", on=list(set(df_to_append).intersection(set(horizon_template2))))\n",
    "\n",
    "    for f in ffill_cols:\n",
    "        df_to_append[f].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    for z in zero_fill_cols:\n",
    "        df_to_append[z].fillna(0, inplace=True)\n",
    "    \n",
    "    for o in one_fill_cols:\n",
    "        df_to_append[o].fillna(1, inplace=True)\n",
    "    dfs_horizon_to_append.append(df_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon missing append süresi: 0:05:40.285531\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing append süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_btt_to_append = []\n",
    "\n",
    "ffill_cols = ['grup_adi', 'ana_kategori_adi', 'kategori_adi',\n",
    "             'marka_adi', 'urun_adi', 'en_guncel_kod', \"portfoy\"]\n",
    "\n",
    "zero_fill_cols = ['koli_i̇ci_adet', 'koli', 'kg', 'tl', 'satis_var']\n",
    "\n",
    "one_fill_cols = [\"adet\"]\n",
    "\n",
    "for row in range(len(btt_sku_fill)):\n",
    "    temp = df_btt[(df_btt[\"en_guncel_kod\"] == btt_sku_fill[row]) & \n",
    "                  (df_btt[\"grup_adi\"] == btt_grup_fill[row])].reset_index(drop=True)\n",
    "    \n",
    "    btt_template2 = btt_template[btt_template[\"date\"] > temp.date.max()].reset_index(drop=True)\n",
    "\n",
    "    df_to_append = temp.merge(btt_template2, how=\"outer\", on=list(set(temp).intersection(set(btt_template2))))\n",
    "    \n",
    "    temp_date = pd.DataFrame(btt_date)[pd.DataFrame(btt_date)[\"date\"] >= df_to_append[\"date\"].min()][\"date\"]\n",
    "    date_df = pd.DataFrame(list(set(temp_date) - set(df_to_append[\"date\"])), columns=[\"date\"])\n",
    "    \n",
    "    if len(date_df) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_to_append = df_to_append.merge(date_df, how=\"outer\", on=\"date\")\n",
    "        df_to_append[\"yil\"] = df_to_append[\"date\"].apply(lambda x: x.date().year)\n",
    "        df_to_append[\"ay\"] = df_to_append[\"date\"].apply(lambda x: x.date().month)\n",
    "        df_to_append = df_to_append.merge(btt_template2, how=\"outer\", on=list(set(df_to_append).intersection(set(btt_template2))))\n",
    "\n",
    "    for f in ffill_cols:\n",
    "        df_to_append[f].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    for z in zero_fill_cols:\n",
    "        df_to_append[z].fillna(0, inplace=True)\n",
    "    \n",
    "    for o in one_fill_cols:\n",
    "        df_to_append[o].fillna(1, inplace=True)\n",
    "    dfs_btt_to_append.append(df_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTT missing append süresi: 0:00:15.806412\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing append süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik2 = df_pasifik.copy()\n",
    "df_horizon2 = df_horizon.copy()\n",
    "df_btt2 = df_btt.copy()\n",
    "\n",
    "dfs_pasifik_to_append = pd.concat(dfs_pasifik_to_append)\n",
    "dfs_horizon_to_append = pd.concat(dfs_horizon_to_append)\n",
    "dfs_btt_to_append = pd.concat(dfs_btt_to_append)\n",
    "\n",
    "df_pasifik3 = pd.concat([df_pasifik2, dfs_pasifik_to_append], axis=0, ignore_index=True)\n",
    "df_pasifik3.drop_duplicates(subset=df_pasifik3.columns.to_list(), ignore_index=True, inplace=True)\n",
    "\n",
    "df_horizon3 = pd.concat([df_horizon2, dfs_horizon_to_append], axis=0, ignore_index=True)\n",
    "df_horizon3.drop_duplicates(subset=df_horizon3.columns.to_list(), ignore_index=True, inplace=True)\n",
    "\n",
    "df_btt3 = pd.concat([df_btt2, dfs_btt_to_append], axis=0, ignore_index=True)\n",
    "df_btt3.drop_duplicates(subset=df_btt3.columns.to_list(), ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique.rename(columns={\"Date\": \"date\", \"En Güncel Kod\": \"en_guncel_kod\", \"Fiyat\": \"fiyat\"}, inplace=True)\n",
    "h_fiyat_unique.rename(columns={\"Date\": \"date\", \"En Güncel Kod\": \"en_guncel_kod\", \"Fiyat\": \"fiyat\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique[\"kanal\"] = \"pasifik\"\n",
    "h_fiyat_unique[\"kanal\"] = \"horizon\"\n",
    "fiyat_unique = pd.concat([p_fiyat_unique, h_fiyat_unique], axis=0, ignore_index=True)\n",
    "#fiyat_unique.to_csv(\"../data/fiyat_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik4 = df_pasifik3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_pasifik4 = df_pasifik4.merge(p_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])\n",
    "\n",
    "df_horizon4 = df_horizon3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_horizon4 = df_horizon4.merge(h_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])\n",
    "\n",
    "df_btt4 = df_btt3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_btt4 = df_btt4.merge(h_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_aktivite_df3.columns = change_col_name(pasifik_aktivite_df3)\n",
    "saha_aktivite_detay3.columns = change_col_name(saha_aktivite_detay3)\n",
    "pasifik_aktivite_df3.rename(columns={\"i̇ndirim_\": \"indirim_\"}, inplace=True)\n",
    "saha_aktivite_detay3.rename(columns={\"i̇i̇skonto_\": \"iskonto_\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik4 = df_pasifik4.drop(columns=['raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', \n",
    "                                        'i̇ndirim_', 'aktivite_tipi'], axis=1)\n",
    "\n",
    "df_horizon4 = df_horizon4.drop(columns=['ciro_kull_i̇ade_dus', 'promosyon_tutari', 'i̇skonto_'], axis=1)\n",
    "df_btt4 = df_btt4.drop(columns=['ciro_kull_i̇ade_dus', 'promosyon_tutari', 'i̇skonto_'], axis=1)\n",
    "\n",
    "pasifik_aktivite_df3.rename(columns={\"Müşteri Grup\": \"Grup adı\"}, inplace=True)\n",
    "df_pasifik5 = pd.merge(df_pasifik4, pasifik_aktivite_df3[['yil', 'ay', 'grup_adi', 'en_guncel_kod', \n",
    "                                                          'raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', 'indirim_', \n",
    "                                                          'aktivite_tipi']], \n",
    "                           left_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                           right_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], how=\"left\")\n",
    "\n",
    "df_horizon5 = df_horizon4.merge(saha_aktivite_detay3[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                      'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], how=\"left\")\n",
    "\n",
    "btt_aktivite = saha_aktivite_detay3[saha_aktivite_detay3[\"grup_adi\"] == \"GELENEKSEL KANAL\"].reset_index(drop=True)\n",
    "btt_aktivite[\"grup_adi\"] = \"BTT\"\n",
    "\n",
    "df_btt5 = df_btt4.merge(btt_aktivite[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                      'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                        on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = df_pasifik5.copy()\n",
    "horizon_saha_df_sorted = df_horizon5.copy()\n",
    "btt_df_sorted = df_btt5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Yarışma ve Enflasyon Verisinin Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.drop(columns=\"enflasyon_etkisi\", axis=1, inplace=True)\n",
    "horizon_saha_df_sorted.drop(columns=\"enflasyon_etkisi\", axis=1, inplace=True)\n",
    "btt_df_sorted.drop(columns=\"enflasyon_etkisi\", axis=1, inplace=True)\n",
    "\n",
    "pasifik_df_sorted = pasifik_df_sorted.merge(enflasyon_df, how=\"left\", on=\"date\")\n",
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(enflasyon_df, how=\"left\", on=\"date\")\n",
    "btt_df_sorted = btt_df_sorted.merge(enflasyon_df, how=\"left\", on=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "yarisma_df[\"date\"] = pd.to_datetime(yarisma_df[\"date\"], errors=\"coerce\", format=\"%d.%m.%Y\")\n",
    "yarisma_df = yarisma_df.dropna().reset_index(drop=True)\n",
    "\n",
    "pasifik_df_sorted[\"yarisma\"] = 0\n",
    "horizon_saha_df_sorted[\"yarisma\"] = 0\n",
    "btt_df_sorted[\"yarisma\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(yarisma_df)):\n",
    "    yarisma_df[\"type\"].fillna(0, inplace=True)\n",
    "    if yarisma_df.loc[idx, \"type\"] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        temp_df = horizon_saha_df_sorted[(horizon_saha_df_sorted[\"date\"] == yarisma_df.loc[idx, \"date\"]) & \n",
    "                                         (horizon_saha_df_sorted[\"grup_adi\"] == yarisma_df.loc[idx, \"grup_adi\"]) & \n",
    "                                         (horizon_saha_df_sorted[yarisma_df.loc[idx, \"type\"]] == yarisma_df.loc[idx, \"col1\"])]\n",
    "        idx_to_replace = temp_df.index\n",
    "        horizon_saha_df_sorted.loc[idx_to_replace, \"yarisma\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yarışam verisinin eklenme süresi: 0:00:12.856033\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Yarışam verisinin eklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Arada tarih verisi olmayan gözlemler\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SKU Sayıları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month_ = datetime.now().month\n",
    "# year_ = datetime.now().year\n",
    "month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "\n",
    "date__ = datetime(year_, month_, 1)\n",
    "\n",
    "pas_df_all2 = pas_backup.copy()\n",
    "hor_df_all2 = hor_backup.copy()\n",
    "btt_df_all2_ = btt_backup.copy()\n",
    "\n",
    "pas_df_all2 = pas_df_all2[pas_df_all2[\"date\"] <= date__]\n",
    "hor_df_all2 = hor_df_all2[hor_df_all2[\"date\"] <= date__]\n",
    "btt_df_all2_ = btt_df_all2_[btt_df_all2_[\"date\"] <= date__]\n",
    "\n",
    "pas_df_all2.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "hor_df_all2.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "btt_df_all2_.rename(columns={\"Date\": \"date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku = []\n",
    "grup_adi = []\n",
    "portfoy = []\n",
    "oran = []\n",
    "gozlem_sayisi = []\n",
    "toplam_satir = []\n",
    "baslangic_tarih = []\n",
    "bitis_tarih = []\n",
    "son_kac_ay_eksik = []\n",
    "kanal = []\n",
    "repeat_num = [] # 1'lerin kaç aralıkla geldiği\n",
    "for idx, temp_df in pas_df_all2[[\"grup_adi\", \"en_guncel_kod\", \"portfoy\", \"adet\", \"date\"]].groupby([\"grup_adi\", \"en_guncel_kod\", \"portfoy\"]):\n",
    "    sku.append(temp_df[\"en_guncel_kod\"].iloc[0])\n",
    "    grup_adi.append(temp_df[\"grup_adi\"].iloc[0])\n",
    "    portfoy.append(temp_df[\"portfoy\"].iloc[0])\n",
    "    oran.append(len(temp_df[temp_df[\"adet\"] != 1]) / len(temp_df))\n",
    "    gozlem_sayisi.append(len(temp_df[temp_df[\"adet\"] != 1]))\n",
    "    baslangic_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].min())\n",
    "    bitis_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].max())\n",
    "    toplam_satir.append(len(temp_df))\n",
    "    son_kac_ay_eksik.append(round((pas_df_all2.date.max() - temp_df[temp_df[\"adet\"] != 1].date.max())/np.timedelta64(1, 'M'),1))\n",
    "    kanal.append(\"Pasifik\")\n",
    "\n",
    "    \n",
    "    lst = temp_df[temp_df[\"adet\"] == 1].index\n",
    "    lst = sorted(lst, reverse=True)\n",
    "    counter = 0\n",
    "    for idx_ in range(len(lst) - 1):\n",
    "        if lst[idx_] - lst[idx_+1] != 1:\n",
    "            counter+=1\n",
    "        else: \n",
    "            pass\n",
    "    if counter != 0:\n",
    "        counter+=1\n",
    "    repeat_num.append(counter)\n",
    "\n",
    "for idx, temp_df in hor_df_all2[[\"grup_adi\", \"en_guncel_kod\", \"portfoy\", \"adet\", \"date\"]].groupby([\"grup_adi\", \"en_guncel_kod\", \"portfoy\"]):\n",
    "    sku.append(temp_df[\"en_guncel_kod\"].iloc[0])\n",
    "    grup_adi.append(temp_df[\"grup_adi\"].iloc[0])\n",
    "    portfoy.append(temp_df[\"portfoy\"].iloc[0])\n",
    "    oran.append(len(temp_df[temp_df[\"adet\"] != 1]) / len(temp_df))\n",
    "    gozlem_sayisi.append(len(temp_df[temp_df[\"adet\"] != 1]))\n",
    "    baslangic_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].min())\n",
    "    bitis_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].max())\n",
    "    toplam_satir.append(len(temp_df))\n",
    "    son_kac_ay_eksik.append(round((hor_df_all2.date.max() - temp_df[temp_df[\"adet\"] != 1].date.max())/np.timedelta64(1, 'M'),1))\n",
    "    kanal.append(\"Horizon\")\n",
    "\n",
    "\n",
    "    lst = temp_df[temp_df[\"adet\"] == 1].index\n",
    "    lst = sorted(lst, reverse=True)\n",
    "    counter = 0\n",
    "    for idx_ in range(len(lst) - 1):\n",
    "        if lst[idx_] - lst[idx_+1] != 1:\n",
    "            counter+=1\n",
    "        else: \n",
    "            pass\n",
    "    if counter != 0:\n",
    "        counter+=1\n",
    "    repeat_num.append(counter)\n",
    "\n",
    "for idx, temp_df in btt_df_all2_[[\"grup_adi\", \"en_guncel_kod\", \"portfoy\", \"adet\", \"date\"]].groupby([\"grup_adi\", \"en_guncel_kod\", \"portfoy\"]):\n",
    "    sku.append(temp_df[\"en_guncel_kod\"].iloc[0])\n",
    "    grup_adi.append(temp_df[\"grup_adi\"].iloc[0])\n",
    "    portfoy.append(temp_df[\"portfoy\"].iloc[0])\n",
    "    oran.append(len(temp_df[temp_df[\"adet\"] != 1]) / len(temp_df))\n",
    "    gozlem_sayisi.append(len(temp_df[temp_df[\"adet\"] != 1]))\n",
    "    baslangic_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].min())\n",
    "    bitis_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].max())\n",
    "    toplam_satir.append(len(temp_df))\n",
    "    son_kac_ay_eksik.append(round((btt_df_all2_.date.max() - temp_df[temp_df[\"adet\"] != 1].date.max())/np.timedelta64(1, 'M'),1))\n",
    "    kanal.append(\"BTT\")\n",
    "\n",
    "\n",
    "    lst = temp_df[temp_df[\"adet\"] == 1].index\n",
    "    lst = sorted(lst, reverse=True)\n",
    "    counter = 0\n",
    "    for idx_ in range(len(lst) - 1):\n",
    "        if lst[idx_] - lst[idx_+1] != 1:\n",
    "            counter+=1\n",
    "        else: \n",
    "            pass\n",
    "    if counter != 0:\n",
    "        counter+=1\n",
    "    repeat_num.append(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_sayilari = pd.DataFrame({\"sku\": sku,\n",
    "                             \"grup_adi\": grup_adi,\n",
    "                             \"portfoy\": portfoy,\n",
    "                             \"gozlem_sayisi\": gozlem_sayisi,\n",
    "                             \"toplam_satir\": toplam_satir,\n",
    "                             \"oran\": oran,\n",
    "                             \"baslangic_tarih\": baslangic_tarih,\n",
    "                             \"bitis_tarih\": bitis_tarih,\n",
    "                             \"son_kac_ay_eksik\": son_kac_ay_eksik,\n",
    "                             \"eksik_repeat_sayisi\": repeat_num,\n",
    "                             \"kanal\": kanal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_sayilari.sort_values(by=[\"sku\", \"grup_adi\", \"oran\"], ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKU gözlem sayılarının elde edilme süresi: 0:00:47.363114\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('SKU gözlem sayılarının elde edilme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adetteki düzenlemeleri new_adet üzerinden devam ettiriyoruz\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"new_adet\"] = pasifik_df_sorted[\"adet\"]\n",
    "horizon_saha_df_sorted[\"new_adet\"] = horizon_saha_df_sorted[\"adet\"]\n",
    "btt_df_sorted[\"new_adet\"] = btt_df_sorted[\"adet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SKU Sayıları Dataya Ekleme, Scope Belirlenmesi İçin Gerekli\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_insights = sku_sayilari.copy()\n",
    "sku_insights.drop(columns=[\"portfoy\", \"kanal\"], axis=1, inplace=True)\n",
    "sku_insights.rename(columns={\"sku\": \"en_guncel_kod\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sku_sayilari = sku_sayilari[[\"sku\"] + sku_insights.columns.to_list()[1:]]\n",
    "    sku_sayilari.columns = sku_insights.columns\n",
    "except:\n",
    "    sku_sayilari.columns = sku_insights.columns    \n",
    "\n",
    "pasifik_df_sorted = pasifik_df_sorted.merge(sku_sayilari, on=[\"en_guncel_kod\", \"grup_adi\"], how=\"left\")\n",
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(sku_sayilari, on=[\"en_guncel_kod\", \"grup_adi\"], how=\"left\")\n",
    "btt_df_sorted = btt_df_sorted.merge(sku_sayilari, on=[\"en_guncel_kod\", \"grup_adi\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SKU Labellama\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"scope\"] = 0\n",
    "horizon_saha_df_sorted[\"scope\"] = 0\n",
    "btt_df_sorted[\"scope\"] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backup_pas1 = pasifik_df_sorted.copy()\n",
    "backup_hor1 = horizon_saha_df_sorted.copy()\n",
    "backup_btt1 = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope(data, obs_threshold, obs_mean_threshold, obs_ratio, latest_obs_date):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    obs_threshold:        int\n",
    "    obs_mean_threshold:   int\n",
    "    obs_ratio:            float\n",
    "    latest_obs_date:      datetime\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    obs_threshold:        Toplam kaç adet gözlemi olsun?\n",
    "    obs_mean_threshold:   Kaç gözlem altında ortalama basılıp geçilsin?\n",
    "    obs_ratio:            Gözlem oranı yüzde kaç olmalı? %50'nin altında ise alınmasın vb.\n",
    "    latest_obs_date:      En son hangi tarihte gözlemi olursa analize dahil edilsin?\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    data[\"scope\"] = 0\n",
    "    for sku in data[\"en_guncel_kod\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "            \n",
    "            temp_df = data[(data[\"en_guncel_kod\"] == sku) & \n",
    "                           (data[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            if len(temp_df) > 1:\n",
    "                gozlem_sayisi = temp_df[\"gozlem_sayisi\"][0]\n",
    "                oran = temp_df[\"oran\"][0]\n",
    "                bitis_tarihi = temp_df[\"bitis_tarih\"][0]\n",
    "                \n",
    "                if (gozlem_sayisi < obs_threshold) and (oran < obs_ratio) and (bitis_tarihi <= latest_obs_date):\n",
    "                    temp_df[\"scope\"] = 0 # Kapsam Dışı\n",
    "                else:\n",
    "                    if (gozlem_sayisi < obs_mean_threshold) and (bitis_tarihi > latest_obs_date):\n",
    "                        temp_df[\"scope\"] = 1 # Ortalama basılacak olan\n",
    "                    elif (gozlem_sayisi >= obs_threshold) and (oran >= obs_ratio) and (bitis_tarihi > latest_obs_date):\n",
    "                        temp_df[\"scope\"] = 2 # Time Series\n",
    "                    elif (gozlem_sayisi >= obs_mean_threshold) and (bitis_tarihi > latest_obs_date):\n",
    "                        temp_df[\"scope\"] = 3 # Regresyon\n",
    "            else:\n",
    "                pass\n",
    "            all_data.append(temp_df)\n",
    "    all_data = pd.concat(all_data)\n",
    "    all_data.reset_index(drop=True, inplace=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = scope(pasifik_df_sorted, \n",
    "                          params_[\"scope_parameters\"][\"ts_threshold\"], \n",
    "                          params_[\"scope_parameters\"][\"mean_threshold\"], \n",
    "                          params_[\"scope_parameters\"][\"full_rate\"], \n",
    "                          datetime(params_[\"scope_parameters\"][\"time_threshold_year\"], params_[\"scope_parameters\"][\"time_threshold_month\"], 1))\n",
    "\n",
    "horizon_saha_df_sorted = scope(horizon_saha_df_sorted, \n",
    "                               params_[\"scope_parameters\"][\"ts_threshold\"], \n",
    "                               params_[\"scope_parameters\"][\"mean_threshold\"], \n",
    "                               params_[\"scope_parameters\"][\"full_rate\"], \n",
    "                               datetime(params_[\"scope_parameters\"][\"time_threshold_year\"], params_[\"scope_parameters\"][\"time_threshold_month\"], 1))\n",
    "\n",
    "btt_df_sorted = scope(btt_df_sorted,\n",
    "                      params_[\"scope_parameters\"][\"ts_threshold\"], \n",
    "                      params_[\"scope_parameters\"][\"mean_threshold\"], \n",
    "                      params_[\"scope_parameters\"][\"full_rate\"], \n",
    "                      datetime(params_[\"scope_parameters\"][\"time_threshold_year\"], params_[\"scope_parameters\"][\"time_threshold_month\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kapsam labellaması süresi: 0:02:56.561380\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kapsam labellaması süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Kapsamı yeniden düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "kapsam_all.columns = change_col_name(kapsam_all)\n",
    "eslenik_kod_df.columns = change_col_name(eslenik_kod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "kapsam_all[\"urun_kodu\"] = kapsam_all[\"urun_kodu\"].apply(lambda x: int(x.split(\"-\")[0]+x.split(\"-\")[1]))\n",
    "kapsam_all = kapsam_all.merge(eslenik_kod_df[[\"urun_kodu\", \"en_guncel_kod\"]], how=\"left\", on=\"urun_kodu\")\n",
    "kapsam_all[\"en_guncel_kod\"].fillna(kapsam_all[\"urun_kodu\"], inplace=True)\n",
    "kapsam_all.drop_duplicates(subset=[\"en_guncel_kod\", \"grup_adi\"], inplace=True, ignore_index=True)\n",
    "kapsam_all = kapsam_all[kapsam_all[\"en_guncel_kod\"] != \"delist\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BACKUP_PASIFIK = pasifik_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df_sorted.merge(kapsam_all[[\"en_guncel_kod\", \"grup_adi\", \"durum\"]], how=\"left\", on=[\"en_guncel_kod\", \"grup_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_write_deneme = list(pasifik_df_sorted[(pasifik_df_sorted[\"durum\"].isna()) & (pasifik_df_sorted[\"grup_adi\"].isin([\"A101\", \"ŞOK\", \"BİM\"]))].index)\n",
    "check = pasifik_df_sorted[(pasifik_df_sorted[\"grup_adi\"] == \"BİM\") & (pasifik_df_sorted[\"portfoy\"] == 1)]\n",
    "check = check[[\"en_guncel_kod\", \"grup_adi\", \"portfoy\", \"durum\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.loc[idx_to_write_deneme, \"durum\"] = \"DENEME\"\n",
    "horizon_saha_df_sorted[\"durum\"] = np.nan\n",
    "btt_df_sorted[\"durum\"] = np.nan\n",
    "pasifik_df_sorted[\"Kanal\"] = \"pasifik\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_backup_kapsam = pasifik_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_scope_index = pasifik_df_sorted[(pasifik_df_sorted[\"portfoy\"] == 1) & \n",
    "                                       (pasifik_df_sorted[\"Kanal\"] == \"pasifik\") & \n",
    "                                       (pasifik_df_sorted[\"durum\"].isin(['DENEME+BÖLGESEL SATIŞ', 'DENEME'])) & \n",
    "                                       (~pasifik_df_sorted[\"grup_adi\"].isin([\"Diğer_Pasifik\", \"MİGROS\"]))].index\n",
    "\n",
    "pasifik_df_sorted.loc[change_scope_index, \"scope\"] = 0\n",
    "pasifik_df_sorted.loc[change_scope_index, \"scope_type\"] = \"kapsam_disi\"\n",
    "pasifik_df_sorted.drop(\"Kanal\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Peak ve Dip Noktalarını Belirleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_detection(data___, sigma):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    sigma:                float\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    sigma:                Standart sapma ile çarpılacak olan sayı. Verinin yüzdelik olarak hangi kısmının threshold olarak alınacağına bu değer ile karar verilir. Örn: 1.96 verilmesi durumunda %95'e tekabul eder. \n",
    "                          (https://www.socscistatistics.com/pvalues/normaldistribution.aspx)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    demand = data___.copy()\n",
    "    demand[\"peak\"] = 0\n",
    "    demand_non_portfoy = demand[demand[\"portfoy\"] == 0]\n",
    "    demand = demand[demand[\"portfoy\"] == 1]\n",
    "\n",
    "    grup = demand[\"grup_adi\"].unique()\n",
    "    urun = demand[\"en_guncel_kod\"].unique()\n",
    "    for grp in grup:\n",
    "        for sku in urun:\n",
    "            df___ = demand[(demand['en_guncel_kod']==sku) & \n",
    "                           (demand[\"grup_adi\"] == grp)]\n",
    "            points = df___['adet']\n",
    "\n",
    "            mean = points.mean()\n",
    "            std = points.std()\n",
    "\n",
    "            peaks=[]\n",
    "            index=[]\n",
    "            for idx in points.index:\n",
    "                if points[idx]>=(sigma*std+mean):\n",
    "                    index.append(idx)\n",
    "\n",
    "            for idx_ in index:\n",
    "                demand.loc[idx_, \"peak\"] = 1\n",
    "    demand = pd.concat([demand, demand_non_portfoy], axis=0)\n",
    "    demand.sort_index(inplace=True)\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_detection(data___, sigma):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    sigma:                float\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    sigma:                Standart sapma ile çarpılacak olan sayı. Verinin yüzdelik olarak hangi kısmının threshold olarak alınacağına bu değer ile karar verilir. Örn: 1.96 verilmesi durumunda %95'e tekabul eder. \n",
    "                          (https://www.socscistatistics.com/pvalues/normaldistribution.aspx)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    demand = data___.copy()\n",
    "    demand_non_portfoy = demand[demand[\"portfoy\"] == 0]\n",
    "    demand = demand[demand[\"portfoy\"] == 1]\n",
    "\n",
    "    grup = demand[\"grup_adi\"].unique()\n",
    "    urun = demand[\"en_guncel_kod\"].unique()\n",
    "    for grp in grup:\n",
    "        for sku in urun:\n",
    "            df___ = demand[(demand['en_guncel_kod']==sku) & \n",
    "                           (demand[\"grup_adi\"] == grp)]\n",
    "            points = df___['adet']\n",
    "\n",
    "            mean = points.mean()\n",
    "            std = points.std()\n",
    "\n",
    "            peaks=[]\n",
    "            index=[]\n",
    "            if mean <= 100000:\n",
    "                for idx in points.index:\n",
    "                    if (points[idx]<=(mean-sigma*std)):\n",
    "                        index.append(idx)\n",
    "\n",
    "\n",
    "                for idx_ in sorted(index, reverse=True):\n",
    "                    try:\n",
    "                        demand.loc[idx_-1, \"new_adet\"] += demand.loc[idx_, \"new_adet\"] \n",
    "                        demand.loc[idx_, \"new_adet\"] = 1\n",
    "                    except KeyError:\n",
    "                        index.remove(idx_)\n",
    "            else:\n",
    "                for idx in points.index:\n",
    "                    if (points[idx]<=(mean-2*std)):\n",
    "                        index.append(idx)\n",
    "\n",
    "\n",
    "                for idx_ in sorted(index, reverse=True):\n",
    "                    try:\n",
    "                        demand.loc[idx_-1, \"new_adet\"] += demand.loc[idx_, \"new_adet\"] \n",
    "                        demand.loc[idx_, \"new_adet\"] = 1\n",
    "                    except KeyError:\n",
    "                        index.remove(idx_)\n",
    "    demand = pd.concat([demand, demand_non_portfoy], axis=0)\n",
    "    demand.sort_index(inplace=True)\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_distribution(data___, sigma):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    sigma:                float\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    sigma:                Standart sapma ile çarpılacak olan sayı. Verinin yüzdelik olarak hangi kısmının threshold olarak alınacağına bu değer ile karar verilir. Örn: 1.96 verilmesi durumunda %95'e tekabul eder. \n",
    "                          (https://www.socscistatistics.com/pvalues/normaldistribution.aspx)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    demand = data___.copy()\n",
    "    demand_non_portfoy = demand[demand[\"portfoy\"] == 0]\n",
    "    demand = demand[demand[\"portfoy\"] == 1]\n",
    "\n",
    "    grup = demand[\"grup_adi\"].unique()\n",
    "    urun = demand[\"en_guncel_kod\"].unique()\n",
    "    for grp in grup:\n",
    "        for sku in urun:\n",
    "            df___= demand[(demand['en_guncel_kod']==sku) & \n",
    "                          (demand[\"grup_adi\"] == grp)]\n",
    "            points = df___['adet']\n",
    "            \n",
    "            mean = points.mean()\n",
    "            std = points.std()\n",
    "            peak_index = df___[df___[\"peak\"] == 1].index\n",
    "            idx_to_add = list(df___[(df___[\"peak\"] != 1) & (df___[\"adet\"] != 1)].index)\n",
    "            for idx in peak_index:\n",
    "                peak_value = demand.loc[idx, \"adet\"]\n",
    "                will_add = (peak_value - (mean+std*sigma)) / len(idx_to_add)\n",
    "                demand.loc[idx_to_add, \"new_adet\"] += will_add\n",
    "                demand.loc[idx, \"new_adet\"] = mean+std*sigma\n",
    "    demand = pd.concat([demand, demand_non_portfoy], axis=0)\n",
    "    demand.sort_index(inplace=True)\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pasifik_df_sorted = pas_backup.copy()\n",
    "horizon_saha_df_sorted = hor_backup.copy()\n",
    "btt_df_sorted = btt_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = peak_detection(pasifik_df_sorted, params_[\"sigma_parameters\"][\"sigma_value\"])\n",
    "horizon_saha_df_sorted = peak_detection(horizon_saha_df_sorted, params_[\"sigma_parameters\"][\"sigma_value\"])\n",
    "btt_df_sorted = peak_detection(btt_df_sorted, params_[\"sigma_parameters\"][\"sigma_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Detection süresi: 0:00:13.339807\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Peak Detection süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pas_backup2 = pasifik_df_sorted.copy()\n",
    "hor_backup2 = horizon_saha_df_sorted.copy()\n",
    "btt_backup2 = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = peak_distribution(pasifik_df_sorted, params_[\"sigma_parameters\"][\"sigma_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_sorted = peak_distribution(btt_df_sorted, params_[\"sigma_parameters\"][\"sigma_value\"])\n",
    "horizon_saha_df_sorted = peak_distribution(horizon_saha_df_sorted, params_[\"sigma_parameters\"][\"sigma_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Distribution süresi: 0:00:23.618553\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Peak Distribution süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = deep_detection(pasifik_df_sorted, params_[\"sigma_parameters\"][\"sigma_value\"])\n",
    "horizon_saha_df_sorted = deep_detection(horizon_saha_df_sorted, params_[\"sigma_parameters\"][\"sigma_value\"])\n",
    "btt_df_sorted = deep_detection(btt_df_sorted, params_[\"sigma_parameters\"][\"sigma_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Detection süresi: 0:00:11.986289\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Deep Detection süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pas_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"raf_tavsiye_satis_fiyati\", \"i̇ndirimli_raf_satis_fiyati\", \n",
    "                        \"aktivite_tipi\", \"indirim_\"]\n",
    "\n",
    "    btt_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"i̇skonto_\"]\n",
    "\n",
    "    hor_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"i̇skonto_\"]\n",
    "\n",
    "    pasifik_df_sorted.drop(columns=pas_cols_to_drop, inplace=True)\n",
    "    horizon_saha_df_sorted.drop(columns=hor_cols_to_drop, inplace=True)\n",
    "    btt_df_sorted.drop(columns=btt_cols_to_drop, inplace=True)\n",
    "except:\n",
    "    pas_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"raf_tavsiye_satis_fiyati\", \"i̇ndirimli_raf_satis_fiyati\", \n",
    "                        \"aktivite_tipi\", \"indirim_\"]\n",
    "\n",
    "    btt_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"iskonto_\"]\n",
    "\n",
    "    hor_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"iskonto_\"]\n",
    "\n",
    "    pasifik_df_sorted.drop(columns=pas_cols_to_drop, inplace=True)\n",
    "    horizon_saha_df_sorted.drop(columns=hor_cols_to_drop, inplace=True)\n",
    "    btt_df_sorted.drop(columns=btt_cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pd.merge(pasifik_df_sorted, pasifik_aktivite_df3[['yil', 'ay', 'grup_adi', 'en_guncel_kod', \n",
    "                                                                      'raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', 'indirim_', \n",
    "                                                                      'aktivite_tipi']], \n",
    "                             left_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                             right_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                             how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(saha_aktivite_detay3[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                                            'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                                      on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], \n",
    "                                                      how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_sorted = btt_df_sorted.merge(btt_aktivite[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                  'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                    on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], \n",
    "                                    how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"indirim__\"] = [0 if akt < 0 else akt for akt in pasifik_df_sorted[\"indirim_\"]]\n",
    "pasifik_df_sorted.drop(\"indirim_\", axis=1, inplace=True)\n",
    "\n",
    "horizon_saha_df_sorted[\"i̇skonto__\"] = [0 if ((akt >= 0.35) or (akt <=0.01)) else akt for akt in horizon_saha_df_sorted[\"i̇skonto_\"]]\n",
    "horizon_saha_df_sorted.drop(\"i̇skonto_\", axis=1, inplace=True)\n",
    "\n",
    "btt_df_sorted[\"i̇skonto__\"] = [0 if ((akt >= 0.35) or (akt <=0.01)) else akt for akt in btt_df_sorted[\"i̇skonto_\"]]\n",
    "btt_df_sorted.drop(\"i̇skonto_\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "chng_cols = dict(zip(['Yıl', 'Ay', 'No_of_days', 'Weekdays_n', 'Weekdays_Ratio', 'Weekend_n',\n",
    "                      'Weekend_Ratio', 'Actual_Holiday_n', 'Actual_Holiday_Ratio',\n",
    "                      'Total_Holiday_n', 'Total_Holiday_Ratio', 'School_Day_n',\n",
    "                      'School_Day_Ratio', 'School_Day_brdg_n', 'School_Day_brdg_Ratio',\n",
    "                      'Ramadan_n', 'Ramadan_Ratio', 'Pandemic', 'Lockdown'],\n",
    "                     \n",
    "                     [\"yil\", \"ay\", \"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \n",
    "                      \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                      \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \n",
    "                      \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                      \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "calender_df.rename(columns=chng_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pd.merge(pasifik_df_sorted, calender_df, on=[\"yil\", \"ay\"], how=\"left\")\n",
    "btt_df_sorted = pd.merge(btt_df_sorted, calender_df, on=[\"yil\", \"ay\"], how=\"left\")\n",
    "horizon_saha_df_sorted = pd.merge(horizon_saha_df_sorted, calender_df, on=[\"yil\", \"ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"indirim__\"].fillna(0, inplace=True)\n",
    "horizon_saha_df_sorted[\"i̇skonto__\"].fillna(0, inplace=True)\n",
    "btt_df_sorted[\"i̇skonto__\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted.rename(columns={\"i̇skonto__\": \"indirim__\"}, inplace=True)\n",
    "btt_df_sorted.rename(columns={\"i̇skonto__\": \"indirim__\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bir önceki aya yansımış aktiviteleri düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aktivite_regulation(df):\n",
    "    df_all = []\n",
    "    for sku in df[\"en_guncel_kod\"].unique():\n",
    "        for grup in df[\"grup_adi\"].unique():\n",
    "            test = df[(df[\"en_guncel_kod\"] == sku) & (df[\"grup_adi\"] == grup)]\n",
    "            for idx in test.index:\n",
    "                if (idx-2 not in test.index) and (idx-1 in test.index): # bir öncekine bakacak. ilk ve sonraki satıra bakacak. -1 götür\n",
    "                    if test.loc[idx-1, \"adet\"] > test.loc[idx, \"adet\"]:\n",
    "                        test.loc[idx-1, \"indirim__\"] += test.loc[idx, \"indirim__\"]\n",
    "                        test.loc[idx, \"indirim__\"] = 0\n",
    "                elif (idx-1 not in test.index): # ilk satırdayız. pass\n",
    "                    pass\n",
    "                else:\n",
    "                    dic = {}\n",
    "                    dic.update({idx-2: test.loc[idx-2, \"adet\"], \n",
    "                                idx-1: test.loc[idx-1, \"adet\"],\n",
    "                                idx: test.loc[idx, \"adet\"]})\n",
    "                    max_idx = max(dic, key=dic.get)\n",
    "                    if max_idx == idx:\n",
    "                        pass\n",
    "                    else:\n",
    "                        test.loc[max_idx, \"indirim__\"] += test.loc[idx, \"indirim__\"]\n",
    "                        test.loc[idx, \"indirim__\"] = 0\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasifik_df_sorted, horizon_saha_df_sorted, btt_df_sorted = aktivite_regulation(pasifik_df_sorted), aktivite_regulation(horizon_saha_df_sorted), aktivite_regulation(btt_df_sorted)\n",
    "#PASİFİK İÇİN İPTAL\n",
    "horizon_saha_df_sorted, btt_df_sorted = aktivite_regulation(horizon_saha_df_sorted), aktivite_regulation(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktivite Regulation süresi: 0:04:19.175964\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Aktivite Regulation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pasifik_aktivite_regulation(df):\n",
    "    df_all = []\n",
    "    for sku in df[\"en_guncel_kod\"].unique():\n",
    "        for grup in df[\"grup_adi\"].unique():\n",
    "            test = df[(df[\"en_guncel_kod\"] == sku) & (df[\"grup_adi\"] == grup)]\n",
    "            aktivite_tip_index = test[~test.aktivite_tipi.isna()].index.to_list()\n",
    "            for idx in aktivite_tip_index:\n",
    "                if idx == test.index[0]: # ilk satırsa atla\n",
    "                    pass\n",
    "                elif idx-1 in aktivite_tip_index: #bir önceki satırda aktivite varsa atla\n",
    "                    pass\n",
    "                else:\n",
    "                    if test.loc[idx, 'adet'] < test.loc[idx-1, 'adet']: # adet sayısı bir önceki satırdan küçükse \n",
    "                        test.loc[idx-1, 'aktivite_tipi'] = test.loc[idx, 'aktivite_tipi'] # aktiviteyi bir önceki satıra yaz\n",
    "                        test.loc[idx, 'aktivite_tipi'] = np.nan\n",
    "                        aktivite_tip_index.remove(idx) #listeden remove et ki bir alt satırda varsa önceki var mı kontrolüne takılmasın\n",
    "                        aktivite_tip_index.insert(0,0) #döngü listedeki elementlerin index'ine göre devam ettiği için en başa 0 insert et\n",
    "\n",
    "            indirim_index = test[test.indirim__ != 0].index.to_list()\n",
    "            for idx in indirim_index:\n",
    "                if idx == test.index[0]: # ilk satır\n",
    "                    pass \n",
    "                elif idx-1 in indirim_index: #bir önceki satırda indirim yüzdesi varsa atla\n",
    "                    pass \n",
    "                else:\n",
    "                    if test.loc[idx, 'adet'] < test.loc[idx-1, 'adet']:\n",
    "                        test.loc[idx-1, 'indirim__'] = test.loc[idx, 'indirim__'] #bir üste taşındı\n",
    "                        test.loc[idx, 'indirim__'] = 0  \n",
    "                        indirim_index.remove(idx)  #listeden remove et ki bir alt satırda varsa önceki var mı kontrolüne takılmasın\n",
    "                        indirim_index.insert(0, 0) #döngü listedeki elementlerin index'ine göre devam ettiği için en başa 0 insert et\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_aktivite_regulation(pasifik_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik Aktivite Regulation süresi: 0:00:42.686004\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik Aktivite Regulation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Binslere ayırma\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, pasifik_df_sorted.indirim__.max()+1]\n",
    "#indirim_bins = [0, 0.009, 0.05, 0.10, 0.15, 0.20, 0.25, 0.50, pasifik_df_sorted.indirim__.max()+1]\n",
    "pasifik_df_sorted.indirim__.hist(bins=indirim_bins)\n",
    "len(indirim_bins) , pasifik_df_sorted.indirim__.value_counts(bins=indirim_bins).sort_index()\n",
    "pasifik_df_sorted['indirim__bins'] = pd.cut(pasifik_df_sorted.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, horizon_saha_df_sorted.indirim__.max()+1]\n",
    "horizon_saha_df_sorted.indirim__.hist(bins=indirim_bins)\n",
    "len(indirim_bins) , horizon_saha_df_sorted.indirim__.value_counts(bins=indirim_bins)\n",
    "horizon_saha_df_sorted['indirim__bins'] = pd.cut(horizon_saha_df_sorted.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD4CAYAAAD2FnFTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAV6UlEQVR4nO3df5Bd5X3f8fc3bDDYchC20q1HUiw6lt1iiFO0BdxkkpWVgCAZi06JB4cY2aNEMzF2acI0QDMpHttM8SSEGOIfVS2NhU29EOJaKgYTjdCGSWLJoODyM5gNYCwVo9iSlQqwHdnf/nEfmTvbZ7WXc3fvPRHv18zOnvOc55z7uRKrz73nnL1EZiJJ0nQ/NuwAkqR2siAkSVUWhCSpyoKQJFVZEJKkqpFhB2hq0aJFuWzZskb7Pvfcc7zqVa+a20BzpM3ZwHz9aHM2aHe+NmeDduebnm337t3fysyf7GnnzPwn+bVixYpsaseOHY33nW9tzpZpvn60OVtmu/O1OVtmu/NNzwbclz3+O+spJklSlQUhSaqyICRJVRaEJKnKgpAkVVkQkqQqC0KSVGVBSJKqLAhJUtXLsiAe3HuQZVd+cdgxJKnVXpYFIUmanQUhSaqyICRJVRaEJKnKgpAkVVkQkqQqC0KSVGVBSJKqLAhJUpUFIUmqmrUgImJTROyLiIe6xv4gIv42Ih6IiP8ZEQu7tl0VEVMR8VhEnNs1vrqMTUXElV3jp0TErjJ+S0QcP4fPT5LUUC/vID4NrJ42tg04LTN/GvgacBVARJwKXAS8uezz8Yg4LiKOAz4GnAecCryzzAX4CHB9Zr4BOACs6+sZSZLmxKwFkZn3APunjf15Zh4uqzuBJWV5DTCRmd/LzCeBKeDM8jWVmU9k5veBCWBNRATwNuC2sv9m4IL+npIkaS5EZs4+KWIZcHtmnlbZ9r+AWzLzsxHxJ8DOzPxs2bYRuLNMXZ2Zv1HG3wWcBXygzH9DGV8K3Fl7nLJ9PbAeYHR0dMXExMRLeKov2rf/IM++AKcvPqnR/vPp0KFDLFiwYNgxZmS+5tqcDdqdr83ZoN35pmdbuXLl7swc62XfkX4eOCJ+DzgM3NzPcXqVmRuADQBjY2M5Pj7e6Dg33ryF6x4c4amLm+0/nyYnJ2n6vAbBfM21ORu0O1+bs0G78/WTrXFBRMS7gV8BVuWLb0P2Aku7pi0pY8ww/m1gYUSMlFNW3fMlSUPU6DbXiFgN/C7w9sx8vmvTVuCiiHhFRJwCLAe+AtwLLC93LB1P50L21lIsO4ALy/5rgS3NnookaS71cpvr54AvA2+KiD0RsQ74E+DVwLaI+GpEfBIgMx8GbgUeAb4EXJqZPyjvDt4H3AU8Ctxa5gJcAfxOREwBrwU2zukzlCQ1Musppsx8Z2V4xn/EM/Ma4JrK+B3AHZXxJ+jc5SRJahF/k1qSVGVBSJKqLAhJUpUFIUmqsiAkSVUWhCSpyoKQJFVZEJKkKgtCklRlQUiSqiwISVKVBSFJqrIgJElVFoQkqcqCkCRVWRCSpCoLQpJUZUFIkqosCElSlQUhSaqyICRJVRaEJKnKgpAkVc1aEBGxKSL2RcRDXWOviYhtEfF4+X5yGY+IuCEipiLigYg4o2uftWX+4xGxtmt8RUQ8WPa5ISJirp+kJOml6+UdxKeB1dPGrgS2Z+ZyYHtZBzgPWF6+1gOfgE6hAFcDZwFnAlcfKZUy5ze79pv+WJKkIZi1IDLzHmD/tOE1wOayvBm4oGv8puzYCSyMiNcB5wLbMnN/Zh4AtgGry7afyMydmZnATV3HkiQN0UjD/UYz85my/E1gtCwvBr7RNW9PGTva+J7KeFVErKfzzoTR0VEmJyebhT8RLj/9cOP959OhQ4damesI8zXX5mzQ7nxtzgbtztdPtqYF8SOZmRGR/R6nx8faAGwAGBsby/Hx8UbHufHmLVz34AhPXdxs//k0OTlJ0+c1COZrrs3ZoN352pwN2p2vn2xN72J6tpweonzfV8b3Aku75i0pY0cbX1IZlyQNWdOC2AocuRNpLbCla/yScjfT2cDBcirqLuCciDi5XJw+B7irbPuHiDi73L10SdexJElDNOsppoj4HDAOLIqIPXTuRroWuDUi1gFfB95Rpt8BnA9MAc8D7wHIzP0R8SHg3jLvg5l55ML3e+ncKXUicGf5kiQN2awFkZnvnGHTqsrcBC6d4TibgE2V8fuA02bLIUkaLH+TWpJUZUFIkqosCElSlQUhSaqyICRJVRaEJKnKgpAkVVkQkqQqC0KSVGVBSJKqLAhJUpUFIUmqsiAkSVUWhCSpyoKQJFVZEJKkKgtCklRlQUiSqiwISVKVBSFJqrIgJElVFoQkqcqCkCRV9VUQEfHbEfFwRDwUEZ+LiBMi4pSI2BURUxFxS0QcX+a+oqxPle3Luo5zVRl/LCLO7fM5SZLmQOOCiIjFwH8AxjLzNOA44CLgI8D1mfkG4ACwruyyDjhQxq8v84iIU8t+bwZWAx+PiOOa5pIkzY1+TzGNACdGxAjwSuAZ4G3AbWX7ZuCCsrymrFO2r4qIKOMTmfm9zHwSmALO7DOXJKlPkZnNd464DLgGeAH4c+AyYGd5l0BELAXuzMzTIuIhYHVm7inb/g44C/hA2eezZXxj2ee2yuOtB9YDjI6OrpiYmGiUe9/+gzz7Apy++KRG+8+nQ4cOsWDBgmHHmJH5mmtzNmh3vjZng3bnm55t5cqVuzNzrJd9R5o+aEScTOfV/ynAd4A/pXOKaN5k5gZgA8DY2FiOj483Os6NN2/hugdHeOriZvvPp8nJSZo+r0EwX3NtzgbtztfmbNDufP1k6+cU0y8CT2bm32fmPwKfB34WWFhOOQEsAfaW5b3AUoCy/STg293jlX0kSUPST0E8DZwdEa8s1xJWAY8AO4ALy5y1wJayvLWsU7bfnZ3zW1uBi8pdTqcAy4Gv9JFLkjQHGp9iysxdEXEb8DfAYeB+Oqd/vghMRMSHy9jGsstG4DMRMQXsp3PnEpn5cETcSqdcDgOXZuYPmuaSJM2NxgUBkJlXA1dPG36Cyl1Imfld4FdnOM41dC52S5Jawt+kliRVWRCSpCoLQpJUZUFIkqosCElSlQUhSaqyICRJVRaEJKnKgpAkVVkQkqQqC0KSVGVBSJKqLAhJUpUFIUmqsiAkSVUWhCSpyoKQJFVZEJKkKgtCklRlQUiSqiwISVKVBSFJqrIgJElVfRVERCyMiNsi4m8j4tGIeGtEvCYitkXE4+X7yWVuRMQNETEVEQ9ExBldx1lb5j8eEWv7fVKSpP71+w7io8CXMvNfAm8BHgWuBLZn5nJge1kHOA9YXr7WA58AiIjXAFcDZwFnAlcfKRVJ0vA0LoiIOAn4eWAjQGZ+PzO/A6wBNpdpm4ELyvIa4Kbs2AksjIjXAecC2zJzf2YeALYBq5vmkiTNjcjMZjtG/AywAXiEzruH3cBlwN7MXFjmBHAgMxdGxO3AtZn5l2XbduAKYBw4ITM/XMZ/H3ghM/+w8pjr6bz7YHR0dMXExESj7Pv2H+TZF+D0xSc12n8+HTp0iAULFgw7xozM11ybs0G787U5G7Q73/RsK1eu3J2ZY73sO9LH444AZwDvz8xdEfFRXjydBEBmZkQ0a6CKzNxAp5QYGxvL8fHxRse58eYtXPfgCE9d3Gz/+TQ5OUnT5zUI5muuzdmg3fnanA3ana+fbP1cg9gD7MnMXWX9NjqF8Ww5dUT5vq9s3wss7dp/SRmbaVySNESNCyIzvwl8IyLeVIZW0TndtBU4cifSWmBLWd4KXFLuZjobOJiZzwB3AedExMnl4vQ5ZUySNET9nGICeD9wc0QcDzwBvIdO6dwaEeuArwPvKHPvAM4HpoDny1wyc39EfAi4t8z7YGbu7zOXJKlPfRVEZn4VqF3sWFWZm8ClMxxnE7CpnyySpLnlb1JLkqosCElSlQUhSaqyICRJVRaEJKnKgpAkVVkQkqQqC0KSVGVBSJKqLAhJUpUFIUmqsiAkSVUWhCSpyoKQJFVZEJKkKgtCklRlQUiSqiwISVKVBSFJqrIgJElVFoQkqcqCkCRVWRCSpKq+CyIijouI+yPi9rJ+SkTsioipiLglIo4v468o61Nl+7KuY1xVxh+LiHP7zSRJ6t9cvIO4DHi0a/0jwPWZ+QbgALCujK8DDpTx68s8IuJU4CLgzcBq4OMRcdwc5JIk9aGvgoiIJcAvA58q6wG8DbitTNkMXFCW15R1yvZVZf4aYCIzv5eZTwJTwJn95JIk9a/fdxB/DPwu8MOy/lrgO5l5uKzvARaX5cXANwDK9oNl/o/GK/tIkoZkpOmOEfErwL7M3B0R43OW6OiPuR5YDzA6Osrk5GSj44yeCJeffrjx/vPp0KFDrcx1hPmaa3M2aHe+NmeDdufrK1tmNvoC/iudV/tPAd8EngduBr4FjJQ5bwXuKst3AW8tyyNlXgBXAVd1HfdH8472tWLFimzqhs9+IV9/xe2N959PO3bsGHaEozJfc23OltnufG3OltnufNOzAfdlj//ONz7FlJlXZeaSzFxG5yLz3Zl5MbADuLBMWwtsKctbyzpl+90l7FbgonKX0ynAcuArTXNJkuZG41NMR3EFMBERHwbuBzaW8Y3AZyJiCthPp1TIzIcj4lbgEeAwcGlm/mAeckmSXoI5KYjMnAQmy/ITVO5CyszvAr86w/7XANfMRRZJ0tzwN6klSVUWhCSpyoKQJFVZEJKkKgtCklRlQUiSqiwISVKVBSFJqrIgJElVFoQkqcqCkCRVWRCSpCoLQpJUZUFIkqosCElSlQUhSaqyICRJVRaEJKnKgpAkVVkQkqQqC0KSVGVBSJKqLAhJUpUFIUmqalwQEbE0InZExCMR8XBEXFbGXxMR2yLi8fL95DIeEXFDRExFxAMRcUbXsdaW+Y9HxNr+n5YkqV/9vIM4DFyemacCZwOXRsSpwJXA9sxcDmwv6wDnAcvL13rgE9ApFOBq4CzgTODqI6UiSRqexgWRmc9k5t+U5f8LPAosBtYAm8u0zcAFZXkNcFN27AQWRsTrgHOBbZm5PzMPANuA1U1zSZLmRmRm/weJWAbcA5wGPJ2ZC8t4AAcyc2FE3A5cm5l/WbZtB64AxoETMvPDZfz3gRcy8w8rj7OezrsPRkdHV0xMTDTKu2//QZ59AU5ffFKj/efToUOHWLBgwbBjzMh8zbU5G7Q7X5uzQbvzTc+2cuXK3Zk51su+I/0+eEQsAP4M+I+Z+Q+dTujIzIyI/hvoxeNtADYAjI2N5fj4eKPj3HjzFq57cISnLm62/3yanJyk6fMaBPM11+Zs0O58bc4G7c7XT7a+7mKKiB+nUw43Z+bny/Cz5dQR5fu+Mr4XWNq1+5IyNtO4JGmI+rmLKYCNwKOZ+Uddm7YCR+5EWgts6Rq/pNzNdDZwMDOfAe4CzomIk8vF6XPKmCRpiPo5xfSzwLuAByPiq2XsPwPXArdGxDrg68A7yrY7gPOBKeB54D0Ambk/Ij4E3FvmfTAz9/eRS5I0BxoXRLnYHDNsXlWZn8ClMxxrE7CpaRZJ0tzzN6klSVUWhCSpyoKQJFVZEJKkKgtCklRlQUiSqiwISVKVBSFJqrIgJElVFoQkqcqCkCRVWRCSpCoLQpJU9bIuiGVXfpFlV35x2DEkqZVe1gUhSZqZBSFJqrIgJElV/fwvR/US9XK94/LTD/PuFl8XMV9zbc4G7c7X5mww2HxPXfvLA3kcsCCAF//hbvoH74VuScciTzF18a4mSXqRBVFhUUiSBXFUFoWklzMLogcWhaSXo9ZcpI6I1cBHgeOAT2XmtUOO9P+ZXhKDvJtAkgatFQUREccBHwN+CdgD3BsRWzPzkeEmOzrfVUg6lrXlFNOZwFRmPpGZ3wcmgDVDziRJL2uRmcPOQERcCKzOzN8o6+8CzsrM902btx5YX1bfBDzW8CEXAd9quO98a3M2MF8/2pwN2p2vzdmg3fmmZ3t9Zv5kLzu24hRTrzJzA7Ch3+NExH2ZOTYHkeZcm7OB+frR5mzQ7nxtzgbtztdPtracYtoLLO1aX1LGJElD0paCuBdYHhGnRMTxwEXA1iFnkqSXtVacYsrMwxHxPuAuOre5bsrMh+fxIfs+TTWP2pwNzNePNmeDdudrczZod77G2VpxkVqS1D5tOcUkSWoZC0KSVHVMF0RErI6IxyJiKiKurGx/RUTcUrbviohlLcr2OxHxSEQ8EBHbI+L1g8rWS76uef8+IjIiBnaLXy/ZIuId5c/v4Yj4H4PK1ku+iPipiNgREfeXv9/zB5htU0Tsi4iHZtgeEXFDyf5ARJzRomwXl0wPRsRfR8RbBpWtl3xd8/5NRBwuv9/VmmwRMR4RXy0/E3/R04Ez85j8onOx+++AfwEcD/xv4NRpc94LfLIsXwTc0qJsK4FXluXfGlS2XvOVea8G7gF2AmNtyQYsB+4HTi7r/6xNf3Z0Lhr+Vlk+FXhqgPl+HjgDeGiG7ecDdwIBnA3salG2f9v1d3reILP1kq/r7/9u4A7gwrZkAxYCjwA/VdZ7+pk4lt9B9PLxHWuAzWX5NmBVREQbsmXmjsx8vqzupPO7IYPS60effAj4CPDdlmX7TeBjmXkAIDP3tSxfAj9Rlk8C/s+gwmXmPcD+o0xZA9yUHTuBhRHxujZky8y/PvJ3yuB/Jnr5swN4P/BnwCD/m+sl268Bn8/Mp8v8nvIdywWxGPhG1/qeMladk5mHgYPAa1uSrds6Oq/qBmXWfOXUw9LMHPQnFvbyZ/dG4I0R8VcRsbN8UvCg9JLvA8CvR8QeOq803z+YaD15qf9tDsugfyZmFRGLgX8HfGLYWSreCJwcEZMRsTsiLullp1b8HoRmFhG/DowBvzDsLEdExI8BfwS8e8hRZjJC5zTTOJ1XmfdExOmZ+Z1hhuryTuDTmXldRLwV+ExEnJaZPxx2sH8KImIlnYL4uWFnmeaPgSsy84eDORHxkowAK4BVwInAlyNiZ2Z+bbadjlW9fHzHkTl7ImKEztv9b7ckGxHxi8DvAb+Qmd8bQK4jZsv3auA0YLL8IPxzYGtEvD0z7xtyNui86t2Vmf8IPBkRX6NTGPfOc7Ze860DVgNk5pcj4gQ6H6g20NMSM2j1x95ExE8DnwLOy8xB/Ky+FGPARPmZWAScHxGHM/MLQ03VsQf4dmY+BzwXEfcAbwGOWhDH8immXj6+YyuwtixfCNyd5QrOsLNFxL8G/hvw9gGfQ581X2YezMxFmbksM5fROR88iHKYNVvxBTrvHoiIRXTeXj8xgGy95nuazis5IuJfAScAfz+gfLPZClxS7mY6GziYmc8MOxR07v4CPg+8a7ZXvsOQmad0/UzcBry3JeUAsAX4uYgYiYhXAmcBj8620zH7DiJn+PiOiPggcF9mbgU20nl7P0XnAs9FLcr2B8AC4E/LK5KnM/PtLco3FD1muws4JyIeAX4A/KdBvdrsMd/lwH+PiN+mc8H63QN6YUJEfI5OeS4q10CuBn68ZP8knWsi5wNTwPPAewaRq8ds/4XONcKPl5+JwznAT1DtId/QzJYtMx+NiC8BDwA/pPN/7Tzq7brgR21IkmZwLJ9ikiT1wYKQJFVZEJKkKgtCklRlQUiSqiwISVKVBSFJqvp/YSQFNg3JIVIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, btt_df_sorted.indirim__.max()+1]\n",
    "btt_df_sorted.indirim__.hist(bins=indirim_bins)\n",
    "len(indirim_bins) , btt_df_sorted.indirim__.value_counts(bins=indirim_bins)\n",
    "btt_df_sorted['indirim__bins'] = pd.cut(btt_df_sorted.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Missing Imputation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_imputation(data):\n",
    "    df_all = []\n",
    "    data[\"new_adet\"] = data[\"adet\"]\n",
    "    for sku in data[\"en_guncel_kod\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "            test = data[(data[\"en_guncel_kod\"] == sku) & (data[\"grup_adi\"] == grup)]\n",
    "            index_full = list(test[test[\"new_adet\"] != 1].index)\n",
    "            for idx in range(len(index_full) - 1):\n",
    "                if index_full[idx+1] - index_full[idx] != 1:\n",
    "                    index_na = list(range(index_full[idx]+1, index_full[idx+1]))\n",
    "                    fark = test.loc[index_full[idx+1], \"new_adet\"] - test.loc[index_full[idx], \"new_adet\"]\n",
    "                    bol = len(index_na)\n",
    "                    ekle = fark/(bol+1)\n",
    "                    for i in index_na:\n",
    "                        test.loc[i, \"new_adet\"] = 0\n",
    "                        test.loc[i, \"new_adet\"] += ekle+test.loc[i-1, \"new_adet\"]\n",
    "                else:\n",
    "                    pass\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = missing_imputation(pasifik_df_sorted)\n",
    "horizon_saha_df_sorted = missing_imputation(horizon_saha_df_sorted)\n",
    "btt_df_sorted = missing_imputation(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Imputation süresi: 0:02:59.890515\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Missing Imputation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_regression = pasifik_df_sorted.copy()\n",
    "horizon_regression = horizon_saha_df_sorted.copy()\n",
    "btt_regression = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_regression = pasifik_regression[pasifik_regression[\"new_adet\"] != 1].reset_index(drop=True)\n",
    "horizon_regression = horizon_regression[horizon_regression[\"new_adet\"] != 1].reset_index(drop=True)\n",
    "btt_regression = btt_regression[btt_regression[\"new_adet\"] != 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_seasonality_decomp(data):\n",
    "    df_all = []\n",
    "    for sku in data[\"en_guncel_kod\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "#           print(grup, sku)\n",
    "            temp_df = data[(data[\"en_guncel_kod\"] == sku) & \n",
    "                           (data[\"grup_adi\"] == grup)]\n",
    "            if len(temp_df) > 2:\n",
    "#                print(sku, grup)\n",
    "                df_ts = temp_df[['new_adet','date']]\n",
    "                df_ts.set_index('date',inplace=True)\n",
    "\n",
    "                result = STL(df_ts).fit()\n",
    "                temp_df['season'] = list(result.seasonal)\n",
    "                temp_df['trend']  = list(result.trend)\n",
    "                temp_df['residual']  = list(result.resid)\n",
    "                df_all.append(temp_df)\n",
    "            else:\n",
    "                pass\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "horizon_saha_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "btt_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "\n",
    "btt_df_sorted.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\", \"date\"], keep=\"first\", ignore_index=True, inplace=True)\n",
    "horizon_saha_df_sorted.drop_duplicates(subset=[\"date\", \"grup_adi\", \"en_guncel_kod\"], keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_reg = pasifik_df_sorted[pasifik_df_sorted[\"scope\"] == 3]\n",
    "horizon_reg = horizon_saha_df_sorted[horizon_saha_df_sorted[\"scope\"] == 3]\n",
    "btt_reg = btt_df_sorted[btt_df_sorted[\"scope\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df = trend_seasonality_decomp(pasifik_df_sorted)\n",
    "horizon_df = trend_seasonality_decomp(horizon_saha_df_sorted)\n",
    "btt_df = trend_seasonality_decomp(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Seasonality süresi: 0:03:11.481595\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Trend Seasonality süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Yeni adet flaglendi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df[\"adet_flag\"] = np.where(pasifik_df[\"adet\"] - pasifik_df[\"new_adet\"] == 0, 0, 1)\n",
    "horizon_df[\"adet_flag\"] = np.where(horizon_df[\"adet\"] - horizon_df[\"new_adet\"] == 0, 0, 1)\n",
    "btt_df[\"adet_flag\"] = np.where(btt_df[\"adet\"] - btt_df[\"new_adet\"] == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df.copy()\n",
    "horizon_saha_df_sorted = horizon_df.copy()\n",
    "btt_df_sorted = btt_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted[\"aktivite_tipi\"] = np.nan\n",
    "btt_df_sorted[\"aktivite_tipi\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.rename(columns={'i̇ndirimli_raf_satis_fiyati': 'ciro_kull_i̇ade_dus',\n",
    "                                  'raf_tavsiye_satis_fiyati': 'promosyon_tutari'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"Kanal\"] = \"pasifik\"\n",
    "horizon_saha_df_sorted[\"Kanal\"] = \"horizon\"\n",
    "btt_df_sorted[\"Kanal\"] = \"btt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_dict = {0: \"kapsam_disi\", 1: \"ortalama_basilacak\", 2: \"ts\", 3: \"regresyon\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"scope_type\"] = pasifik_df_sorted[\"scope\"].map(scope_dict)\n",
    "horizon_saha_df_sorted[\"scope_type\"] = horizon_saha_df_sorted[\"scope\"].map(scope_dict)\n",
    "btt_df_sorted[\"scope_type\"] = btt_df_sorted[\"scope\"].map(scope_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([pasifik_df_sorted, horizon_saha_df_sorted, btt_df_sorted], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Aktivite Dictionary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "aktivite_dict = {0:0, 'Mağaza içi/Dağılım':2, 'İn&out':5, \n",
    "                 'Çoklu Alım':8, 'Mutluluk':11, 'Kasiyer':14, 'CRM':17}\n",
    "\n",
    "df_all.aktivite_tipi.fillna(0, inplace=True)\n",
    "df_all.aktivite_tipi = df_all.aktivite_tipi.map(aktivite_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datetime.now()\n",
    "df_all.to_excel(\"../data/data_backup/all_data_{}_{}_{}_{}_{}.xlsx\".format(d.year, d.month, d.day, d.minute, d.second), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datanın dışarıya aktarılması\n",
    "### Fiyata Göre Sıralanmış Datanın Dışarıya Aktarılması\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_all.drop(\"kanal\", axis=1, inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all = df_all[~((df_all[\"yil\"] == 2021) & (df_all[\"ay\"].isin([6, 7, 8, 9])))]\n",
    "data_main_ = df_all.copy()\n",
    "df = df_all.copy()\n",
    "df2 = df_all.copy()\n",
    "df3 = df2[[\"yil\", \"ay\", \"Kanal\", \"grup_adi\", \"urun_adi\", \"date\", \"new_adet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datayı dışarıya çıkarma süresi: 0:00:00.679005\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Datayı dışarıya çıkarma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eşlenik Kod ve Ürün İsim Kod kısmında düzenleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "urun_isim_kod = df2[[\"urun_adi\", \"en_guncel_kod\"]].drop_duplicates(subset=[\"urun_adi\", \"en_guncel_kod\"], ignore_index=True)\n",
    "urun_isim_kod.rename(columns={\"urun_adi\": \"urun\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "eslenik_kod_df.rename(columns={\"en_guncel_kod\": \"en_guncel_kod\", \"urun_adi_orjinal\": \"new_urun\"}, inplace=True)\n",
    "eslenik_kod_df = eslenik_kod_df[eslenik_kod_df[\"en_guncel_kod\"] != \"delist\"].reset_index(drop=True)\n",
    "eslenik_kod_df[\"en_guncel_kod\"] = eslenik_kod_df[\"en_guncel_kod\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for idx in eslenik_kod_df.index:\n",
    "    if eslenik_kod_df[\"kod1\"][idx] == eslenik_kod_df[\"en_guncel_kod\"][idx]:\n",
    "        indexes.append(idx)\n",
    "\n",
    "eslenik_kod_df2 = eslenik_kod_df[eslenik_kod_df.index.isin(indexes)].reset_index(drop=True)\n",
    "eslenik_kod_df2 = eslenik_kod_df2[[\"en_guncel_kod\", \"new_urun\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datanı altına kombinasyonların eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_all[(df_all[\"portfoy\"] == 1) & (df_all[\"scope_type\"] != \"kapsam_disi\")].reset_index(drop=True)\n",
    "data_backup = data.copy()\n",
    "sku_list = data.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], keep=\"first\", ignore_index=True)[[\"grup_adi\", \"en_guncel_kod\", \"Kanal\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 6, 1, 0, 0)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime(2021, 10, 1)-relativedelta(months=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enflasyon(df):\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "    month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "    year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "    if month_ == 12:\n",
    "        month_ = 1\n",
    "        year_+=1\n",
    "    else:\n",
    "        month_+=1\n",
    "\n",
    "    enflasyon_data = df[(df[\"date\"] >= datetime(year_, month_, 1)-relativedelta(months=4))]\n",
    "    enflasyon_data = enflasyon_data[[\"yil\", \"ay\", \"date\", \"enflasyon_etkisi\"]].drop_duplicates(subset=[\"ay\", \"yil\", \"date\", \"enflasyon_etkisi\"], ignore_index=True)\n",
    "    enflasyon_data = pd.concat([enflasyon_data]*2, ignore_index=True)\n",
    "    \n",
    "    for idx in range(4, len(enflasyon_data)):\n",
    "        new_date = datetime(year_, month_, 1) + relativedelta(months=idx-4)\n",
    "        enflasyon_data.loc[idx, \"enflasyon_etkisi\"] = np.nan\n",
    "        enflasyon_data.loc[idx, \"yil\"] = new_date.year\n",
    "        enflasyon_data.loc[idx, \"ay\"] = new_date.month\n",
    "        enflasyon_data.loc[idx, \"date\"] = new_date\n",
    "    for idx in range(4, len(enflasyon_data)):\n",
    "        enflasyon_data.loc[idx, \"enflasyon_etkisi\"] = enflasyon_data.loc[idx-4: idx-1, \"enflasyon_etkisi\"].mean()\n",
    "    enf_prev = data[data[\"date\"] < datetime(year_, month_, 1)][[\"yil\", \"ay\", \n",
    "                                                                \"date\", \"enflasyon_etkisi\"]].drop_duplicates(subset=[\"yil\", \"ay\", \n",
    "                                                                                                                     \"date\", \"enflasyon_etkisi\"]).sort_values(by=\"date\", ignore_index=True)\n",
    "    enflasyon_final = pd.concat([enf_prev, enflasyon_data.iloc[-4:, :]], axis=0, ignore_index=True)\n",
    "    return enflasyon_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "enflasyon_data = create_enflasyon(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "enf_dict_fill = {}\n",
    "for idx in range(len(enflasyon_data)):\n",
    "    enf_dict_fill.update({enflasyon_data[\"date\"][idx]: enflasyon_data[\"enflasyon_etkisi\"][idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comb(excels):\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "    month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "    year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "    pas_s=list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0, 2, 5, 8, 11, 14, 17]))\n",
    "    diger_s=list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0]))\n",
    "    df_all__=[]\n",
    "    for i in range(len(sku_list)):\n",
    "        tmp_=excels[(excels[\"grup_adi\"] == sku_list.loc[i, \"grup_adi\"]) & \n",
    "                   (excels[\"en_guncel_kod\"] == sku_list.loc[i, \"en_guncel_kod\"])]\n",
    "        new_date = datetime(year_, month_, 1)\n",
    "        if tmp_['Kanal'].values[0]=='pasifik':\n",
    "            for trh in range(1, 5):\n",
    "                new_date_ = new_date + relativedelta(months=trh)\n",
    "                create_new = pd.DataFrame(tmp_.iloc[-1])\n",
    "                create_new.loc[\"yil\"] = new_date_.year\n",
    "                create_new.loc[\"ay\"] = new_date_.month\n",
    "                create_new.loc[\"date\"] = datetime(new_date_.year, new_date_.month, 1)\n",
    "                create_new.loc[\"new_adet\"] = np.nan\n",
    "                create_new.loc[\"adet\"] = np.nan\n",
    "                create_new.loc[\"koli\"] = np.nan\n",
    "                create_new.loc[\"kg\"] = np.nan\n",
    "                create_new.loc[\"tl\"] = np.nan\n",
    "                create_new.loc[\"satis_var\"] = np.nan\n",
    "                create_new.loc[\"enflasyon_etkisi\"] = np.nan\n",
    "                create_new.loc[\"yarisma\"] = np.nan\n",
    "                create_new.loc[\"peak\"] = 0\n",
    "                create_new.loc[\"fiyat\"] = np.nan\n",
    "                create_new.loc[\"fiyat_gecisi\"] = np.nan\n",
    "                create_new.loc[\"pandemic\"] = 1\n",
    "                create_new.loc[\"lockdown\"] = 0\n",
    "                create_new.loc[\"season\"] = np.nan\n",
    "                create_new.loc[\"trend\"] = np.nan\n",
    "                tmp = pd.concat([create_new.T]*len(pas_s), ignore_index=True)\n",
    "                for j in range(len(pas_s)):\n",
    "                    tmp.loc[j,'indirim__bins'] = pas_s[j][0]\n",
    "                    tmp.loc[j,'aktivite_tipi'] = pas_s[j][1]\n",
    "                df_all__.append(tmp)\n",
    "        else:\n",
    "            for trh in range(1, 5):\n",
    "                new_date_ = new_date + relativedelta(months=trh)\n",
    "                create_new = pd.DataFrame(tmp_.iloc[-1])\n",
    "                create_new.loc[\"yil\"] = new_date_.year\n",
    "                create_new.loc[\"ay\"] = new_date_.month\n",
    "                create_new.loc[\"date\"] = datetime(new_date_.year, new_date_.month, 1)\n",
    "                create_new.loc[\"new_adet\"] = np.nan\n",
    "                create_new.loc[\"adet\"] = np.nan\n",
    "                create_new.loc[\"koli\"] = np.nan\n",
    "                create_new.loc[\"kg\"] = np.nan\n",
    "                create_new.loc[\"tl\"] = np.nan\n",
    "                create_new.loc[\"satis_var\"] = np.nan\n",
    "                create_new.loc[\"enflasyon_etkisi\"] = np.nan\n",
    "                create_new.loc[\"yarisma\"] = np.nan\n",
    "                create_new.loc[\"peak\"] = 0\n",
    "                create_new.loc[\"fiyat\"] = np.nan\n",
    "                create_new.loc[\"fiyat_gecisi\"] = np.nan\n",
    "                create_new.loc[\"pandemic\"] = 1\n",
    "                create_new.loc[\"lockdown\"] = 0\n",
    "                create_new.loc[\"season\"] = np.nan\n",
    "                create_new.loc[\"trend\"] = np.nan\n",
    "                tmp = pd.concat([create_new.T]*len(diger_s), ignore_index=True)\n",
    "                for k in range(len(diger_s)):\n",
    "                    tmp.loc[k,'indirim__bins']=diger_s[k][0]\n",
    "                    tmp.loc[k,'aktivite_tipi']=diger_s[k][1]\n",
    "                df_all__.append(tmp)\n",
    "    df_all__ = pd.concat(df_all__, ignore_index=True)\n",
    "    return df_all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = create_comb(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = all_combinations[['new_adet', 'yil', 'ay', 'date', 'Kanal', 'grup_adi', 'ana_kategori_adi', \n",
    "                                     'kategori_adi', 'marka_adi', 'urun_adi', 'enflasyon_etkisi',  'peak', 'indirim__bins', \n",
    "                                     'aktivite_tipi', 'lockdown', 'season', 'trend', 'scope', 'scope_type', 'portfoy']]\n",
    "\n",
    "all_combinations[\"enflasyon_etkisi\"] = all_combinations[\"date\"].map(enf_dict_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021-10-01    41288\n",
       "2022-01-01    41288\n",
       "2021-11-01    41288\n",
       "2021-12-01    41288\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_combinations.date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodun çalışma süresi: 0:05:01.617468\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combination_df(df_):\n",
    "    \"\"\" \n",
    "    Tüm kombinasyonlar için bir dictionary içerisinde 91 tane dataframe oluşturuyor. Input olarak aldığı df'in sonuna 3 satır \n",
    "    ekleyip indirim__bins ve aktivite_tipi'ni df_indirimbins_aktivitetipi olacak şekilde giriyor. new_adet değişkenine son 3 satır için NA atıyor.  \n",
    "    \"\"\"\n",
    "\n",
    "    indirim_bins_list = ['minus_one','zero','one','two','three','four','five','six','seven','eight','nine','ten','eleven']\n",
    "    aktivite_tipi_list = ['zero','two','five','eight','eleven','fourteen','seventeen']\n",
    "    df_names_dict = list(itertools.product(indirim_bins_list, aktivite_tipi_list))\n",
    "\n",
    "    ind_akt = list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0, 2, 5, 8, 11, 14, 17]))\n",
    "\n",
    "    df_names = []\n",
    "    for idx in df_names_dict:\n",
    "        df_names.append('df_' + idx[0] + '_' + idx[1])\n",
    "\n",
    "    df_dict = {}\n",
    "    for i in range(len(df_names)):\n",
    "        main_data = data.copy()\n",
    "        tmp = df_[(df_[\"aktivite_tipi\"] == ind_akt[i][1]) & \n",
    "                  (df_[\"indirim__bins\"] == ind_akt[i][0])]\n",
    "\n",
    "        new_comb_with_main_data = pd.concat([main_data, tmp], ignore_index=True)\n",
    "        new_comb_with_main_data.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], inplace=True, ignore_index=True)\n",
    "        df_dict[df_names[i]] = new_comb_with_main_data.copy()\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combs_df = create_combination_df(all_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodun çalışma süresi: 0:00:23.529227\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWAT connections was never created or you lost the connection. Trying to create connection...\n",
      "Accessed!\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_MINUS_ONE_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_MINUS_ONE_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_MINUS_ONE_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_MINUS_ONE_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_MINUS_ONE_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_MINUS_ONE_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MINUS_ONE_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_MINUS_ONE_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_ZERO_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_ZERO_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_ZERO_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_ZERO_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_ZERO_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_ZERO_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ZERO_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_ZERO_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_ONE_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_ONE_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_ONE_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_ONE_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_ONE_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_ONE_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ONE_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_ONE_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_TWO_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_TWO_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_TWO_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_TWO_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_TWO_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_TWO_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TWO_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_TWO_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_THREE_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_THREE_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_THREE_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_THREE_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_THREE_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_THREE_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_THREE_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_THREE_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_FOUR_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_FOUR_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_FOUR_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_FOUR_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_FOUR_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_FOUR_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FOUR_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_FOUR_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_FIVE_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_FIVE_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_FIVE_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_FIVE_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_FIVE_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_FIVE_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_FIVE_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_FIVE_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_SIX_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_SIX_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_SIX_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_SIX_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_SIX_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_SIX_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SIX_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_SIX_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_SEVEN_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_SEVEN_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_SEVEN_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_SEVEN_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_SEVEN_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_SEVEN_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_SEVEN_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_SEVEN_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_EIGHT_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_EIGHT_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_EIGHT_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_EIGHT_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_EIGHT_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_EIGHT_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_EIGHT_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_EIGHT_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_NINE_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_NINE_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_NINE_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_NINE_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_NINE_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_NINE_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_NINE_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_NINE_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_TEN_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_TEN_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_TEN_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_TEN_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_TEN_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_TEN_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_TEN_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_TEN_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_ZERO in caslib DSENS_T.\n",
      "NOTE: The table DF_ELEVEN_ZERO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_TWO in caslib DSENS_T.\n",
      "NOTE: The table DF_ELEVEN_TWO has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_FIVE in caslib DSENS_T.\n",
      "NOTE: The table DF_ELEVEN_FIVE has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_EIGHT in caslib DSENS_T.\n",
      "NOTE: The table DF_ELEVEN_EIGHT has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_ELEVEN in caslib DSENS_T.\n",
      "NOTE: The table DF_ELEVEN_ELEVEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_FOURTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_ELEVEN_FOURTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_ELEVEN_SEVENTEEN in caslib DSENS_T.\n",
      "NOTE: The table DF_ELEVEN_SEVENTEEN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n"
     ]
    }
   ],
   "source": [
    "data_list = list(all_combs_df.keys())\n",
    "\n",
    "for dfs in data_list:\n",
    "    tmp = all_combs_df[dfs]\n",
    "    tmp = tmp[tmp[\"scope_type\"] == \"ts\"]\n",
    "    tmp = tmp[[\"date\", \"new_adet\", \"ana_kategori_adi\", \"grup_adi\", \"Kanal\", \"kategori_adi\", \"marka_adi\", \"urun_adi\", \"aktivite_tipi\",\n",
    "               \"enflasyon_etkisi\", \"indirim__bins\", \"lockdown\", \"peak\", \"season\", \"trend\"]]\n",
    "    tmp[\"enflasyon_etkisi\"] = tmp[\"date\"].map(enf_dict_fill)\n",
    "    tmp[\"new_adet\"] = tmp[\"new_adet\"].astype(\"float\")\n",
    "    tmp[\"aktivite_tipi\"] = tmp[\"aktivite_tipi\"].astype(\"float\")\n",
    "    tmp[\"indirim__bins\"] = tmp[\"indirim__bins\"].astype(\"float\")\n",
    "    tmp[\"lockdown\"] = tmp[\"lockdown\"].astype(\"float\")\n",
    "    tmp[\"peak\"] = tmp[\"peak\"].astype(\"float\")\n",
    "    tmp[\"date\"] = tmp[\"date\"].apply(lambda x: (x - datetime(1960, 1, 1)).days)\n",
    "    try: \n",
    "        conn.userinfo()\n",
    "    except: \n",
    "        print(\"SWAT connections was never created or you lost the connection. Trying to create connection...\")\n",
    "        access_ = True\n",
    "        while access_:\n",
    "            try:\n",
    "                conn = swat.CAS('yhtrcl-sasccnt1.yildiz.domain', 5570, username=params_[\"login_info\"][\"username\"], password=params_[\"login_info\"][\"password\"])\n",
    "                access_ = False\n",
    "                print(\"Accessed!\")\n",
    "            except:\n",
    "                print(\"Got error. Trying to reconnect...\")\n",
    "                continue\n",
    "    ds_table = conn.CASTable(dfs, caslib=params_[\"caslib_info\"][\"caslib_name\"], replace=True)\n",
    "    ds_table.table.dropTable(quiet=True)\n",
    "    conn.upload(data=tmp, casout={'caslib':params_[\"caslib_info\"][\"caslib_name\"], 'name':dfs, 'promote':True}, \n",
    "                importoptions={'vars':{'date':{'format': 'MMDDYY', \"type\": \"double\", \"length\": 10}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_list = [i.replace(\"df_\", \"ts_results_\") for i in data_list]\n",
    "input_names = pd.DataFrame({\"tables\": data_list})\n",
    "output_names = pd.DataFrame({\"tables\": output_data_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table DS_COMB_INPUT_TABLE_NAMES in caslib DSENS_T.\n",
      "NOTE: The table DS_COMB_INPUT_TABLE_NAMES has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DS_COMB_OUTPUT_TABLE_NAMES in caslib DSENS_T.\n",
      "NOTE: The table DS_COMB_OUTPUT_TABLE_NAMES has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>DSENS_T</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>DS_COMB_OUTPUT_TABLE_NAMES</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('DS_COMB_OUTPUT_TABLE_NAMES', caslib='DSENS_T')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.111s</span> &#183; <span class=\"cas-user\">user 0.0186s</span> &#183; <span class=\"cas-sys\">sys 0.0285s</span> &#183; <span class=\"cas-memory\">mem 33.9MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'DSENS_T'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'DS_COMB_OUTPUT_TABLE_NAMES'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('DS_COMB_OUTPUT_TABLE_NAMES', caslib='DSENS_T')\n",
       "\n",
       "+ Elapsed: 0.111s, user: 0.0186s, sys: 0.0285s, mem: 33.9mb"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_table = conn.CASTable(\"DS_COMB_INPUT_TABLE_NAMES\", caslib=params_[\"caslib_info\"][\"caslib_name\"], replace=True)\n",
    "ds_table.table.dropTable(quiet=True)\n",
    "\n",
    "ds_table = conn.CASTable(\"DS_COMB_OUTPUT_TABLE_NAMES\", caslib=params_[\"caslib_info\"][\"caslib_name\"], replace=True)\n",
    "ds_table.table.dropTable(quiet=True)\n",
    "\n",
    "conn.upload(data=input_names, casout={'caslib':params_[\"caslib_info\"][\"caslib_name\"], 'name':'DS_COMB_INPUT_TABLE_NAMES', 'promote':True})\n",
    "conn.upload(data=output_names, casout={'caslib':params_[\"caslib_info\"][\"caslib_name\"], 'name':'DS_COMB_OUTPUT_TABLE_NAMES', 'promote':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodun çalışma süresi: 0:30:02.183990\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kodun çalışma süresi: 1:42:47.138546\n"
     ]
    }
   ],
   "source": [
    "end_all_process = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_all_process - start_all_process))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# MODELLEME KISMI\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    return np.mean(np.abs((np.array(y_true) - np.array(y_pred)) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.copy()\n",
    "tmp = tmp[(tmp[\"scope_type\"] == \"ts\") & (tmp[\"portfoy\"] == 1)]\n",
    "tmp = tmp[[\"date\", \"new_adet\", \"ana_kategori_adi\", \"grup_adi\", \"Kanal\", \"kategori_adi\", \"marka_adi\", \"urun_adi\", \"aktivite_tipi\",\n",
    "           \"enflasyon_etkisi\", \"indirim__bins\", \"lockdown\", \"peak\", \"season\", \"trend\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Length was specified for column 'date', but the column is a varchar or fixed numeric type.  The length will be ignored.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table DF_MAIN in caslib DSENS_T.\n",
      "NOTE: The table DF_MAIN has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>DSENS_T</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>DF_MAIN</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('DF_MAIN', caslib='DSENS_T')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 17s</span> &#183; <span class=\"cas-user\">user 0.351s</span> &#183; <span class=\"cas-sys\">sys 0.187s</span> &#183; <span class=\"cas-memory\">mem 52.2MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'DSENS_T'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'DF_MAIN'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('DF_MAIN', caslib='DSENS_T')\n",
       "\n",
       "+ Elapsed: 17s, user: 0.351s, sys: 0.187s, mem: 52.2mb"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df.copy()\n",
    "tmp = tmp[(tmp[\"scope_type\"] == \"ts\") & (tmp[\"portfoy\"] == 1)]\n",
    "tmp = tmp[[\"date\", \"new_adet\", \"ana_kategori_adi\", \"grup_adi\", \"Kanal\", \"kategori_adi\", \"marka_adi\", \"urun_adi\", \"aktivite_tipi\",\n",
    "           \"enflasyon_etkisi\", \"indirim__bins\", \"lockdown\", \"peak\", \"season\", \"trend\"]]\n",
    "tmp[\"new_adet\"] = tmp[\"new_adet\"].astype(\"float\")\n",
    "tmp[\"aktivite_tipi\"] = tmp[\"aktivite_tipi\"].astype(\"float\")\n",
    "tmp[\"indirim__bins\"] = tmp[\"indirim__bins\"].astype(\"float\")\n",
    "tmp[\"lockdown\"] = tmp[\"lockdown\"].astype(\"float\")\n",
    "tmp[\"peak\"] = tmp[\"peak\"].astype(\"float\")\n",
    "tmp[\"date\"] = pd.to_datetime(tmp[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "tmp[\"date\"] = tmp[\"date\"].apply(lambda x: (x - datetime(1960, 1, 1)).days)\n",
    "\n",
    "ds_table = conn.CASTable(\"DF_MAIN\", caslib=params_[\"caslib_info\"][\"caslib_name\"], replace=True)\n",
    "ds_table.table.dropTable(quiet=True)\n",
    "\n",
    "conn.upload(data=tmp, casout={'caslib':params_[\"caslib_info\"][\"caslib_name\"], 'name':\"DF_MAIN\", 'promote':True}, \n",
    "            importoptions={'vars':{'date':{'format': 'MMDDYY', \"type\": \"double\", \"length\": 10}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"python ../pyviyatools/submit_jobdef.py -id {}\".format(params_[\"sas_job\"][\"ts_job_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAS TS script triger süresi: 0:03:51.676848\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('SAS TS script triger süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirim_bins_list = ['minus_one','zero','one','two','three','four','five','six','seven','eight','nine','ten','eleven']\n",
    "aktivite_tipi_list = ['zero','two','five','eight','eleven','fourteen','seventeen']\n",
    "df_names_dict = list(itertools.product(indirim_bins_list, aktivite_tipi_list))\n",
    "\n",
    "ind_akt = list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0, 2, 5, 8, 11, 14, 17]))\n",
    "\n",
    "df_names = []\n",
    "for idx in df_names_dict:\n",
    "    df_names.append('df_' + idx[0] + '_' + idx[1])\n",
    "\n",
    "# ts kombinasyonlarını dataframe'de tutabilmek için bunu yapıyoruz.\n",
    "df_ts_comb_names = [i.replace(\"df\", \"ts_results\") for i in df_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_comb_results = []\n",
    "for dfs in df_ts_comb_names:\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "    month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "    year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "    if month_ == 12:\n",
    "        month_ = 1\n",
    "        year+=1\n",
    "    else:\n",
    "        month_+=1\n",
    "\n",
    "    date_greater = datetime(year_, month_, 1) - relativedelta(months=1)\n",
    "    date_less = datetime(year_, month_, 1) + relativedelta(months=4)\n",
    "    try: \n",
    "        conn.userinfo()\n",
    "    except: \n",
    "        print(\"SWAT bağlantısı koptu. Bağlantı tekrar kuruluyor.\")\n",
    "        access_ = True\n",
    "        while access_:\n",
    "            try:\n",
    "                conn = swat.CAS('yhtrcl-sasccnt1.yildiz.domain', 5570, username=params_[\"login_info\"][\"username\"], password=params_[\"login_info\"][\"password\"])\n",
    "                access_ = False\n",
    "                print(\"Accessed!\")\n",
    "            except:\n",
    "                print(\"Got error. Trying to reconnect...\")\n",
    "                continue\n",
    "    outfor = pd.DataFrame(conn.CASTable(caslib=params_[\"caslib_info\"][\"caslib_name\"],name=dfs).to_frame())\n",
    "    outfor = outfor[[\"Kanal\", \"grup_adi\", \"urun_adi\", \"date\", \"ACTUAL\", \"PREDICT\"]]\n",
    "    outfor[\"date\"] = pd.to_datetime(outfor[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    outfor = outfor[(outfor[\"date\"] > date_greater) & (outfor[\"date\"] < date_less)]\n",
    "    outfor[\"data_from\"] = dfs\n",
    "    ts_comb_results.append(outfor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_comb_results = pd.concat(ts_comb_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    conn.userinfo()\n",
    "except: \n",
    "    print(\"SWAT bağlantısı koptu. Bağlantı tekrar kuruluyor.\")\n",
    "    access_ = True\n",
    "    while access_:\n",
    "        try:\n",
    "            conn = swat.CAS('yhtrcl-sasccnt1.yildiz.domain', 5570, username=params_[\"login_info\"][\"username\"], password=params_[\"login_info\"][\"password\"])\n",
    "            access_ = False\n",
    "            print(\"Accessed!\")\n",
    "        except:\n",
    "            print(\"Got error. Trying to reconnect...\")\n",
    "            continue\n",
    "df_ts = pd.DataFrame(conn.CASTable(caslib=params_[\"caslib_info\"][\"caslib_name\"],name=\"TS_RESULTS_MAIN\").to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = df_ts[[\"Kanal\", \"grup_adi\", \"urun_adi\", \"date\", \"ACTUAL\", \"PREDICT\"]]\n",
    "df_ts[\"date\"] = pd.to_datetime(df_ts[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "if month_ == 12:\n",
    "    month_ = 1\n",
    "    year+=1\n",
    "else:\n",
    "    month_+=1\n",
    "\n",
    "ytrue_all = []\n",
    "yhat_all = []\n",
    "sku_all = []\n",
    "grp_all = []\n",
    "date_all = []\n",
    "mape_ts = []\n",
    "sku_ts = []\n",
    "grup_adi_ts = []\n",
    "for sku in df_ts.urun_adi.unique():\n",
    "    for grp in df_ts.grup_adi.unique():\n",
    "        temp_df = df_ts[(df_ts[\"grup_adi\"] == grp) & (df_ts[\"urun_adi\"] == sku)]\n",
    "        if len(temp_df) < 1:\n",
    "            pass\n",
    "        else:\n",
    "            if month_ < 7:\n",
    "                temp_df = temp_df[(temp_df[\"date\"] > datetime(year_-1, month_+6, 1)) & (temp_df[\"date\"] < datetime(year_, month_, 1)) & (temp_df[\"ACTUAL\"] != 0)]\n",
    "                ytrue = temp_df.iloc[-6:, -2:-1]\n",
    "                yhat = temp_df.iloc[-6:, -1:]\n",
    "                date_ = temp_df.iloc[-6:, -3:-2]\n",
    "                mape_ts.append(mape(np.array(ytrue), np.array(yhat)))\n",
    "                sku_ts.append(sku)\n",
    "                grup_adi_ts.append(grp)\n",
    "\n",
    "                ytrue_all.extend(list(ytrue.ACTUAL))\n",
    "                yhat_all.extend(list(yhat.PREDICT))\n",
    "                sku_all.extend([sku]*len(ytrue))\n",
    "                grp_all.extend([grp]*len(ytrue))\n",
    "                date_all.extend(list(date_.date))\n",
    "            else:\n",
    "                temp_df = temp_df[(temp_df[\"date\"] > datetime(year_, month_-6, 1)) & (temp_df[\"date\"] < datetime(year_, month_, 1)) & (temp_df[\"ACTUAL\"] != 0)]\n",
    "                ytrue = temp_df.iloc[-6:, -2:-1]\n",
    "                yhat = temp_df.iloc[-6:, -1:]\n",
    "                date_ = temp_df.iloc[-6:, -3:-2]\n",
    "                mape_ts.append(mape(np.array(ytrue), np.array(yhat)))\n",
    "                sku_ts.append(sku)\n",
    "                grup_adi_ts.append(grp)\n",
    "\n",
    "                ytrue_all.extend(list(ytrue.ACTUAL))\n",
    "                yhat_all.extend(list(yhat.PREDICT))\n",
    "                sku_all.extend([sku]*len(ytrue))\n",
    "                grp_all.extend([grp]*len(ytrue))\n",
    "                date_all.extend(list(date_.date))\n",
    "ts_results =  pd.DataFrame({\"grup\": grup_adi_ts,\n",
    "                            \"urun_adi\": sku_ts,\n",
    "                            \"mape\": mape_ts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_results[\"mape_bins\"] = ts_results[\"mape\"].apply(lambda x: \"10'dan küçük\" \\\n",
    "                                                   if x<10 else (\"10 ile 20 arasında\" \\\n",
    "                                                                 if x<20 else (\"20 ile 30 arasında\" \\\n",
    "                                                                               if x<30 else(\"30 ile 50 arasında\" \\\n",
    "                                                                                            if x<50 else(\"50 ile 100 arasında\" \\\n",
    "                                                                                                         if x<100 else \"100'den büyük\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_dict = {'Diğer_Pasifik': 'pasifik', \n",
    "              'MİGROS': 'pasifik', \n",
    "              'ŞOK': 'pasifik', \n",
    "              'A101': 'pasifik', \n",
    "              'BİM': 'pasifik', \n",
    "              'Diğer_Horizon': 'horizon',\n",
    "              'GELENEKSEL KANAL': 'horizon', \n",
    "              'ORTA MARKET': 'horizon', \n",
    "              'POTANSİYEL MARKET': 'horizon',\n",
    "              'YEREL ZİNCİR': 'horizon',\n",
    "              'BTT': 'btt'}\n",
    "\n",
    "ts_results[\"kanal_l\"] = ts_results[\"grup\"].map(kanal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_results_raw_data = pd.DataFrame({\"grup\": grp_all,\n",
    "                                    \"urun_adi\": sku_all,\n",
    "                                    \"date\": date_all,\n",
    "                                    \"ytrue\": ytrue_all,\n",
    "                                    \"yhat\": yhat_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_results_raw_data = ts_results_raw_data.merge(ts_results[[\"kanal_l\", \"grup\"]].drop_duplicates(subset=[\"kanal_l\", \"grup\"]), how=\"left\", on=\"grup\")\n",
    "ts_list_to_reg = ts_results[ts_results[\"mape\"] >= 30][[\"grup\", \"urun_adi\"]].drop_duplicates(subset=[\"grup\", \"urun_adi\"], ignore_index=True, keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regresyona girecek ürünlerin listesini burada tutuyoruz\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date, format='%Y-%m-%d', errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = df[(df[\"scope\"] == 3) & (df[\"portfoy\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cols_to_drop = ['en_guncel_kod', 'koli_i̇ci_adet', 'koli',\n",
    "                    'kg', 'tl', 'adet', 'satis_var', 'promosyon_tutari',\n",
    "                    'ciro_kull_i̇ade_dus', 'weekdays_ratio', 'weekend_ratio',\n",
    "                    'total_holiday_ratio', 'school_day_ratio', 'school_day_brdg_n', 'school_day_brdg_ratio',\n",
    "                    'ramadan_ratio', 'gozlem_sayisi', 'actual_holiday_ratio',\n",
    "                    'toplam_satir', 'oran', 'baslangic_tarih', 'bitis_tarih',\n",
    "                    'son_kac_ay_eksik', 'eksik_repeat_sayisi', 'adet_flag', \n",
    "                    'scope_type', 'durum']\n",
    "    df_droped = df[cols_to_drop]\n",
    "except:\n",
    "    cols_to_drop = ['en_guncel_kod', 'koli_i̇ci_adet', 'koli',\n",
    "                    'kg', 'tl', 'adet', 'satis_var', 'promosyon_tutari',\n",
    "                    'ciro_kull_i̇ade_dus', 'weekdays_ratio', 'weekend_ratio',\n",
    "                    'total_holiday_ratio', 'school_day_ratio', 'school_day_brdg_n', 'school_day_brdg_ratio',\n",
    "                    'ramadan_ratio', 'gozlem_sayisi', 'actual_holiday_ratio',\n",
    "                    'toplam_satir', 'oran', 'baslangic_tarih', 'bitis_tarih',\n",
    "                    'son_kac_ay_eksik', 'eksik_repeat_sayisi', 'scope_type', 'durum']\n",
    "    df_droped = df[cols_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=cols_to_drop, axis=1, inplace=True)\n",
    "df.rename(columns={\"Kanal\": \"kanal\"}, inplace=True)\n",
    "df_reg = df_reg[df_reg[\"portfoy\"] == 1]\n",
    "df_reg.drop(columns=[\"portfoy\", \"scope\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vars = ['yil', 'ay', 'date', 'Kanal', 'grup_adi', 'ana_kategori_adi', \n",
    "          'kategori_adi', 'marka_adi', 'urun_adi', 'enflasyon_etkisi', \n",
    "          'peak', 'indirim__bins', 'aktivite_tipi', 'lockdown', 'season', 'trend']\n",
    "\n",
    "y_vars = [\"new_adet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = df_reg[X_vars+y_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"enflasyon_etkisi\"].fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_to_reg_df = []\n",
    "for row in range(len(ts_list_to_reg)):\n",
    "    tmp = df[(df[\"urun_adi\"] == ts_list_to_reg[\"urun_adi\"][row]) & (df[\"grup_adi\"] == ts_list_to_reg[\"grup\"][row])]\n",
    "    ts_to_reg_df.append(tmp)\n",
    "ts_to_reg_df = pd.concat(ts_to_reg_df)\n",
    "ts_to_reg_df.reset_index(drop=True, inplace=True)\n",
    "ts_to_reg_df.drop_duplicates(subset=ts_to_reg_df.columns.to_list(), ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_to_reg_df.rename(columns={\"kanal\": \"Kanal\"}, inplace=True)\n",
    "ts_to_reg_df = ts_to_reg_df[df_reg.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.concat([df_reg, ts_to_reg_df], axis=0, ignore_index=True)\n",
    "df_reg[\"enflasyon_etkisi\"].fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_kanal = LabelEncoder()\n",
    "le_grup_adi = LabelEncoder()\n",
    "le_ana_kategori_adi = LabelEncoder()\n",
    "le_kategori_adi = LabelEncoder()\n",
    "le_marka_adi = LabelEncoder()\n",
    "le_urun_adi = LabelEncoder()\n",
    "\n",
    "df_reg[\"Kanal\"] = le_kanal.fit_transform(df_reg[\"Kanal\"])\n",
    "df_reg[\"grup_adi\"] = le_grup_adi.fit_transform(df_reg[\"grup_adi\"])\n",
    "df_reg[\"ana_kategori_adi\"] = le_ana_kategori_adi.fit_transform(df_reg[\"ana_kategori_adi\"])\n",
    "df_reg[\"kategori_adi\"] = le_kategori_adi.fit_transform(df_reg[\"kategori_adi\"])\n",
    "df_reg[\"marka_adi\"] = le_marka_adi.fit_transform(df_reg[\"marka_adi\"])\n",
    "df_reg[\"urun_adi\"] = le_urun_adi.fit_transform(df_reg[\"urun_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_seasonality_decomp(data):\n",
    "    df_all = []\n",
    "    for sku in data[\"urun_adi\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "            temp_df = data[(data[\"urun_adi\"] == sku) & \n",
    "                           (data[\"grup_adi\"] == grup)]\n",
    "\n",
    "            if len(temp_df) > 2:\n",
    "#                print(sku, grup)\n",
    "                df_ts = temp_df[['new_adet','date']]\n",
    "                df_ts.set_index('date',inplace=True)\n",
    "\n",
    "                result = STL(df_ts).fit()\n",
    "                temp_df['season'] = list(result.seasonal)\n",
    "                temp_df['trend']  = list(result.trend)\n",
    "                df_all.append(temp_df)\n",
    "            else:\n",
    "                pass\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = trend_seasonality_decomp(df_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Son 3 aya seasonality ve trend eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_list = df_reg.drop_duplicates(subset=[\"urun_adi\", \"grup_adi\"], ignore_index=True)[[\"urun_adi\", \"grup_adi\"]]\n",
    "\n",
    "df_all = []\n",
    "for idx in loop_list.index:\n",
    "    test = df_reg[(df_reg[\"urun_adi\"] == loop_list[\"urun_adi\"][idx]) & (df_reg[\"grup_adi\"] == loop_list[\"grup_adi\"][idx])]\n",
    "    trend_idx = test.iloc[-7:-3, :].index\n",
    "    calc_df = test.loc[trend_idx, [\"date\", \"trend\"]]\n",
    "    last_value = calc_df.iloc[-1, 1:].values[0]\n",
    "    first_value = calc_df.iloc[0, 1:].values[0]\n",
    "    add_value = (last_value - first_value)  / 4\n",
    "\n",
    "    for index_ in test.iloc[-3:].index:\n",
    "        if len(test) >= 27:\n",
    "            test.loc[index_, \"season\"] = test.loc[index_-24, \"season\"]\n",
    "        elif len(test) >= 15 and len(test) < 27:\n",
    "            test.loc[index_, \"season\"] = test.loc[index_-12, \"season\"]\n",
    "        else:\n",
    "            test.loc[index_, \"season\"] = test.loc[list(range(index_-1, index_-7, -1)), \"season\"].mean()\n",
    "        test.loc[index_, \"trend\"] = test.loc[index_-1, \"trend\"] + add_value\n",
    "        df_all.append(test)\n",
    "df_all = pd.concat(df_all)\n",
    "df_all.drop_duplicates(subset=df_all.columns.to_list(), ignore_index=True, inplace=True)\n",
    "\n",
    "df_reg = df_all.copy()\n",
    "df_reg.drop(columns=\"date\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for sku in df_reg.urun_adi.unique():\n",
    "    for grp in df_reg.grup_adi.unique():\n",
    "        temp_df = df_reg[(df_reg[\"urun_adi\"] == sku) & (df_reg[\"grup_adi\"] == grp)]\n",
    "        temp_df.sort_values(by=[\"yil\", \"ay\"], inplace=True)\n",
    "        X_train.append(temp_df.iloc[:-3, :-1])\n",
    "        X_test.append(temp_df.iloc[-3:, :-1])\n",
    "        y_train.append(temp_df.iloc[:-3, -1:])\n",
    "        y_test.append(temp_df.iloc[-3:, -1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pd.concat(X_train), pd.concat(X_test), pd.concat(y_train), pd.concat(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15511, 15), (1071, 15), (15511, 1), (1071, 1))"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regresyon\n",
    "---\n",
    "# Linear Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "yhat_lm = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 124342.06986420412\n"
     ]
    }
   ],
   "source": [
    "print(\"MAPE:\", mape(np.array(y_test), yhat_lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# KNN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(metric='euclidean', n_neighbors=25)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm_knn = KNeighborsRegressor(algorithm=params_[\"gridsearch_params\"][\"knn_params\"][\"algorithm\"], \n",
    "                              leaf_size=params_[\"gridsearch_params\"][\"knn_params\"][\"leaf_size\"], \n",
    "                              metric=params_[\"gridsearch_params\"][\"knn_params\"][\"metric\"], \n",
    "                              n_neighbors=params_[\"gridsearch_params\"][\"knn_params\"][\"n_neighbors\"], \n",
    "                              weights=params_[\"gridsearch_params\"][\"knn_params\"][\"weights\"])\n",
    "gsm_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: new_adet    100864.628925\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "best_params_knn = gsm_knn.get_params()\n",
    "yhat_knn = gsm_knn.predict(X_test)\n",
    "print(\"MAPE:\", mape(y_test, yhat_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# XGBoost\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:06:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=None,\n",
       "             colsample_bynode=None, colsample_bytree=0.9, gamma=None, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints=None,\n",
       "             learning_rate=0.1, max_delta_step=None, max_depth=7,\n",
       "             min_child_weight=10, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=8, num_parallel_tree=None,\n",
       "             random_state=42, reg_alpha=0, reg_lambda=0, scale_pos_weight=1,\n",
       "             subsample=0.5, tree_method=None, validate_parameters=1,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm_xgb = XGBRegressor(booster=params_[\"gridsearch_params\"][\"xgboost_params\"][\"booster\"], \n",
    "                       colsample_bytree=params_[\"gridsearch_params\"][\"xgboost_params\"][\"colsample_bytree\"], \n",
    "                       learning_rate=params_[\"gridsearch_params\"][\"xgboost_params\"][\"learning_rate\"], \n",
    "                       max_depth=params_[\"gridsearch_params\"][\"xgboost_params\"][\"max_depth\"], \n",
    "                       min_child_weight=params_[\"gridsearch_params\"][\"xgboost_params\"][\"min_child_weight\"], \n",
    "                       objective=params_[\"gridsearch_params\"][\"xgboost_params\"][\"objective\"], \n",
    "                       random_state=params_[\"gridsearch_params\"][\"xgboost_params\"][\"random_state\"], \n",
    "                       subsample=params_[\"gridsearch_params\"][\"xgboost_params\"][\"subsample\"])\n",
    "\n",
    "gsm_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 744875.074324917\n"
     ]
    }
   ],
   "source": [
    "best_params_xgb = gsm_xgb.get_params()\n",
    "yhat_xgb = gsm_xgb.predict(X_test)\n",
    "print(\"MAPE:\", mape(np.array(y_test), yhat_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Decision Tree\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mae', max_depth=10, min_samples_leaf=8,\n",
       "                      min_samples_split=3, random_state=42)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm_dt = DecisionTreeRegressor(criterion=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"criterion\"], \n",
    "                               max_depth=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"max_depth\"], \n",
    "                               min_samples_leaf=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"min_samples_leaf\"], \n",
    "                               min_samples_split=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"min_samples_split\"], \n",
    "                               random_state=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"random_state\"])\n",
    "\n",
    "gsm_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 710303.7953382271\n"
     ]
    }
   ],
   "source": [
    "best_params_dt = gsm_dt.get_params()\n",
    "yhat_dt = gsm_dt.predict(X_test)\n",
    "print(\"MAPE:\", mape(np.array(y_test), yhat_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Random Forest Regressor\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(criterion='mae', max_depth=8, min_samples_leaf=5,\n",
       "                      min_samples_split=5, random_state=42)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm_rf = RandomForestRegressor(criterion=params_[\"gridsearch_params\"][\"randomforest_params\"][\"criterion\"], \n",
    "                               max_depth=params_[\"gridsearch_params\"][\"randomforest_params\"][\"max_depth\"], \n",
    "                               min_samples_leaf=params_[\"gridsearch_params\"][\"randomforest_params\"][\"min_samples_leaf\"], \n",
    "                               min_samples_split=params_[\"gridsearch_params\"][\"randomforest_params\"][\"min_samples_split\"], \n",
    "                               random_state=params_[\"gridsearch_params\"][\"randomforest_params\"][\"random_state\"])\n",
    "\n",
    "gsm_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 709052.5591357916\n"
     ]
    }
   ],
   "source": [
    "best_params_rf = gsm_rf.get_params()\n",
    "yhat_rf = gsm_rf.predict(X_test)\n",
    "print(\"MAPE:\", mape(np.array(y_test), yhat_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient Boosting Regressor\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(criterion='mae', learning_rate=0.2, max_depth=5,\n",
       "                          max_features='log2', min_samples_leaf=0.1,\n",
       "                          min_samples_split=0.1, n_estimators=10,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsm_gb = GradientBoostingRegressor(criterion=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"criterion\"], \n",
    "                                   learning_rate=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"learning_rate\"], \n",
    "                                   max_depth=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"max_depth\"], \n",
    "                                   max_features=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"max_features\"], \n",
    "                                   min_samples_leaf=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"min_samples_leaf\"], \n",
    "                                   min_samples_split=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"min_samples_split\"], \n",
    "                                   n_estimators=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"n_estimators\"], \n",
    "                                   random_state=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"random_state\"], \n",
    "                                   subsample=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"subsample\"])\n",
    "gsm_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 495711.26511725405\n"
     ]
    }
   ],
   "source": [
    "best_params_gb = gsm_gb.get_params()\n",
    "yhat_gb = gsm_gb.predict(X_test)\n",
    "print(\"MAPE:\", mape(np.array(y_test), yhat_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regresyon test sonuçlarının çıktılarının alınması\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[\"Kanal\"] = le_kanal.inverse_transform(test_results[\"Kanal\"])\n",
    "test_results[\"grup_adi\"] = le_grup_adi.inverse_transform(test_results[\"grup_adi\"])\n",
    "test_results[\"ana_kategori_adi\"] = le_ana_kategori_adi.inverse_transform(test_results[\"ana_kategori_adi\"])\n",
    "test_results[\"kategori_adi\"] = le_kategori_adi.inverse_transform(test_results[\"kategori_adi\"])\n",
    "test_results[\"marka_adi\"] = le_marka_adi.inverse_transform(test_results[\"marka_adi\"])\n",
    "test_results[\"urun_adi\"] = le_urun_adi.inverse_transform(test_results[\"urun_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[\"linear_regression\"] = yhat_lm.reshape(-1)\n",
    "test_results[\"knn\"] = yhat_knn.reshape(-1)\n",
    "test_results[\"xgboost\"] = yhat_xgb.reshape(-1)\n",
    "test_results[\"decision_tree\"] = yhat_dt.reshape(-1)\n",
    "test_results[\"random_forest\"] = yhat_rf.reshape(-1)\n",
    "test_results[\"gradient_boosting\"] = yhat_gb.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.concat([test_results, y_test], axis=1)\n",
    "#test_results.to_excel(\"../results/36_12_siparis/scores/regression_all_test_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_l = []\n",
    "grup = []\n",
    "urun_l = []\n",
    "model_type = []\n",
    "mape_l = []\n",
    "\n",
    "for kanal in test_results.Kanal.unique():\n",
    "    for grp in test_results.grup_adi.unique():\n",
    "        for urun in test_results.urun_adi.unique():\n",
    "            for model in test_results.columns.to_list()[-7:-1]:\n",
    "                temp_df = test_results[(test_results[\"Kanal\"] == kanal) & \n",
    "                                       (test_results[\"grup_adi\"] == grp) & \n",
    "                                       (test_results[\"urun_adi\"] == urun)]\n",
    "                kanal_l.append(kanal)\n",
    "                grup.append(grp)\n",
    "                urun_l.append(urun)\n",
    "                model_type.append(model)\n",
    "                model_com = temp_df[[model, \"new_adet\"]]\n",
    "                mape_l.append(mape(np.array(model_com[\"new_adet\"]), np.array(model_com[model])))\n",
    "\n",
    "\n",
    "results_by_sku = pd.DataFrame({\"kanal_l\": kanal_l,\n",
    "                               \"grup\": grup,\n",
    "                               \"urun\": urun_l,\n",
    "                               \"model_type\": model_type,\n",
    "                               \"mape\": mape_l})\n",
    "\n",
    "results_by_sku.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = pd.DataFrame({\"models\": [\"KNN\", \"Gradient Boosting\", \"XGBoost\", \"Decision Tree\", \"Random Forest\"],\n",
    "                           \"parameters\": [best_params_knn, best_params_gb, best_params_xgb, best_params_dt, best_params_rf]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params.set_index(\"models\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    logging.basicConfig(filename='../log/gridsearchparameters.log', encoding='utf-8', level=logging.DEBUG)\n",
    "    logging.debug('{}'.format(all_params.to_dict()))\n",
    "except:\n",
    "    logging.basicConfig(filename='../log/gridsearchparameters.log', encoding='utf-8', level=logging.DEBUG)\n",
    "    logging.debug('{}'.format(all_params.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datetime.now()\n",
    "all_params.to_excel(\"../log/gridsearchparameters_{}_{}_{}_{}_{}.xlsx\".format(d.year, d.month, d.day, d.minute, d.second), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# MAPE'si en düşük modeli seçtirme\n",
    "---\n",
    "# ORTALAMA\n",
    "## Regresyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg2 = df_reg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg2[\"Kanal\"] = le_kanal.inverse_transform(df_reg2[\"Kanal\"])\n",
    "df_reg2[\"grup_adi\"] = le_grup_adi.inverse_transform(df_reg2[\"grup_adi\"])\n",
    "df_reg2[\"ana_kategori_adi\"] = le_ana_kategori_adi.inverse_transform(df_reg2[\"ana_kategori_adi\"])\n",
    "df_reg2[\"kategori_adi\"] = le_kategori_adi.inverse_transform(df_reg2[\"kategori_adi\"])\n",
    "df_reg2[\"marka_adi\"] = le_marka_adi.inverse_transform(df_reg2[\"marka_adi\"])\n",
    "df_reg2[\"urun_adi\"] = le_urun_adi.inverse_transform(df_reg2[\"urun_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_result = results_by_sku.groupby([\"kanal_l\", \"grup\", \"urun\"]).agg({\"mape\": \"min\"}).reset_index()\n",
    "reg_winner_results = temp_result.merge(results_by_sku[[\"urun\", \"model_type\", \"mape\"]], on=[\"urun\", \"mape\"], how=\"left\")\n",
    "reg_winner_results_sku = reg_winner_results.drop_duplicates(subset=[\"grup\", \"urun\"], keep=\"first\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ort = df_reg2.copy()\n",
    "reg_ort.sort_values(by=[\"urun_adi\", \"grup_adi\", \"yil\", \"ay\"], ignore_index=True, inplace=True)\n",
    "reg_ort.drop(columns=[\"enflasyon_etkisi\", \"peak\", \"indirim__bins\", \"lockdown\", \"season\", \"trend\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\"], axis=1, inplace=True)\n",
    "reg_ort[\"yhat\"] = np.nan\n",
    "reg_ort.rename(columns={\"new_adet\": \"ytrue\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ort_mape_data = []\n",
    "for idx in reg_winner_results_sku.index:\n",
    "    test = reg_ort[(reg_ort[\"grup_adi\"] == reg_winner_results_sku[\"grup\"][idx]) & \n",
    "                   (reg_ort[\"urun_adi\"] == reg_winner_results_sku[\"urun\"][idx])]\n",
    "    start_idx = test.index.max()-2 # Son üç ayın ilk indexi. Burayı mape hesabı için kullanıyoruz.\n",
    "    end_idx = test.index.max() # Son üç ayın son indexi\n",
    "    if test[\"aktivite_tipi\"].max() == 0:\n",
    "        ort_hesaplama = list(test.loc[start_idx-3: end_idx-3][\"ytrue\"])\n",
    "        for i in range(start_idx, end_idx+1):\n",
    "            test.loc[i, \"yhat\"] = np.mean(ort_hesaplama)\n",
    "            ort_hesaplama.pop(0)\n",
    "            ort_hesaplama.append(test.loc[i, \"yhat\"])\n",
    "        reg_ort_mape_data.append(test.loc[start_idx:end_idx])\n",
    "    else:\n",
    "        akt_df_ = list(test[test[\"aktivite_tipi\"] != 0][\"ytrue\"])\n",
    "        for i in range(start_idx, end_idx+1):\n",
    "            test.loc[i, \"yhat\"] = np.mean(akt_df_)\n",
    "            akt_df_.pop(0)\n",
    "            akt_df_.append(test.loc[i, \"yhat\"])\n",
    "        reg_ort_mape_data.append(test.loc[start_idx:end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ort_mape_data = pd.concat(reg_ort_mape_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_l_ = []\n",
    "grup_adi_ = []\n",
    "urun_adi_ = []\n",
    "mape_ = []\n",
    "for idx in reg_winner_results_sku.index:\n",
    "    tmp = reg_ort_mape_data[(reg_ort_mape_data[\"grup_adi\"] == reg_winner_results_sku[\"grup\"][idx]) & \n",
    "                            (reg_ort_mape_data[\"urun_adi\"] == reg_winner_results_sku[\"urun\"][idx])]\n",
    "    kanal_l_.append(tmp[\"Kanal\"].unique()[0])\n",
    "    grup_adi_.append(tmp[\"grup_adi\"].unique()[0])\n",
    "    urun_adi_.append(tmp[\"urun_adi\"].unique()[0])\n",
    "    mape_.append(mape(tmp[\"ytrue\"], tmp[\"yhat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ort_mape_all = pd.DataFrame({\"kanal_l\": kanal_l_,\n",
    "                                 \"grup\": grup_adi_,\n",
    "                                 \"urun\": urun_adi_,\n",
    "                                 \"reg_ort_mape\": mape_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ort_mape_comparison = reg_winner_results.merge(reg_ort_mape_all, how=\"left\")\n",
    "reg_ort_mape_comparison.rename(columns={\"mape\": \"reg_mape\"}, inplace=True)\n",
    "reg_ort_mape_comparison.drop(\"model_type\", axis=1, inplace=True)\n",
    "reg_ort_mape_comparison[\"margin\"] = reg_ort_mape_comparison[\"reg_mape\"] - reg_ort_mape_comparison[\"reg_ort_mape\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_sku_list = ts_results[[\"grup\", \"urun_adi\"]]\n",
    "df_ts.sort_values(by=[\"urun_adi\", \"grup_adi\", \"date\"], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_ort_mape_data(data):\n",
    "    ts_ort_mape_data = []\n",
    "    month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "    year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "    date_goes_back = datetime(year_, month_, 1) - relativedelta(months=6)\n",
    "    df_ts_w_akt = df_ts.merge(df[[\"grup_adi\", \"urun_adi\", \"date\", \"aktivite_tipi\"]], how=\"left\")\n",
    "    for idx in data.index:\n",
    "        test = df_ts_w_akt[(df_ts_w_akt[\"grup_adi\"] == data[\"grup\"][idx]) & \n",
    "                           (df_ts_w_akt[\"urun_adi\"] == data[\"urun_adi\"][idx])]\n",
    "        test.ACTUAL.fillna(0, inplace=True)\n",
    "        start_idx = test[(test[\"date\"] > date_goes_back)].iloc[0].name\n",
    "        end_idx = test[(test[\"ACTUAL\"] == 0) & (test[\"date\"] > date_goes_back)].iloc[0].name - 1\n",
    "        if test[\"aktivite_tipi\"].max() == 0:\n",
    "            ort_hesaplama = list(test.loc[start_idx-6: end_idx-6][\"PREDICT\"])\n",
    "            for i in range(start_idx, end_idx+1):\n",
    "                test.loc[i, \"PREDICT\"] = np.mean(ort_hesaplama)\n",
    "                ort_hesaplama.pop(0)\n",
    "                ort_hesaplama.append(test.loc[i, \"PREDICT\"])\n",
    "            ts_ort_mape_data.append(test.loc[start_idx:end_idx])\n",
    "        else:\n",
    "            akt_df_ = list(test[test[\"aktivite_tipi\"] != 0][\"PREDICT\"])\n",
    "            for i in range(start_idx, end_idx+1):\n",
    "                test.loc[i, \"PREDICT\"] = np.mean(akt_df_)\n",
    "                akt_df_.pop(0)\n",
    "                akt_df_.append(test.loc[i, \"PREDICT\"])\n",
    "            ts_ort_mape_data.append(test.loc[start_idx:end_idx])\n",
    "    return pd.concat(ts_ort_mape_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ort_mape_data = time_series_ort_mape_data(ts_sku_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ort_mape_data.rename(columns={\"ACTUAL\":\"ytrue\", \"PREDICT\":\"yhat\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_l_ = []\n",
    "grup_adi_ = []\n",
    "urun_adi_ = []\n",
    "mape_ = []\n",
    "for idx in ts_sku_list.index:\n",
    "    tmp = ts_ort_mape_data[(ts_ort_mape_data[\"grup_adi\"] == ts_sku_list[\"grup\"][idx]) & \n",
    "                           (ts_ort_mape_data[\"urun_adi\"] == ts_sku_list[\"urun_adi\"][idx])]\n",
    "    kanal_l_.append(tmp[\"Kanal\"].unique()[0])\n",
    "    grup_adi_.append(tmp[\"grup_adi\"].unique()[0])\n",
    "    urun_adi_.append(tmp[\"urun_adi\"].unique()[0])\n",
    "    mape_.append(mape(tmp[\"ytrue\"], tmp[\"yhat\"]))\n",
    "    \n",
    "ts_ort_mape = pd.DataFrame({\"kanal_l\": kanal_l_,\n",
    "                            \"grup\": grup_adi_,\n",
    "                            \"urun\": urun_adi_,\n",
    "                            \"ts_ort_mape\": mape_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_list_to_reg.rename(columns={\"urun_adi\": \"urun\"}, inplace=True)\n",
    "ts_list_to_reg = ts_list_to_reg.merge(reg_ort_mape_comparison[[\"grup\", \"urun\", \"reg_mape\"]], on=[\"grup\", \"urun\"], how=\"left\")\n",
    "ts_results.rename(columns={\"urun_adi\": \"urun\"}, inplace=True)\n",
    "ts_list_to_reg = ts_list_to_reg.merge(ts_results[[\"grup\", \"urun\", \"mape\"]], on=[\"grup\", \"urun\"], how=\"left\")\n",
    "ts_list_to_reg.rename(columns={\"mape\": \"ts_mape\"}, inplace=True)\n",
    "ts_list_to_reg = ts_list_to_reg.merge(ts_ort_mape[[\"grup\", \"urun\", \"ts_ort_mape\"]], how=\"left\")\n",
    "ts_list_to_reg.rename(columns={\"ts_ort_mape\": \"ort_mape\"}, inplace=True)\n",
    "ts_list_to_reg = ts_list_to_reg[[\"grup\", \"urun\", \"ts_mape\", \"reg_mape\", \"ort_mape\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index = []\n",
    "for idx in ts_list_to_reg.index:\n",
    "    tmp = reg_ort_mape_comparison[(reg_ort_mape_comparison[\"grup\"] == ts_list_to_reg[\"grup\"][idx]) & \n",
    "                                  (reg_ort_mape_comparison[\"urun\"] == ts_list_to_reg[\"urun\"][idx])].index[0]\n",
    "    all_index.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ort_mape_comparison_exclude_ts = reg_ort_mape_comparison[~(reg_ort_mape_comparison.index.isin(all_index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_list_to_reg_min_mape = ts_list_to_reg.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_list_to_reg_winners = []\n",
    "for idx in ts_list_to_reg.index:\n",
    "    temp = ts_list_to_reg[ts_list_to_reg.index.isin([idx])]\n",
    "    if len(ts_list_to_reg[ts_list_to_reg[\"ts_mape\"] == ts_list_to_reg_min_mape[idx]]) > 0:\n",
    "        tmp = ts_list_to_reg[ts_list_to_reg[\"ts_mape\"] == ts_list_to_reg_min_mape[idx]]\n",
    "        tmp[\"winner\"] = \"time series\"\n",
    "        ts_list_to_reg_winners.append(tmp)\n",
    "    elif len(ts_list_to_reg[ts_list_to_reg[\"reg_mape\"] == ts_list_to_reg_min_mape[idx]]) > 0:\n",
    "        tmp = ts_list_to_reg[ts_list_to_reg[\"reg_mape\"] == ts_list_to_reg_min_mape[idx]]\n",
    "        tmp[\"winner\"] = \"regression\"\n",
    "        ts_list_to_reg_winners.append(tmp)\n",
    "    else:\n",
    "        tmp = ts_list_to_reg[ts_list_to_reg[\"ort_mape\"] == ts_list_to_reg_min_mape[idx]]\n",
    "        tmp[\"winner\"] = \"mean\"\n",
    "        ts_list_to_reg_winners.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_list_to_reg_winners = pd.concat(ts_list_to_reg_winners, ignore_index=True)\n",
    "reg_ort_mape_comparison_exclude_ts[\"winner\"] = reg_ort_mape_comparison_exclude_ts[\"margin\"].apply(lambda x: \"regression\" if x<0 else \"mean\")\n",
    "reg_ort_mape_comparison_exclude_ts.drop(\"margin\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts_list_to_reg_winners.to_excel(\"../results/36_12_siparis/from_ts_time_series_vs_regresyon_vs_ortalama.xlsx\", index=False)\n",
    "#reg_ort_mape_comparison_exclude_ts.to_excel(\"../results/36_12_siparis/from_reg_regresyon_vs_ortalama.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_results2 = ts_results.merge(ts_list_to_reg, on=[\"grup\", \"urun\"], how=\"left\")\n",
    "ts_results3 = ts_results2.copy()\n",
    "ts_results3[\"best_mape\"] = ts_results2[[\"mape\", \"ts_mape\", \"reg_mape\", \"ort_mape\"]].min(axis=1)\n",
    "ts_results3.drop(columns=[\"mape\", \"mape_bins\", \"ts_mape\", \"reg_mape\", \"ort_mape\"], axis=1, inplace=True)\n",
    "ts_results3.rename(columns={\"best_mape\": \"mape\"}, inplace=True)\n",
    "\n",
    "ts_results3[\"mape_bins\"] = ts_results3[\"mape\"].apply(lambda x: \"10'dan küçük\" \\\n",
    "                                                     if x<10 else (\"10 ile 20 arasında\" \\\n",
    "                                                                   if x<20 else (\"20 ile 30 arasında\" \\\n",
    "                                                                                 if x<30 else(\"30 ile 50 arasında\" \\\n",
    "                                                                                              if x<50 else(\"50 ile 100 arasında\" \\\n",
    "                                                                                                           if x<100 else \"100'den büyük\")))))\n",
    "\n",
    "reg_ort_mape_comparison_exclude_ts[\"best_mape\"] = reg_ort_mape_comparison_exclude_ts[[\"reg_mape\", \"reg_ort_mape\"]].min(axis=1)\n",
    "\n",
    "reg_results3 = reg_ort_mape_comparison_exclude_ts.copy()\n",
    "reg_results3.drop(columns=[\"reg_mape\", \"reg_ort_mape\", \"winner\"], axis=1, inplace=True)\n",
    "reg_results3.rename(columns={\"best_mape\": \"mape\"}, inplace=True)\n",
    "\n",
    "portfoy_lst = df[[\"urun_adi\", \"grup_adi\", \"portfoy\"]].drop_duplicates(subset=[\"urun_adi\", \"grup_adi\", \"portfoy\"])\n",
    "portfoy_lst.rename(columns={\"urun_adi\": \"urun\", \"grup_adi\": \"grup\"}, inplace=True)\n",
    "ts_results3 = ts_results3.merge(portfoy_lst, how=\"left\")\n",
    "ts_results3 = ts_results3[ts_results3[\"portfoy\"] == 1].reset_index(drop=True)\n",
    "ts_results3.drop(\"portfoy\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## MAPE Visualization\n",
    "---\n",
    "# Regression MAPE Bins\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_results3[\"mape_bins\"] = reg_results3[\"mape\"].apply(lambda x: \"10'dan küçük\" \\\n",
    "                                                       if x<10 else (\"10 ile 20 arasında\" \\\n",
    "                                                                     if x<20 else (\"20 ile 30 arasında\" \\\n",
    "                                                                                   if x<30 else(\"30 ile 50 arasında\" \\\n",
    "                                                                                                if x<50 else(\"50 ile 100 arasında\" \\\n",
    "                                                                                                             if x<100 else \"100'den büyük\")))))\n",
    "\n",
    "mape_dist = (reg_results3.mape_bins.value_counts() / len(reg_results3))*100\n",
    "reorderlist = [\"10'dan küçük\", \"10 ile 20 arasında\", \"20 ile 30 arasında\", \"30 ile 50 arasında\", \"50 ile 100 arasında\", \"100'den büyük\"]\n",
    "mape_dist = mape_dist.reindex(reorderlist)\n",
    "\n",
    "btt_best_results_sku = reg_results3[reg_results3[\"kanal_l\"] == \"btt\"]\n",
    "pas_best_results_sku = reg_results3[reg_results3[\"kanal_l\"] == \"pasifik\"]\n",
    "hor_best_results_sku = reg_results3[reg_results3[\"kanal_l\"] == \"horizon\"]\n",
    "\n",
    "btt_mape_dist = (btt_best_results_sku.mape_bins.value_counts() / len(btt_best_results_sku))*100\n",
    "btt_mape_dist = btt_mape_dist.reindex(reorderlist)\n",
    "\n",
    "hor_mape_dist = (hor_best_results_sku.mape_bins.value_counts() / len(hor_best_results_sku))*100\n",
    "hor_mape_dist = hor_mape_dist.reindex(reorderlist)\n",
    "\n",
    "pas_mape_dist = (pas_best_results_sku.mape_bins.value_counts() / len(pas_best_results_sku))*100\n",
    "pas_mape_dist = pas_mape_dist.reindex(reorderlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_mape_dist.fillna(0, inplace=True)\n",
    "btt_sku_num = str(reg_results3.groupby(\"kanal_l\").count()[\"grup\"][\"btt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_mape_dist.fillna(0, inplace=True)\n",
    "hor_sku_num = str(reg_results3.groupby(\"kanal_l\").count()[\"grup\"][\"horizon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_mape_dist.fillna(0, inplace=True)\n",
    "pas_sku_num = str(reg_results3.groupby(\"kanal_l\").count()[\"grup\"][\"pasifik\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_all_sku_num = str(reg_results3.groupby(\"kanal_l\").count()[\"grup\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_results3.to_excel(\"../results/36_12_siparis/scores/reg_sku_mape_dagilimi_36_12.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Time Series MAPE Bins\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_results3.dropna(inplace=True, axis=0)\n",
    "\n",
    "ts_mape_dist = (ts_results3.mape_bins.value_counts() / len(ts_results3))*100\n",
    "ts_mape_dist = ts_mape_dist.reindex(reorderlist)\n",
    "\n",
    "btt_ts_results_sku = ts_results3[ts_results3[\"kanal_l\"] == \"btt\"]\n",
    "pas_ts_results_sku = ts_results3[ts_results3[\"kanal_l\"] == \"pasifik\"]\n",
    "hor_ts_results_sku = ts_results3[ts_results3[\"kanal_l\"] == \"horizon\"]\n",
    "\n",
    "btt_ts_mape_dist = (btt_ts_results_sku.mape_bins.value_counts() / len(btt_ts_results_sku))*100\n",
    "btt_ts_mape_dist = btt_ts_mape_dist.reindex(reorderlist)\n",
    "\n",
    "hor_ts_mape_dist = (hor_ts_results_sku.mape_bins.value_counts() / len(hor_ts_results_sku))*100\n",
    "hor_ts_mape_dist = hor_ts_mape_dist.reindex(reorderlist)\n",
    "\n",
    "pas_ts_mape_dist = (pas_ts_results_sku.mape_bins.value_counts() / len(pas_ts_results_sku))*100\n",
    "pas_ts_mape_dist = pas_ts_mape_dist.reindex(reorderlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_ts_mape_dist.fillna(0, inplace=True)\n",
    "btt_ts_sku_num = str(ts_results3.groupby(\"kanal_l\").count()[\"grup\"][\"btt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_ts_mape_dist.fillna(0, inplace=True)\n",
    "hor_ts_sku_num = str(ts_results3.groupby(\"kanal_l\").count()[\"grup\"][\"horizon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_ts_mape_dist.fillna(0, inplace=True)\n",
    "pas_ts_sku_num = str(ts_results3.groupby(\"kanal_l\").count()[\"grup\"][\"pasifik\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_all_sku_num = str(ts_results3.groupby(\"kanal_l\").count()[\"grup\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts_results3.to_excel(\"../results/36_12_siparis/scores/ts_sku_mape_dagilimi_36_12.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regression + Time Series MAPE Bins\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ts_results_sku = pd.concat([ts_results3, reg_results3], axis=0, ignore_index=True)\n",
    "\n",
    "ts_reg_mape_dist = (reg_ts_results_sku.mape_bins.value_counts() / len(reg_ts_results_sku))*100\n",
    "ts_reg_mape_dist = ts_reg_mape_dist.reindex(reorderlist)\n",
    "\n",
    "btt_reg_ts_results_sku = reg_ts_results_sku[reg_ts_results_sku[\"kanal_l\"] == \"btt\"]\n",
    "pas_reg_ts_results_sku = reg_ts_results_sku[reg_ts_results_sku[\"kanal_l\"] == \"pasifik\"]\n",
    "hor_reg_ts_results_sku = reg_ts_results_sku[reg_ts_results_sku[\"kanal_l\"] == \"horizon\"]\n",
    "\n",
    "btt_reg_ts_mape_dist = (btt_reg_ts_results_sku.mape_bins.value_counts() / len(btt_reg_ts_results_sku))*100\n",
    "btt_reg_ts_mape_dist = btt_reg_ts_mape_dist.reindex(reorderlist)\n",
    "\n",
    "hor_reg_ts_mape_dist = (hor_reg_ts_results_sku.mape_bins.value_counts() / len(hor_reg_ts_results_sku))*100\n",
    "hor_reg_ts_mape_dist = hor_reg_ts_mape_dist.reindex(reorderlist)\n",
    "\n",
    "pas_reg_ts_mape_dist = (pas_reg_ts_results_sku.mape_bins.value_counts() / len(pas_reg_ts_results_sku))*100\n",
    "pas_reg_ts_mape_dist = pas_reg_ts_mape_dist.reindex(reorderlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_reg_ts_mape_dist.fillna(0, inplace=True)\n",
    "btt_ts_reg_sku_num = str(reg_ts_results_sku.groupby(\"kanal_l\").count()[\"grup\"][\"btt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_reg_ts_mape_dist.fillna(0, inplace=True)\n",
    "hor_ts_reg_sku_num = str(reg_ts_results_sku.groupby(\"kanal_l\").count()[\"grup\"][\"horizon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_reg_ts_mape_dist.fillna(0, inplace=True)\n",
    "pas_ts_reg_sku_num = str(reg_ts_results_sku.groupby(\"kanal_l\").count()[\"grup\"][\"pasifik\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ts_mape_dist = (reg_ts_results_sku.mape_bins.value_counts() / len(reg_ts_results_sku))*100\n",
    "reg_ts_mape_dist = reg_ts_mape_dist.reindex(reorderlist)\n",
    "ts_reg_all_sku_num = str(reg_ts_results_sku.groupby(\"kanal_l\").count()[\"grup\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_results4 = ts_results3.merge(ts_list_to_reg_winners[[\"grup\", \"urun\", \"winner\"]], how=\"left\")\n",
    "ts_results4.winner.fillna(\"time series\", inplace=True)\n",
    "reg_results4 = reg_results3.merge(reg_ort_mape_comparison_exclude_ts[[\"kanal_l\", \"grup\", \"urun\", \"winner\"]], how=\"left\")\n",
    "ts_results4 = ts_results4[reg_results4.columns.to_list()]\n",
    "\n",
    "ts_results4[\"data_from\"] = \"time series\"\n",
    "reg_results4[\"data_from\"] = \"regression\"\n",
    "all_results4 = pd.concat([ts_results4, reg_results4], axis=0, ignore_index=True)\n",
    "\n",
    "reg_final_test_results = []\n",
    "for idx in reg_winner_results_sku.index:\n",
    "    tmp = test_results[(test_results[\"urun_adi\"] == reg_winner_results_sku[\"urun\"][idx]) &\n",
    "                       (test_results[\"grup_adi\"] == reg_winner_results_sku[\"grup\"][idx])][[\"Kanal\", \n",
    "                                                                                           \"grup_adi\", \n",
    "                                                                                           \"urun_adi\", \n",
    "                                                                                           \"yil\", \n",
    "                                                                                           \"ay\", \n",
    "                                                                                           \"new_adet\", \n",
    "                                                                                           reg_winner_results_sku[\"model_type\"][idx]]].rename(columns={\"Kanal\": \"kanal\",\n",
    "                                                                                                                                                      \"grup_adi\": \"grup\",\n",
    "                                                                                                                                                      \"urun_adi\": \"urun\",\n",
    "                                                                                                                                                      \"new_adet\": \"ytrue\",\n",
    "                                                                                                                                                      reg_winner_results_sku[\"model_type\"][idx]: \"yhat\"})\n",
    "    reg_final_test_results.append(tmp)\n",
    "reg_final_test_results = pd.concat(reg_final_test_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_final_test_results[\"date\"] = reg_final_test_results[\"yil\"].astype(str) + \"-\" + reg_final_test_results[\"ay\"].astype(str) + \"-01\"\n",
    "reg_final_test_results[\"date\"] = pd.to_datetime(reg_final_test_results[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "reg_final_raw_prediction = []\n",
    "for idx in reg_winner_results_sku.index:\n",
    "    tmp = df_reg2[(df_reg2[\"urun_adi\"] == reg_winner_results_sku[\"urun\"][idx]) &\n",
    "                  (df_reg2[\"grup_adi\"] == reg_winner_results_sku[\"grup\"][idx])][[\"Kanal\", \n",
    "                                                                                 \"grup_adi\", \n",
    "                                                                                 \"urun_adi\", \n",
    "                                                                                 \"yil\", \n",
    "                                                                                 \"ay\", \n",
    "                                                                                 \"new_adet\"]].rename(columns={\"Kanal\": \"kanal\",\n",
    "                                                                                                              \"grup_adi\": \"grup\",\n",
    "                                                                                                              \"urun_adi\": \"urun\",\n",
    "                                                                                                              \"new_adet\": \"ytrue\"})\n",
    "    reg_final_raw_prediction.append(tmp)\n",
    "reg_final_raw_prediction = pd.concat(reg_final_raw_prediction, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_final_raw_prediction[\"date\"] = reg_final_raw_prediction[\"yil\"].astype(str) + \"-\" + reg_final_raw_prediction[\"ay\"].astype(str) + \"-01\"\n",
    "reg_final_raw_prediction[\"date\"] = pd.to_datetime(reg_final_raw_prediction[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "reg_final_raw_prediction = reg_final_raw_prediction.merge(reg_final_test_results, how=\"left\")\n",
    "reg_final_raw_prediction.drop(columns=[\"yil\", \"ay\"], axis=1, inplace=True)\n",
    "reg_final_raw_prediction = reg_final_raw_prediction[[\"kanal\", \"grup\", \"urun\", \"date\", \"ytrue\", \"yhat\"]]\n",
    "\n",
    "reg_ort_mape_data.rename(columns={\"Kanal\": \"kanal\", \"grup_adi\": \"grup\", \"urun_adi\": \"urun\"}, inplace=True)\n",
    "ts_ort_mape_data.rename(columns={\"Kanal\": \"kanal\", \"grup_adi\": \"grup\", \"urun_adi\": \"urun\"}, inplace=True)\n",
    "\n",
    "reg_ort_mape_data[\"date\"] = reg_ort_mape_data[\"yil\"].astype(str) + \"-\" + reg_ort_mape_data[\"ay\"].astype(str) + \"-01\"\n",
    "reg_ort_mape_data[\"date\"] = pd.to_datetime(reg_ort_mape_data[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "reg_ort_mape_data.drop(columns=[\"yil\", \"ay\"], axis=1, inplace=True)\n",
    "reg_ort_mape_data = reg_ort_mape_data[ts_ort_mape_data.columns.to_list()]\n",
    "\n",
    "reg_ort_mape_data[\"data_from\"] = \"regression\"\n",
    "ts_ort_mape_data[\"data_from\"] = \"time series\"\n",
    "all_test_ort_mape_data = pd.concat([reg_ort_mape_data, ts_ort_mape_data], axis=0, ignore_index=True)\n",
    "\n",
    "df3_ort_raw_data = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_ort_raw_data.rename(columns={\"Kanal\": \"kanal\", \"grup_adi\": \"grup\", \"urun_adi\": \"urun\", \"new_adet\": \"ytrue\"}, inplace=True)\n",
    "df3_ort_raw_data[\"date\"] = pd.to_datetime(df3_ort_raw_data[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_ort_raw_data_reg = df3_ort_raw_data.merge(reg_ort_mape_data[[\"kanal\", \"grup\", \"urun\", \"date\", \"yhat\"]], on=[\"kanal\", \"grup\", \"urun\", \"date\"], how=\"left\")\n",
    "df4_ort_raw_data_ts = df3_ort_raw_data.merge(ts_ort_mape_data[[\"kanal\", \"grup\", \"urun\", \"date\", \"yhat\"]], on=[\"kanal\", \"grup\", \"urun\", \"date\"], how=\"left\")\n",
    "\n",
    "df4_ort_raw_data_ts.drop([\"yil\", \"ay\"], axis=1, inplace=True)\n",
    "df4_ort_raw_data_reg.drop([\"yil\", \"ay\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for idx in all_results4.index:\n",
    "    if all_results4[\"winner\"][idx] == \"time series\":\n",
    "        tmp_df = df_ts[(df_ts[\"urun_adi\"] == all_results4[\"urun\"][idx]) & \n",
    "                       (df_ts[\"urun_adi\"] == all_results4[\"urun\"][idx])]\n",
    "        tmp_df.rename(columns={\"Kanal\": \"kanal\",\n",
    "                              \"grup_adi\": \"grup\",\n",
    "                              \"urun_adi\": \"urun\",\n",
    "                              \"ACTUAL\": \"ytrue\",\n",
    "                              \"PREDICT\": \"yhat\"}, inplace=True)\n",
    "        tmp_df[\"data_from\"] = \"time series\"\n",
    "        all_data.append(tmp_df)\n",
    "    elif all_results4[\"winner\"][idx] == \"regression\":\n",
    "        tmp_df = reg_final_raw_prediction[(reg_final_raw_prediction[\"urun\"] == all_results4[\"urun\"][idx]) & \n",
    "                                          (reg_final_raw_prediction[\"grup\"] == all_results4[\"grup\"][idx])]\n",
    "        tmp_df[\"data_from\"] = \"regression\"\n",
    "        all_data.append(tmp_df)\n",
    "    else:\n",
    "        if all_results4[\"data_from\"][idx] == \"time series\":\n",
    "            tmp_df = df4_ort_raw_data_ts[(df4_ort_raw_data_ts[\"urun\"] == all_results4[\"urun\"][idx]) & \n",
    "                                         (df4_ort_raw_data_ts[\"grup\"] == all_results4[\"grup\"][idx])]\n",
    "            tmp_df[\"data_from\"] = \"mean\"\n",
    "            all_data.append(tmp_df)\n",
    "        else:\n",
    "            tmp_df = df4_ort_raw_data_reg[(df4_ort_raw_data_reg[\"urun\"] == all_results4[\"urun\"][idx]) & \n",
    "                                         (df4_ort_raw_data_reg[\"grup\"] == all_results4[\"grup\"][idx])]\n",
    "            tmp_df[\"data_from\"] = \"mean\"\n",
    "            all_data.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat(all_data, ignore_index=True)\n",
    "all_data[\"yhat\"] = np.abs(all_data[\"yhat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data.to_excel(\"../results/36_12_siparis/final_predictions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresyondan Gelen Kısım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:18:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"colsample_bytree\", \"max_depth\", \"min_child_weight\", \"subsample\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(criterion='mae', learning_rate=0.2, max_depth=5,\n",
       "                          max_features='log2', min_samples_leaf=0.1,\n",
       "                          min_samples_split=0.1, n_estimators=10,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
    "Y = pd.concat([y_train, y_test], axis=0, ignore_index=True)\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, Y)\n",
    "\n",
    "gsm_knn = KNeighborsRegressor(algorithm=params_[\"gridsearch_params\"][\"knn_params\"][\"algorithm\"], \n",
    "                              leaf_size=params_[\"gridsearch_params\"][\"knn_params\"][\"leaf_size\"], \n",
    "                              metric=params_[\"gridsearch_params\"][\"knn_params\"][\"metric\"], \n",
    "                              n_neighbors=params_[\"gridsearch_params\"][\"knn_params\"][\"n_neighbors\"], \n",
    "                              weights=params_[\"gridsearch_params\"][\"knn_params\"][\"weights\"])\n",
    "gsm_knn.fit(X, Y)\n",
    "\n",
    "gsm_xgb = XGBRegressor(booster=params_[\"gridsearch_params\"][\"xgboost_params\"][\"booster\"], \n",
    "                       colsample_bytree=params_[\"gridsearch_params\"][\"xgboost_params\"][\"colsample_bytree\"], \n",
    "                       learning_rate=params_[\"gridsearch_params\"][\"xgboost_params\"][\"learning_rate\"], \n",
    "                       max_depth=params_[\"gridsearch_params\"][\"xgboost_params\"][\"max_depth\"], \n",
    "                       min_child_weight=params_[\"gridsearch_params\"][\"xgboost_params\"][\"min_child_weight\"], \n",
    "                       objective=params_[\"gridsearch_params\"][\"xgboost_params\"][\"objective\"], \n",
    "                       random_state=params_[\"gridsearch_params\"][\"xgboost_params\"][\"random_state\"], \n",
    "                       subsample=params_[\"gridsearch_params\"][\"xgboost_params\"][\"subsample\"])\n",
    "\n",
    "gsm_xgb.fit(X, Y)\n",
    "\n",
    "gsm_dt = DecisionTreeRegressor(criterion=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"criterion\"], \n",
    "                               max_depth=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"max_depth\"], \n",
    "                               min_samples_leaf=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"min_samples_leaf\"], \n",
    "                               min_samples_split=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"min_samples_split\"], \n",
    "                               random_state=params_[\"gridsearch_params\"][\"decisiontree_params\"][\"random_state\"])\n",
    "\n",
    "gsm_dt.fit(X, Y)\n",
    "\n",
    "gsm_rf = RandomForestRegressor(criterion=params_[\"gridsearch_params\"][\"randomforest_params\"][\"criterion\"], \n",
    "                               max_depth=params_[\"gridsearch_params\"][\"randomforest_params\"][\"max_depth\"], \n",
    "                               min_samples_leaf=params_[\"gridsearch_params\"][\"randomforest_params\"][\"min_samples_leaf\"], \n",
    "                               min_samples_split=params_[\"gridsearch_params\"][\"randomforest_params\"][\"min_samples_split\"], \n",
    "                               random_state=params_[\"gridsearch_params\"][\"randomforest_params\"][\"random_state\"])\n",
    "\n",
    "gsm_rf.fit(X, Y)\n",
    "\n",
    "gsm_gb = GradientBoostingRegressor(criterion=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"criterion\"], \n",
    "                                   learning_rate=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"learning_rate\"], \n",
    "                                   max_depth=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"max_depth\"], \n",
    "                                   max_features=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"max_features\"], \n",
    "                                   min_samples_leaf=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"min_samples_leaf\"], \n",
    "                                   min_samples_split=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"min_samples_split\"], \n",
    "                                   n_estimators=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"n_estimators\"], \n",
    "                                   random_state=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"random_state\"], \n",
    "                                   subsample=params_[\"gridsearch_params\"][\"gradientboosting_params\"][\"subsample\"])\n",
    "gsm_gb.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_regression_reg_june = all_results4[(all_results4[\"data_from\"] == \"regression\") & (all_results4[\"winner\"] == \"regression\")]\n",
    "from_regression_mean_june = all_results4[(all_results4[\"data_from\"] == \"regression\") & (all_results4[\"winner\"] == \"mean\")]\n",
    "\n",
    "june_reg_sku = from_regression_reg_june.merge(reg_winner_results.drop(\"mape\", axis=1), how=\"left\")[[\"kanal_l\", \"grup\", \"urun\", \"data_from\", \"winner\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku = june_reg_sku.merge(urun_isim_kod, on=\"urun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku_winner_algorithms = june_reg_sku.copy()\n",
    "june_reg_sku_winner_algorithms.rename(columns={\"kanal_l\": \"Kanal\", \"grup\": \"grup_adi\", \"urun\": \"urun_adi\"}, inplace=True)\n",
    "june_reg_sku_winner_algorithms.drop(\"en_guncel_kod\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg2[\"date\"] = pd.to_datetime(df_reg2[\"yil\"].astype(str) + \"-\" + df_reg2[\"ay\"].astype(str) + \"-01\", format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trend_seasonality(data__):\n",
    "    month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "    year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "    for idx in data__.index:\n",
    "        tmp = df_reg2[(df_reg2[\"grup_adi\"] == data__[\"grup_adi\"][idx]) & \n",
    "                      (df_reg2[\"urun_adi\"] == data__[\"urun_adi\"][idx])]\n",
    "        \n",
    "        trend_index = tmp.iloc[-4:, :].index\n",
    "        calc_df = tmp.loc[trend_index, [\"date\", \"trend\"]]\n",
    "        last_value = calc_df.iloc[-1, 1:].values[0]\n",
    "        first_value = calc_df.iloc[0, 1:].values[0]\n",
    "        add_value = (last_value - first_value)  / 4\n",
    "        \n",
    "        addition_value = last_value\n",
    "        index_max = tmp.index.max()\n",
    "        for trh_trend in range(1, 5):\n",
    "            addition_value += add_value\n",
    "            new_date = datetime(year_, month_, 1) + relativedelta(months=trh_trend)\n",
    "            index_ = list(all_combinations[(all_combinations[\"yil\"] == new_date.year) & \n",
    "                                           (all_combinations[\"ay\"] == new_date.month) & \n",
    "                                           (all_combinations[\"grup_adi\"] == data__[\"grup_adi\"][idx]) & \n",
    "                                           (all_combinations[\"urun_adi\"] == data__[\"urun_adi\"][idx])].index)\n",
    "            all_combinations.loc[index_, \"trend\"] = addition_value\n",
    "            if len(tmp) >= 26:\n",
    "                all_combinations.loc[index_, \"season\"] = tmp.loc[index_max-23, \"season\"]\n",
    "            elif len(tmp) >= 12 and len(tmp) < 24:\n",
    "                all_combinations.loc[index_, \"season\"] = tmp.loc[index_max-11, \"season\"]\n",
    "            else:\n",
    "                season_ = list(tmp.season[-6:])\n",
    "                season_value = np.mean(season_)\n",
    "                all_combinations.loc[index_, \"season\"] = season_value\n",
    "                season_.pop(0)\n",
    "                season_.append(season_value)\n",
    "            index_max += 1\n",
    "    return all_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku = create_trend_seasonality(df_reg2.drop_duplicates(subset=[\"grup_adi\", \"urun_adi\"], ignore_index=True)[[\"grup_adi\", \"urun_adi\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku = june_reg_sku[~(june_reg_sku.season.isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku[\"season\"] = june_reg_sku[\"season\"].astype(float)\n",
    "june_reg_sku[\"trend\"] = june_reg_sku[\"trend\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku_backup = june_reg_sku.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku.rename(columns={\"kanal_l\": \"Kanal\", \"grup\": \"grup_adi\", \"urun\": \"urun_adi\", \"enflasyon\": \"enflasyon_etkisi\"}, inplace=True)\n",
    "june_reg_sku = june_reg_sku[X_train.columns.to_list()]\n",
    "june_reg_sku_backup = june_reg_sku.copy()\n",
    "june_reg_sku = june_reg_sku.merge(reg_winner_results[[\"kanal_l\", \"grup\", \"urun\", \"model_type\"]], left_on=[\"Kanal\", \"grup_adi\", \"urun_adi\"], right_on=[\"kanal_l\", \"grup\", \"urun\"], how=\"left\")\n",
    "june_reg_sku[\"Kanal\"] = le_kanal.transform(june_reg_sku[\"Kanal\"])\n",
    "june_reg_sku[\"grup_adi\"] = le_grup_adi.transform(june_reg_sku[\"grup_adi\"])\n",
    "june_reg_sku[\"ana_kategori_adi\"] = le_ana_kategori_adi.transform(june_reg_sku[\"ana_kategori_adi\"])\n",
    "june_reg_sku[\"kategori_adi\"] = le_kategori_adi.transform(june_reg_sku[\"kategori_adi\"])\n",
    "june_reg_sku[\"marka_adi\"] = le_marka_adi.transform(june_reg_sku[\"marka_adi\"])\n",
    "june_reg_sku[\"urun_adi\"] = le_urun_adi.transform(june_reg_sku[\"urun_adi\"])\n",
    "june_reg_sku.drop([\"kanal_l\", \"grup\", \"urun\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku_winners = june_reg_sku.copy()\n",
    "june_reg_sku_winners[\"Kanal\"] = le_kanal.inverse_transform(june_reg_sku_winners[\"Kanal\"])\n",
    "june_reg_sku_winners[\"grup_adi\"] = le_grup_adi.inverse_transform(june_reg_sku_winners[\"grup_adi\"])\n",
    "june_reg_sku_winners[\"ana_kategori_adi\"] = le_ana_kategori_adi.inverse_transform(june_reg_sku_winners[\"ana_kategori_adi\"])\n",
    "june_reg_sku_winners[\"kategori_adi\"] = le_kategori_adi.inverse_transform(june_reg_sku_winners[\"kategori_adi\"])\n",
    "june_reg_sku_winners[\"marka_adi\"] = le_marka_adi.inverse_transform(june_reg_sku_winners[\"marka_adi\"])\n",
    "june_reg_sku_winners[\"urun_adi\"] = le_urun_adi.inverse_transform(june_reg_sku_winners[\"urun_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku_winners = june_reg_sku_winners[[\"Kanal\", \"grup_adi\", \"urun_adi\", \"model_type\"]].drop_duplicates(subset=[\"Kanal\", \"grup_adi\", \"urun_adi\", \"model_type\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(data):\n",
    "    yhat_results = []\n",
    "    for idx in data.index:\n",
    "        tmp_df = pd.DataFrame(data.loc[idx]).T\n",
    "        model_type = data[\"model_type\"][idx]\n",
    "        for x in data.columns:\n",
    "            tmp_df[x] = tmp_df[x].astype(data[x].dtypes.name)\n",
    "        if model_type == \"linear_regression\":\n",
    "            yhat = lm.predict(tmp_df.drop(\"model_type\", axis=1))\n",
    "            yhat_results.append(yhat[0][0])\n",
    "        elif model_type == \"knn\":\n",
    "            yhat = gsm_knn.predict(tmp_df.drop(\"model_type\", axis=1))\n",
    "            yhat_results.append(yhat[0][0])\n",
    "        elif model_type == \"xgboost\":\n",
    "            yhat = gsm_xgb.predict(tmp_df.drop(\"model_type\", axis=1))\n",
    "            yhat_results.append(yhat[0])\n",
    "        elif model_type == \"decision_tree\":\n",
    "            yhat = gsm_dt.predict(tmp_df.drop(\"model_type\", axis=1))\n",
    "            yhat_results.append(yhat[0])\n",
    "        elif model_type == \"random_forest\":\n",
    "            yhat = gsm_rf.predict(tmp_df.drop(\"model_type\", axis=1))\n",
    "            yhat_results.append(yhat[0])\n",
    "        else:\n",
    "            yhat = gsm_gb.predict(tmp_df.drop(\"model_type\", axis=1))\n",
    "            yhat_results.append(yhat[0])\n",
    "    return yhat_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku[\"yil\"] = june_reg_sku[\"yil\"].astype(\"int16\")\n",
    "june_reg_sku[\"ay\"] = june_reg_sku[\"ay\"].astype(\"int8\")\n",
    "\n",
    "june_reg_sku[\"peak\"] = june_reg_sku[\"peak\"].astype(\"int8\")\n",
    "june_reg_sku[\"indirim__bins\"] = june_reg_sku[\"indirim__bins\"].astype(\"int8\")\n",
    "\n",
    "june_reg_sku[\"aktivite_tipi\"] = june_reg_sku[\"aktivite_tipi\"].astype(\"int8\")\n",
    "june_reg_sku[\"lockdown\"] = june_reg_sku[\"lockdown\"].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku[\"yhat\"] = get_prediction(june_reg_sku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku[\"Kanal\"] = le_kanal.inverse_transform(june_reg_sku[\"Kanal\"])\n",
    "june_reg_sku[\"grup_adi\"] = le_grup_adi.inverse_transform(june_reg_sku[\"grup_adi\"])\n",
    "june_reg_sku[\"ana_kategori_adi\"] = le_ana_kategori_adi.inverse_transform(june_reg_sku[\"ana_kategori_adi\"])\n",
    "june_reg_sku[\"kategori_adi\"] = le_kategori_adi.inverse_transform(june_reg_sku[\"kategori_adi\"])\n",
    "june_reg_sku[\"marka_adi\"] = le_marka_adi.inverse_transform(june_reg_sku[\"marka_adi\"])\n",
    "june_reg_sku[\"urun_adi\"] = le_urun_adi.inverse_transform(june_reg_sku[\"urun_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku[\"yhat\"] = np.abs(june_reg_sku[\"yhat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku = june_reg_sku.merge(june_reg_sku_winner_algorithms, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "#june_reg_sku.to_excel(\"../results/36_12_siparis/regresyondan_gelen_modeller.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ortalama Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_ort_raw_data_reg = df4_ort_raw_data_reg.merge(df[[\"grup_adi\", \"urun_adi\", \"date\", \"aktivite_tipi\", \"indirim__bins\"]].rename(columns={\"grup_adi\": \"grup\", \"urun_adi\": \"urun\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_prediction(data):\n",
    "    final_df = []\n",
    "    month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "    year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "    for idx in data.index:\n",
    "        tmp = df4_ort_raw_data_reg[(df4_ort_raw_data_reg[\"grup\"] == data[\"grup\"][idx]) & \n",
    "                                   (df4_ort_raw_data_reg[\"urun\"] == data[\"urun\"][idx])]\n",
    "        grup_ = data[\"grup\"][idx]\n",
    "        urun_ = data[\"urun\"][idx]\n",
    "\n",
    "        if tmp[\"aktivite_tipi\"].max() == 0:\n",
    "            ytrue_ = list(tmp.iloc[-3:, -4:-3].ytrue)\n",
    "        \n",
    "        else:\n",
    "            ytrue_index = tmp[tmp[\"aktivite_tipi\"] != 0].index\n",
    "            ytrue_ = list(tmp.loc[ytrue_index, \"ytrue\"])\n",
    "        \n",
    "        for trh_trend in range(1, 5):\n",
    "            new_date = datetime(year_, month_, 1) + relativedelta(months=trh_trend)\n",
    "            yprediction = np.mean(ytrue_)\n",
    "            if len(ytrue_) >= 3:\n",
    "                ytrue_.pop(0)\n",
    "                ytrue_.append(yprediction)\n",
    "            else:\n",
    "                ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [new_date],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "            \n",
    "\n",
    "    final_df = pd.concat(final_df, ignore_index=True)    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_regression_mean_june_backup = from_regression_mean_june.copy()\n",
    "from_regression_mean_june_winner_algorithms = from_regression_mean_june.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_regression_mean_june_winner_algorithms.rename(columns={\"kanal_l\": \"Kanal\", \"grup\": \"grup_adi\", \"urun\": \"urun_adi\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_mean_sku = mean_prediction(from_regression_mean_june)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SADECE SÜTUN İSMİ İÇİN KULLANIYORUM\n",
    "june_reg_mean_sku2 = june_reg_mean_sku.copy()\n",
    "june_reg_mean_sku2.rename(columns={\"urun\": \"urun_adi\", \"grup\": \"grup_adi\"}, inplace=True)\n",
    "june_reg_mean_sku2.drop(\"kanal\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_mean_sku_with_combs = []\n",
    "for idx in june_reg_mean_sku2.index:\n",
    "    tmp = all_combinations[(all_combinations[\"grup_adi\"] == june_reg_mean_sku2[\"grup_adi\"][idx]) & \n",
    "                           (all_combinations[\"grup_adi\"] == june_reg_mean_sku2[\"grup_adi\"][idx])]\n",
    "    tmp = tmp.merge(june_reg_mean_sku2, how=\"left\", on=[\"grup_adi\", \"urun_adi\", \"date\"])\n",
    "#    tmp = tmp[[\"Kanal\", \"grup_adi\", \"urun_adi\", \"date\", \"ytrue\", \"yhat\"]]\n",
    "    june_reg_mean_sku_with_combs.append(tmp)\n",
    "june_reg_mean_sku_with_combs = pd.concat(june_reg_mean_sku_with_combs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_mean_sku_with_combs = june_reg_mean_sku_with_combs[~(june_reg_mean_sku_with_combs[\"yhat\"].isna())].reset_index(drop=True)\n",
    "june_reg_mean_sku_with_combs = june_reg_mean_sku_with_combs.merge(from_regression_mean_june_winner_algorithms, how=\"left\")\n",
    "june_reg_mean_sku_with_combs[\"model_type\"] = np.nan\n",
    "june_reg_mean_sku_with_combs = june_reg_mean_sku_with_combs[june_reg_sku.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_sku[\"date\"] = june_reg_sku[\"yil\"].astype(str) + \"-\" + june_reg_sku[\"ay\"].astype(str) + \"-01\"\n",
    "june_reg_sku[\"date\"] = pd.to_datetime(june_reg_sku[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "june_reg_sku[\"ytrue\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_mean_sku_with_combs[\"date\"] = june_reg_mean_sku_with_combs[\"yil\"].astype(str) + \"-\" + june_reg_mean_sku_with_combs[\"ay\"].astype(str) + \"-01\"\n",
    "june_reg_mean_sku_with_combs[\"date\"] = pd.to_datetime(june_reg_mean_sku_with_combs[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "june_reg_mean_sku_with_combs[\"ytrue\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_kat_marka_df = df2[[\"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\", \"urun_adi\"]].drop_duplicates(subset=[\"grup_adi\", \n",
    "                                                                                                                          \"ana_kategori_adi\", \n",
    "                                                                                                                          \"kategori_adi\", \n",
    "                                                                                                                          \"marka_adi\", \n",
    "                                                                                                                          \"urun_adi\"], ignore_index=True)\n",
    "\n",
    "ana_kat_marka_df.rename(columns={\"grup_adi\": \"grup\", \"urun_adi\": \"urun\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "aktli_options = june_reg_mean_sku_with_combs.drop_duplicates(subset=[\"aktivite_tipi\", \"indirim__bins\"])[[\"indirim__bins\", \"aktivite_tipi\"]]\n",
    "aktli_options = aktli_options[aktli_options[\"aktivite_tipi\"] != 0].reset_index(drop=True)\n",
    "\n",
    "aktsiz_options = june_reg_mean_sku_with_combs.drop_duplicates(subset=[\"aktivite_tipi\", \"indirim__bins\"])[[\"indirim__bins\", \"aktivite_tipi\"]]\n",
    "aktsiz_options = aktsiz_options[aktsiz_options[\"aktivite_tipi\"] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geçmişte aktivitesi olmayan ve prediction'da aktivite uygulanmış skular için çarpılaracak katsayıların olduğu dataframe'in okunması. Katsayısı 1'in altında olanlar 1 yapıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "katsayi_df = pd.read_excel(params_[\"files\"][\"ort_katsayi_file\"], sheet_name=\"katsayi\")\n",
    "katsayi_df[\"katsayi\"] = katsayi_df[\"katsayi\"].apply(lambda x: 1 if x < 1 else x)\n",
    "katsayi_df[\"kac_kati\"] = katsayi_df[\"kac_kati\"].apply(lambda x: 1 if x < 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_prediction_comb(data__, data_main, relevant_data):\n",
    "    \"\"\"\n",
    "    data__ = Unique SKU & Grup isimlerinin yer aldığı dataframe\n",
    "    \n",
    "    data_main = Kombinasyonların alındığı ilgili data. Örn: Regresyon için june_reg_mean_sku_with_combs\n",
    "    \n",
    "    relevant_data = Kombinasyonların alındığı ilgili datanın geçmişi. Refere edilen data verilir. \n",
    "    Örn: Regresyon içinse df4_ort_raw_data_reg, Time Series içinse df4_ort_raw_data_ts\n",
    "    \"\"\"\n",
    "    final_df = []\n",
    "    for idx in data__.index:\n",
    "\n",
    "#        tmp = june_reg_mean_sku_with_combs[(june_reg_mean_sku_with_combs[\"grup_adi\"] == data__[\"grup_adi\"][idx]) & \n",
    "#                                           (june_reg_mean_sku_with_combs[\"urun_adi\"] == data__[\"urun_adi\"][idx])]\n",
    "#        tmp.drop_duplicates(inplace=True)\n",
    "        tmp = data_main[(data_main[\"grup_adi\"] == data__[\"grup\"][idx]) & \n",
    "                        (data_main[\"urun_adi\"] == data__[\"urun\"][idx])]\n",
    "        tmp.drop_duplicates(inplace=True)\n",
    "\n",
    "#        tmp_relevant = df4_ort_raw_data_reg[(df4_ort_raw_data_reg[\"grup\"] == relevant_data[\"grup\"][idx]) & \n",
    "#                                            (df4_ort_raw_data_reg[\"urun\"] == relevant_data[\"urun\"][idx])]\n",
    "\n",
    "#         YUKARIDAKİ COMMENTLİ YERLER ÖRNEK DATALARI İFADE EDİYOR. ORADAN KOPYA ÇEK.\n",
    "\n",
    "        tmp_relevant = relevant_data[(relevant_data[\"grup\"] == data__[\"grup\"][idx]) & \n",
    "                                     (relevant_data[\"urun\"] == data__[\"urun\"][idx])]\n",
    "    # eğer kanal horizon veya btt ise\n",
    "        if tmp[\"Kanal\"].unique()[0] == \"horizon\" or tmp[\"Kanal\"].unique()[0] == \"btt\":\n",
    "            # indirim olmayan case için çalışıyoruz.\n",
    "            tmp_indsiz = tmp[tmp[\"indirim__bins\"] == -1]\n",
    "            # indirim uygulanmamış historik datanın son üç gözlemini liste için alıyoruz.\n",
    "            ort_list = list(tmp_relevant[tmp_relevant[\"indirim__bins\"] == -1].iloc[-3:, :].ytrue)\n",
    "            # filtrelenmiş datanın indekslerini dönüyoruz. 4 aylık prediction olacağı için 4 indexte de dönüyoruz.\n",
    "            for index_ in tmp_indsiz.index:\n",
    "                # aşağıda ort_list içindeki değerlerin ortalamasını alıyoruz. ilkini düşürüp hesaplanan değeri yine listeye ekliyoruz. moving average olmuş oluyor.\n",
    "                tmp_indsiz.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                ort_list.pop(0)\n",
    "                ort_list.append(tmp_indsiz.loc[index_, \"yhat\"])\n",
    "            # tüm indexleri dönünce final_df'e append ediyoruz.\n",
    "            final_df.append(tmp_indsiz)\n",
    "            \n",
    "            # indirim uygulamak istenmesi durumundaki case'ler için çalışıyoruz.\n",
    "            tmp_indli = tmp[tmp[\"indirim__bins\"] != -1]\n",
    "            # eğer indirim uygulanmak istenirse geçmişinde indirim var mı yok mu buna bakmamız gerekiyor. Var ise\n",
    "            if tmp_relevant[\"indirim__bins\"].max() > -1:\n",
    "                for i in tmp_indli.indirim__bins.unique():\n",
    "                    # elimizde toplam 13 tip indirim var. her indirim tipi için 4 gözlem var. o yüzden her bir indirim tipi için döngü olması gerekiyor.\n",
    "                    # ayrıca historik datada sadece indirim uygulanmış gözlemleri alıyoruz. bunların da son 3 ayını ele alıyoruz.\n",
    "                    ort_list = list(tmp_relevant[tmp_relevant[\"indirim__bins\"] != -1].ytrue)\n",
    "                    tmp_indli_specific = tmp_indli[tmp_indli[\"indirim__bins\"] == i] # spesifik indirim tipi filtrelendi. elimizde 4 gözlem var.\n",
    "                    for index_ in tmp_indli_specific.index:\n",
    "                        tmp_indli_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                        if len(ort_list) >= 3:\n",
    "                            # eğer historik datada 3 ve daha fazla indirim uygulanmışsa ilk satırı uçurup yeni hesaplanan değeri append ediyoruz. moving average elde edilmiş oluyor.\n",
    "                            ort_list.pop(0)\n",
    "                            ort_list.append(tmp_indli_specific.loc[index_, \"yhat\"])\n",
    "                        else:\n",
    "                            # eğer historik datada 3'ten az indirim uygulanmışsa, ilk satırı silmiyoruz. böylelikle 3 değer elde edilene kadar devam ediyor.\n",
    "                            ort_list.append(tmp_indli_specific.loc[index_, \"yhat\"])\n",
    "                    final_df.append(tmp_indli_specific)\n",
    "            else:\n",
    "                # eğer indirim uygulanmak istenirse ve geçmişte ilgili sku için herhangi bir indirim uygulanmamışsa;\n",
    "                for i in tmp_indli.indirim__bins.unique():\n",
    "                    # indirim uygulanmamış tüm geçmişi alıyoruz ve sadece son 3 ayını elimizde tutuyoruz.\n",
    "                    ort_list = list(tmp_relevant[tmp_relevant[\"indirim__bins\"] == -1].iloc[-3:, :].ytrue)\n",
    "                    tmp_indli_specific = tmp_indli[tmp_indli[\"indirim__bins\"] == i]\n",
    "                    for index_ in tmp_indli_specific.index:\n",
    "                        tmp_indli_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                        ort_list.pop(0)\n",
    "                        ort_list.append(tmp_indli_specific.loc[index_, \"yhat\"])\n",
    "                    # elde edilen predictionları ilgili kategorinin değişimi ile çarpıyoruz. böylelikle indirim etkisini yakalamış olma ihtimalimiz artıyor.\n",
    "                    try:\n",
    "                        tmp_katsayi = katsayi_df[(katsayi_df[\"Kanal\"] == tmp_indli_specific[\"Kanal\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"kategori_adi\"] == tmp_indli_specific[\"kategori_adi\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"indirim_tipi\"] == tmp_indli_specific[\"indirim_yuzdesi\"].unique()[0])].kac_kati.values[0]\n",
    "                    except:\n",
    "                        tmp_katsayi = katsayi_df[(katsayi_df[\"Kanal\"] == tmp_indli_specific[\"Kanal\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"kategori_adi\"] == tmp_indli_specific[\"kategori_adi\"].unique()[0])].katsayi.values[0]\n",
    "                    tmp_indli_specific[\"yhat\"] *= tmp_katsayi\n",
    "                    final_df.append(tmp_indli_specific)\n",
    "        else:\n",
    "            tmp_aktsiz = tmp[tmp[\"aktivite_tipi\"] == 0]\n",
    "            for i in aktsiz_options.index:\n",
    "                ort_list = list(tmp_relevant[tmp_relevant[\"aktivite_tipi\"] == 0].iloc[-3:, :].ytrue)\n",
    "                tmp_aktsiz_specific = tmp_aktsiz[(tmp_aktsiz[\"aktivite_tipi\"] == aktsiz_options[\"aktivite_tipi\"][i]) & (tmp_aktsiz[\"indirim__bins\"] == aktsiz_options[\"indirim__bins\"][i])]\n",
    "                for index_ in tmp_aktsiz_specific.index:\n",
    "                    tmp_aktsiz_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                    ort_list.pop(0)\n",
    "                    ort_list.append(tmp_aktsiz_specific.loc[index_, \"yhat\"])\n",
    "                final_df.append(tmp_aktsiz_specific)\n",
    "\n",
    "            tmp_aktli = tmp[tmp[\"aktivite_tipi\"] != 0]\n",
    "            if tmp_relevant[\"aktivite_tipi\"].max() > 0:\n",
    "                for i in aktli_options.index:\n",
    "                    ort_list = list(tmp_relevant[tmp_relevant[\"aktivite_tipi\"] != 0].ytrue)\n",
    "                    tmp_akt_specific = tmp_aktli[(tmp_aktli[\"aktivite_tipi\"] == aktli_options[\"aktivite_tipi\"][i]) & (tmp_aktli[\"indirim__bins\"] == aktli_options[\"indirim__bins\"][i])]\n",
    "                    for index_ in tmp_akt_specific.index:\n",
    "                        tmp_akt_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                        if len(ort_list) >= 3:\n",
    "                            ort_list.pop(0)\n",
    "                            ort_list.append(tmp_akt_specific.loc[index_, \"yhat\"])\n",
    "                        else:\n",
    "                            ort_list.append(tmp_akt_specific.loc[index_, \"yhat\"])\n",
    "                    final_df.append(tmp_akt_specific)\n",
    "            else:\n",
    "                for i in aktli_options.index:\n",
    "                    ort_list = list(tmp_relevant[tmp_relevant[\"aktivite_tipi\"] == 0].iloc[-3:, :].ytrue)\n",
    "                    tmp_akt_specific = tmp_aktli[(tmp_aktli[\"aktivite_tipi\"] == aktli_options[\"aktivite_tipi\"][i]) & (tmp_aktli[\"indirim__bins\"] == aktli_options[\"indirim__bins\"][i])]\n",
    "                    for index_ in tmp_akt_specific.index:\n",
    "                        tmp_akt_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                        ort_list.pop(0)\n",
    "                        ort_list.append(tmp_akt_specific.loc[index_, \"yhat\"])\n",
    "                    try:\n",
    "                        tmp_katsayi = katsayi_df[(katsayi_df[\"Kanal\"] == tmp_akt_specific[\"Kanal\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"kategori_adi\"] == tmp_akt_specific[\"kategori_adi\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"indirim_tipi\"] == tmp_akt_specific[\"aktivite_tipi\"].unique()[0])].kac_kati.values[0]\n",
    "                    except:\n",
    "                        try:\n",
    "                            tmp_katsayi = katsayi_df[(katsayi_df[\"Kanal\"] == tmp_akt_specific[\"Kanal\"].unique()[0]) & \n",
    "                                                     (katsayi_df[\"kategori_adi\"] == tmp_akt_specific[\"kategori_adi\"].unique()[0])].katsayi.values[0]\n",
    "                        except:\n",
    "                            tmp_katsayi = 1\n",
    "                    tmp_akt_specific[\"yhat\"] *= tmp_katsayi\n",
    "                    final_df.append(tmp_akt_specific)\n",
    "    final_df = pd.concat(final_df, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_reg_mean_sku_with_combs = mean_prediction_comb(from_regression_mean_june, june_reg_mean_sku_with_combs, df4_ort_raw_data_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series'ten gelen kısım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ts_reg_june = all_results4[(all_results4[\"data_from\"] == \"time series\") & (all_results4[\"winner\"] == \"regression\")]\n",
    "from_ts_mean_june = all_results4[(all_results4[\"data_from\"] == \"time series\") & (all_results4[\"winner\"] == \"mean\")]\n",
    "\n",
    "june_ts_reg_sku = from_ts_reg_june.merge(reg_winner_results.drop(\"mape\", axis=1), how=\"left\")[[\"kanal_l\", \"grup\", \"urun\", \"model_type\"]]\n",
    "june_ts_reg_sku = june_ts_reg_sku.merge(urun_isim_kod, on=\"urun\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ts_reg_june_winner_algorithms = from_ts_reg_june.copy()\n",
    "from_ts_mean_june_winner_algorithms = from_ts_mean_june.copy()\n",
    "from_ts_reg_june_winner_algorithms.rename(columns={\"kanal_l\": \"Kanal\", \"grup\": \"grup_adi\", \"urun\": \"urun_adi\"}, inplace=True)\n",
    "from_ts_mean_june_winner_algorithms.rename(columns={\"kanal_l\": \"Kanal\", \"grup\": \"grup_adi\", \"urun\": \"urun_adi\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku = create_trend_seasonality(june_ts_reg_sku.rename(columns={\"grup\": \"grup_adi\", \"urun\": \"urun_adi\"}))\n",
    "june_ts_reg_sku = june_ts_reg_sku[~(june_ts_reg_sku[\"season\"].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku.rename(columns={\"kanal_l\": \"Kanal\", \"grup\": \"grup_adi\", \"urun\": \"urun_adi\", \"enflasyon\": \"enflasyon_etkisi\"}, inplace=True)\n",
    "june_ts_reg_sku_backup = june_ts_reg_sku.copy()\n",
    "june_ts_reg_sku = june_ts_reg_sku.merge(reg_winner_results[[\"kanal_l\", \"grup\", \"urun\", \"model_type\"]], left_on=[\"Kanal\", \"grup_adi\", \"urun_adi\"], right_on=[\"kanal_l\", \"grup\", \"urun\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ts_reg_june_winners = june_ts_reg_sku[[\"Kanal\", \"grup_adi\", \"urun_adi\", \"model_type\"]].drop_duplicates(subset=[\"Kanal\", \"grup_adi\", \"urun_adi\", \"model_type\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku[\"season\"] = june_ts_reg_sku[\"season\"].astype(float)\n",
    "june_ts_reg_sku[\"trend\"] = june_ts_reg_sku[\"trend\"].astype(float)\n",
    "\n",
    "june_ts_reg_sku = june_ts_reg_sku[X_train.columns.to_list() + [\"model_type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku[\"Kanal\"] = le_kanal.transform(june_ts_reg_sku[\"Kanal\"])\n",
    "june_ts_reg_sku[\"grup_adi\"] = le_grup_adi.transform(june_ts_reg_sku[\"grup_adi\"])\n",
    "june_ts_reg_sku[\"ana_kategori_adi\"] = le_ana_kategori_adi.transform(june_ts_reg_sku[\"ana_kategori_adi\"])\n",
    "june_ts_reg_sku[\"kategori_adi\"] = le_kategori_adi.transform(june_ts_reg_sku[\"kategori_adi\"])\n",
    "june_ts_reg_sku[\"marka_adi\"] = le_marka_adi.transform(june_ts_reg_sku[\"marka_adi\"])\n",
    "june_ts_reg_sku[\"urun_adi\"] = le_urun_adi.transform(june_ts_reg_sku[\"urun_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku[\"yil\"] = june_ts_reg_sku[\"yil\"].astype(\"int16\")\n",
    "june_ts_reg_sku[\"ay\"] = june_ts_reg_sku[\"ay\"].astype(\"int16\")\n",
    "june_ts_reg_sku[\"peak\"] = june_ts_reg_sku[\"peak\"].astype(\"int8\")\n",
    "june_ts_reg_sku[\"indirim__bins\"] = june_ts_reg_sku[\"indirim__bins\"].astype(\"int8\")\n",
    "june_ts_reg_sku[\"aktivite_tipi\"] = june_ts_reg_sku[\"aktivite_tipi\"].astype(\"int8\")\n",
    "june_ts_reg_sku[\"lockdown\"] = june_ts_reg_sku[\"lockdown\"].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku[\"yhat\"] = get_prediction(june_ts_reg_sku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku[\"Kanal\"] = le_kanal.inverse_transform(june_ts_reg_sku[\"Kanal\"])\n",
    "june_ts_reg_sku[\"grup_adi\"] = le_grup_adi.inverse_transform(june_ts_reg_sku[\"grup_adi\"])\n",
    "june_ts_reg_sku[\"ana_kategori_adi\"] = le_ana_kategori_adi.inverse_transform(june_ts_reg_sku[\"ana_kategori_adi\"])\n",
    "june_ts_reg_sku[\"kategori_adi\"] = le_kategori_adi.inverse_transform(june_ts_reg_sku[\"kategori_adi\"])\n",
    "june_ts_reg_sku[\"marka_adi\"] = le_marka_adi.inverse_transform(june_ts_reg_sku[\"marka_adi\"])\n",
    "june_ts_reg_sku[\"urun_adi\"] = le_urun_adi.inverse_transform(june_ts_reg_sku[\"urun_adi\"])\n",
    "june_ts_reg_sku[\"yhat\"] = np.abs(june_ts_reg_sku[\"yhat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku = june_ts_reg_sku.merge(from_ts_reg_june_winner_algorithms, how=\"left\")\n",
    "june_ts_reg_sku = june_ts_reg_sku[~(june_ts_reg_sku[\"mape\"].isna())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#june_ts_reg_sku.to_excel(\"../results/36_12_siparis/time_seriesten_gelen_modeller.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku[\"date\"] = june_ts_reg_sku[\"yil\"].astype(str) + \"-\" + june_ts_reg_sku[\"ay\"].astype(str) + \"-01\"\n",
    "june_ts_reg_sku[\"date\"] = pd.to_datetime(june_ts_reg_sku[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "june_ts_reg_sku[\"ytrue\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_reg_sku = june_ts_reg_sku[june_reg_sku.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_mean_sku_backup = from_ts_mean_june.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_mean_prediction(data):\n",
    "    final_df = []\n",
    "    month_ = params_[\"time_info_for_debugging\"][\"ay\"] \n",
    "    year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "#    month_ = datetime.now().month \n",
    "#    year_ = datetime.now().year\n",
    "    if month_ == 12:\n",
    "        month_ = 1\n",
    "        year+=1\n",
    "    else:\n",
    "        month_+=1\n",
    "\n",
    "    for idx in data.index:\n",
    "        tmp = df4_ort_raw_data_ts[(df4_ort_raw_data_ts[\"grup\"] == data[\"grup\"][idx]) & \n",
    "                                  (df4_ort_raw_data_ts[\"urun\"] == data[\"urun\"][idx])]\n",
    "        grup_ = data[\"grup\"][idx]\n",
    "        urun_ = data[\"urun\"][idx]\n",
    "        ytrue_ = list(tmp.iloc[-6:, -2:-1].ytrue)\n",
    "                \n",
    "        if month_ == 10:\n",
    "            \n",
    "            date_1 = datetime(year_, month_, 1)\n",
    "            date_2 = datetime(year_, month_+1, 1)\n",
    "            date_3 = datetime(year_, month_+2, 1)\n",
    "            date_4 = datetime(year_+1, 2, 1)\n",
    "            \n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_1],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "\n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_2],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "\n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "\n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_3],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            final_df.append(new_df)\n",
    "            \n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_4],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "\n",
    "            final_df.append(new_df)\n",
    "\n",
    "\n",
    "        elif month_ == 11:\n",
    "            \n",
    "            date_1 = datetime(year_, month_, 1)\n",
    "            date_2 = datetime(year_, month_+1, 1)\n",
    "            date_3 = datetime(year_+1, 1, 1)\n",
    "            date_4 = datetime(year_+1, 2, 1)\n",
    "            \n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_1],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "\n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_2],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "\n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "\n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_3],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            final_df.append(new_df)\n",
    "            \n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_4],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "\n",
    "            final_df.append(new_df)\n",
    "            \n",
    "        elif month_ == 12:\n",
    "            \n",
    "            date_1 = datetime(year_, month_, 1)\n",
    "            date_2 = datetime(year_+1, 1, 1)\n",
    "            date_3 = datetime(year_+1, 2, 1)\n",
    "            date_4 = datetime(year_+1, 3, 1)\n",
    "            \n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_1],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "\n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_2],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "\n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_3],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "            \n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_4],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "                          \n",
    "            final_df.append(new_df)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            date_1 = datetime(year_, month_, 1)\n",
    "            date_2 = datetime(year_, month_+1, 1)\n",
    "            date_3 = datetime(year_, month_+2, 1)\n",
    "            date_4 = datetime(year_, month_+3, 1)\n",
    "\n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "                          \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_1],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "\n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "            \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_2],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "                                      \n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "                        \n",
    "                          \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_3],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "            \n",
    "            yprediction = np.mean(ytrue_)\n",
    "            ytrue_.pop(0)\n",
    "            ytrue_.append(yprediction)\n",
    "                          \n",
    "            new_df = pd.DataFrame({\"kanal\": np.nan,\n",
    "                                   \"grup\": [grup_],\n",
    "                                   \"urun\": [urun_],\n",
    "                                   \"date\": [date_4],\n",
    "                                   \"ytrue\": np.nan,\n",
    "                                   \"yhat\": yprediction})\n",
    "            \n",
    "            final_df.append(new_df)\n",
    "    final_df = pd.concat(final_df, ignore_index=True)    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_mean_sku = ts_mean_prediction(from_ts_mean_june)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SADECE SÜTUN İSMİ İÇİN KULLANIYORUM\n",
    "june_ts_mean_sku2 = june_ts_mean_sku.copy()\n",
    "june_ts_mean_sku2.rename(columns={\"urun\": \"urun_adi\", \"grup\": \"grup_adi\"}, inplace=True)\n",
    "june_ts_mean_sku2.drop(\"kanal\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_mean_sku_with_combs = []\n",
    "for idx in june_ts_mean_sku2.index:\n",
    "    tmp = all_combinations[(all_combinations[\"grup_adi\"] == june_ts_mean_sku2[\"grup_adi\"][idx]) & \n",
    "                           (all_combinations[\"grup_adi\"] == june_ts_mean_sku2[\"grup_adi\"][idx])]\n",
    "    tmp = tmp.merge(june_ts_mean_sku2, how=\"left\", on=[\"grup_adi\", \"urun_adi\", \"date\"])\n",
    "    june_ts_mean_sku_with_combs.append(tmp)\n",
    "june_ts_mean_sku_with_combs = pd.concat(june_ts_mean_sku_with_combs, ignore_index=True)\n",
    "june_ts_mean_sku_with_combs = june_ts_mean_sku_with_combs[~(june_ts_mean_sku_with_combs[\"yhat\"].isna())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_mean_sku_with_combs = june_ts_mean_sku_with_combs.merge(from_ts_mean_june_winner_algorithms, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_mean_sku_with_combs[\"model_type\"] = np.nan\n",
    "june_ts_mean_sku_with_combs = june_ts_mean_sku_with_combs[june_reg_sku.columns.to_list()]\n",
    "june_ts_mean_sku_with_combs[\"date\"] = june_ts_mean_sku_with_combs[\"yil\"].astype(str) + \"-\" + june_ts_mean_sku_with_combs[\"ay\"].astype(str) + \"-01\"\n",
    "june_ts_mean_sku_with_combs[\"date\"] = pd.to_datetime(june_ts_mean_sku_with_combs[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "june_ts_mean_sku_with_combs[\"ytrue\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_ort_raw_data_ts = df4_ort_raw_data_ts.merge(df[[\"grup_adi\", \"urun_adi\", \"date\", \"aktivite_tipi\", \"indirim__bins\"]].rename(columns={\"grup_adi\": \"grup\", \"urun_adi\": \"urun\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "june_ts_mean_sku_with_combs = mean_prediction_comb(from_ts_mean_june, june_ts_mean_sku_with_combs, df4_ort_raw_data_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data4 = all_data.copy()\n",
    "all_data4_sku_list = all_data4[[\"grup\", \"urun\"]].drop_duplicates(subset=[\"grup\", \"urun\"]).reset_index(drop=True)\n",
    "\n",
    "data_before_2021 = []\n",
    "for idx in all_data4_sku_list.index:\n",
    "    tmp = all_data[(all_data[\"grup\"] == all_data4_sku_list[\"grup\"][idx]) & \n",
    "                   (all_data[\"urun\"] == all_data4_sku_list[\"urun\"][idx]) & \n",
    "                   (all_data[\"date\"] < (datetime(params_[\"time_info_for_debugging\"][\"yil\"], 1, 1)))]\n",
    "    data_before_2021.append(tmp)\n",
    "data_before_2021 = pd.concat(data_before_2021, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_before_2021.rename(columns={\"kanal\": \"Kanal\", \"grup\": \"grup_adi\", \"urun\": \"urun_adi\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapsam Ortalama Kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ort_history = data_main_.copy()\n",
    "df_ort_history = data_main_[(data_main_[\"portfoy\"] == 1) & (data_main_[\"scope\"] == 1)]\n",
    "df_ort_history.rename(columns={\"adet\": \"ytrue\"}, inplace=True)\n",
    "df_ort_sku = df_ort_history.drop_duplicates(subset=[\"grup_adi\", \"urun_adi\"], ignore_index=True)[[\"grup_adi\", \"urun_adi\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ort_combinations = []\n",
    "for idx in df_ort_sku.index:\n",
    "    tmp_ort = all_combinations[(all_combinations[\"grup_adi\"] == df_ort_sku[\"grup_adi\"][idx]) & \n",
    "                               (all_combinations[\"urun_adi\"] == df_ort_sku[\"urun_adi\"][idx])]\n",
    "    df_ort_combinations.append(tmp_ort)\n",
    "df_ort_combinations = pd.concat(df_ort_combinations, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ort_combinations[\"yhat\"], df_ort_combinations[\"ytrue\"] = np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_calculation(data__, data_main, relevant_data):\n",
    "    \"\"\"\n",
    "    data__ = Unique SKU & Grup isimlerinin yer aldığı dataframe\n",
    "\n",
    "    data_main = Tüm kombinasyonların yer aldığı ilgili data.\n",
    "\n",
    "    relevant_data = Kombinasyonların alındığı ilgili datanın geçmişi. Refere edilen data verilir. \n",
    "    Örn: Regresyon içinse df4_ort_raw_data_reg, Time Series içinse df4_ort_raw_data_ts\n",
    "    \"\"\"\n",
    "    final_df = []\n",
    "    for idx in data__.index:\n",
    "\n",
    "        tmp = data_main[(data_main[\"grup_adi\"] == data__[\"grup_adi\"][idx]) & \n",
    "                        (data_main[\"urun_adi\"] == data__[\"urun_adi\"][idx])]\n",
    "        tmp.drop_duplicates(inplace=True)\n",
    "        tmp_relevant = relevant_data[(relevant_data[\"grup_adi\"] == data__[\"grup_adi\"][idx]) & \n",
    "                                     (relevant_data[\"urun_adi\"] == data__[\"urun_adi\"][idx])]\n",
    "    # eğer kanal horizon veya btt ise\n",
    "        if tmp[\"Kanal\"].unique()[0] == \"horizon\" or tmp[\"Kanal\"].unique()[0] == \"btt\":\n",
    "            # indirim olmayan case için çalışıyoruz.\n",
    "            tmp_indsiz = tmp[tmp[\"indirim__bins\"] == -1]\n",
    "            # indirim uygulanmamış historik datanın son üç gözlemini liste için alıyoruz.\n",
    "            ort_list = list(tmp_relevant[tmp_relevant[\"indirim__bins\"] == -1].iloc[-4:, :].ytrue)\n",
    "            # filtrelenmiş datanın indekslerini dönüyoruz. 4 aylık prediction olacağı için 4 indexte de dönüyoruz.\n",
    "            for index_ in tmp_indsiz.index:\n",
    "                # aşağıda ort_list içindeki değerlerin ortalamasını alıyoruz. ilkini düşürüp hesaplanan değeri yine listeye ekliyoruz. moving average olmuş oluyor.\n",
    "                tmp_indsiz.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                if len(ort_list) >= 3:\n",
    "                    ort_list.pop(0)\n",
    "                    ort_list.append(tmp_indsiz.loc[index_, \"yhat\"])\n",
    "                else:\n",
    "                    ort_list.append(tmp_indsiz.loc[index_, \"yhat\"])\n",
    "            # tüm indexleri dönünce final_df'e append ediyoruz.\n",
    "            final_df.append(tmp_indsiz)\n",
    "\n",
    "            # indirim uygulamak istenmesi durumundaki case'ler için çalışıyoruz.\n",
    "            tmp_indli = tmp[tmp[\"indirim__bins\"] != -1]\n",
    "            # eğer indirim uygulanmak istenirse geçmişinde indirim var mı yok mu buna bakmamız gerekiyor. Var ise\n",
    "            if tmp_relevant[\"indirim__bins\"].max() > -1:\n",
    "                for i in tmp_indli.indirim__bins.unique():\n",
    "                    # elimizde toplam 13 tip indirim var. her indirim tipi için 4 gözlem var. o yüzden her bir indirim tipi için döngü olması gerekiyor.\n",
    "                    # ayrıca historik datada sadece indirim uygulanmış gözlemleri alıyoruz. bunların da son 3 ayını ele alıyoruz.\n",
    "                    ort_list = list(tmp_relevant[tmp_relevant[\"indirim__bins\"] != -1].ytrue)\n",
    "                    tmp_indli_specific = tmp_indli[tmp_indli[\"indirim__bins\"] == i] # spesifik indirim tipi filtrelendi. elimizde 4 gözlem var.\n",
    "                    for index_ in tmp_indli_specific.index:\n",
    "                        tmp_indli_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                        if len(ort_list) >= 3:\n",
    "                            # eğer historik datada 3 ve daha fazla indirim uygulanmışsa ilk satırı uçurup yeni hesaplanan değeri append ediyoruz. moving average elde edilmiş oluyor.\n",
    "                            ort_list.pop(0)\n",
    "                            ort_list.append(tmp_indli_specific.loc[index_, \"yhat\"])\n",
    "                        else:\n",
    "                            # eğer historik datada 3'ten az indirim uygulanmışsa, ilk satırı silmiyoruz. böylelikle 3 değer elde edilene kadar devam ediyor.\n",
    "                            ort_list.append(tmp_indli_specific.loc[index_, \"yhat\"])\n",
    "                    final_df.append(tmp_indli_specific)\n",
    "            else:\n",
    "                # eğer indirim uygulanmak istenirse ve geçmişte ilgili sku için herhangi bir indirim uygulanmamışsa;\n",
    "                for i in tmp_indli.indirim__bins.unique():\n",
    "                    # indirim uygulanmamış tüm geçmişi alıyoruz ve sadece son 3 ayını elimizde tutuyoruz.\n",
    "                    ort_list = list(tmp_relevant[tmp_relevant[\"indirim__bins\"] == -1].iloc[-4:, :].ytrue)\n",
    "                    tmp_indli_specific = tmp_indli[tmp_indli[\"indirim__bins\"] == i]\n",
    "                    for index_ in tmp_indli_specific.index:\n",
    "                        tmp_indli_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                        if len(ort_list) >= 3:\n",
    "                            ort_list.pop(0)\n",
    "                            ort_list.append(tmp_indli_specific.loc[index_, \"yhat\"])\n",
    "                        else:\n",
    "                            ort_list.append(tmp_indli_specific.loc[index_, \"yhat\"])\n",
    "                    # elde edilen predictionları ilgili kategorinin değişimi ile çarpıyoruz. böylelikle indirim etkisini yakalamış olma ihtimalimiz artıyor.\n",
    "                    try:\n",
    "                        tmp_katsayi = katsayi_df[(katsayi_df[\"Kanal\"] == tmp_indli_specific[\"Kanal\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"kategori_adi\"] == tmp_indli_specific[\"kategori_adi\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"indirim_tipi\"] == tmp_indli_specific[\"indirim_yuzdesi\"].unique()[0])].kac_kati.values[0]\n",
    "                    except:\n",
    "                        tmp_katsayi = katsayi_df[(katsayi_df[\"Kanal\"] == tmp_indli_specific[\"Kanal\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"kategori_adi\"] == tmp_indli_specific[\"kategori_adi\"].unique()[0])].katsayi.values[0]\n",
    "                    tmp_indli_specific[\"yhat\"] *= tmp_katsayi\n",
    "                    final_df.append(tmp_indli_specific)\n",
    "        else:\n",
    "            tmp_aktsiz = tmp[tmp[\"aktivite_tipi\"] == 0]\n",
    "            for i in aktsiz_options.index:\n",
    "                ort_list = list(tmp_relevant[tmp_relevant[\"aktivite_tipi\"] == 0].iloc[-4:, :].ytrue)\n",
    "                tmp_aktsiz_specific = tmp_aktsiz[(tmp_aktsiz[\"aktivite_tipi\"] == aktsiz_options[\"aktivite_tipi\"][i]) & (tmp_aktsiz[\"indirim__bins\"] == aktsiz_options[\"indirim__bins\"][i])]\n",
    "                for index_ in tmp_aktsiz_specific.index:\n",
    "                    tmp_aktsiz_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                    if len(ort_list) >= 3:\n",
    "                        ort_list.pop(0)\n",
    "                        ort_list.append(tmp_aktsiz_specific.loc[index_, \"yhat\"])\n",
    "                    else:\n",
    "                        ort_list.append(tmp_aktsiz_specific.loc[index_, \"yhat\"])\n",
    "                final_df.append(tmp_aktsiz_specific)\n",
    "\n",
    "            tmp_aktli = tmp[tmp[\"aktivite_tipi\"] != 0]\n",
    "            if tmp_relevant[\"aktivite_tipi\"].max() > 0:\n",
    "                for i in aktli_options.index:\n",
    "                    ort_list = list(tmp_relevant[tmp_relevant[\"aktivite_tipi\"] != 0].ytrue)\n",
    "                    tmp_akt_specific = tmp_aktli[(tmp_aktli[\"aktivite_tipi\"] == aktli_options[\"aktivite_tipi\"][i]) & (tmp_aktli[\"indirim__bins\"] == aktli_options[\"indirim__bins\"][i])]\n",
    "                    for index_ in tmp_akt_specific.index:\n",
    "                        tmp_akt_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                        if len(ort_list) >= 3:\n",
    "                            ort_list.pop(0)\n",
    "                            ort_list.append(tmp_akt_specific.loc[index_, \"yhat\"])\n",
    "                        else:\n",
    "                            ort_list.append(tmp_akt_specific.loc[index_, \"yhat\"])\n",
    "                    final_df.append(tmp_akt_specific)\n",
    "            else:\n",
    "                for i in aktli_options.index:\n",
    "                    ort_list = list(tmp_relevant[tmp_relevant[\"aktivite_tipi\"] == 0].iloc[-4:, :].ytrue)\n",
    "                    tmp_akt_specific = tmp_aktli[(tmp_aktli[\"aktivite_tipi\"] == aktli_options[\"aktivite_tipi\"][i]) & (tmp_aktli[\"indirim__bins\"] == aktli_options[\"indirim__bins\"][i])]\n",
    "                    for index_ in tmp_akt_specific.index:\n",
    "                        tmp_akt_specific.loc[index_, \"yhat\"] = np.mean(ort_list)\n",
    "                        if len(ort_list) >= 3:\n",
    "                            ort_list.pop(0)\n",
    "                            ort_list.append(tmp_akt_specific.loc[index_, \"yhat\"])\n",
    "                        else:\n",
    "                            ort_list.append(tmp_akt_specific.loc[index_, \"yhat\"])\n",
    "                    try:\n",
    "                        tmp_katsayi = katsayi_df[(katsayi_df[\"Kanal\"] == tmp_akt_specific[\"Kanal\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"kategori_adi\"] == tmp_akt_specific[\"kategori_adi\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"indirim_tipi\"] == tmp_akt_specific[\"aktivite_tipi\"].unique()[0])].kac_kati.values[0]\n",
    "                    except:\n",
    "                        tmp_katsayi = katsayi_df[(katsayi_df[\"Kanal\"] == tmp_akt_specific[\"Kanal\"].unique()[0]) & \n",
    "                                                 (katsayi_df[\"kategori_adi\"] == tmp_akt_specific[\"kategori_adi\"].unique()[0])].katsayi.values[0]\n",
    "                    tmp_akt_specific[\"yhat\"] *= tmp_katsayi\n",
    "                    final_df.append(tmp_akt_specific)\n",
    "    final_df = pd.concat(final_df, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ort_combinations = average_calculation(df_ort_sku, df_ort_combinations, df_ort_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_ = params_[\"time_info_for_debugging\"][\"ay\"] \n",
    "year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "\n",
    "if month_ == 12:\n",
    "    month_ = 1\n",
    "    year_+=1\n",
    "else:\n",
    "    month_+=1\n",
    "new_date = datetime(year_, month_, 1)\n",
    "\n",
    "all_data4.rename(columns={\"kanal\": \"Kanal\", \"grup\": \"grup_adi\", \"urun\": \"urun_adi\"}, inplace=True)\n",
    "all_data4 = all_data4[(all_data4[\"date\"] > new_date - relativedelta(month=6)) & (all_data4[\"date\"] < new_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb1 = june_ts_mean_sku_with_combs[[\"grup_adi\", \"urun_adi\", \"winner\", \"data_from\"]].drop_duplicates(subset=[\"grup_adi\", \"urun_adi\"])\n",
    "comb2 = june_ts_reg_sku[[\"grup_adi\", \"urun_adi\", \"winner\", \"data_from\"]].drop_duplicates(subset=[\"grup_adi\", \"urun_adi\"])\n",
    "comb3 = june_reg_sku[[\"grup_adi\", \"urun_adi\", \"winner\", \"data_from\"]].drop_duplicates(subset=[\"grup_adi\", \"urun_adi\"])\n",
    "comb4 = june_reg_mean_sku_with_combs[[\"grup_adi\", \"urun_adi\", \"winner\", \"data_from\"]].drop_duplicates(subset=[\"grup_adi\", \"urun_adi\"])\n",
    "all_comb_for_winners = pd.concat([comb1, comb2, comb3, comb4], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comb_for_winners.drop(\"data_from\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_before_2021 = data_before_2021.merge(data[[\"yil\", \"ay\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\", \"urun_adi\", \"enflasyon_etkisi\",\n",
    "                                                \"peak\", \"indirim__bins\", \"aktivite_tipi\", \"lockdown\", \"season\", \"trend\", \"date\"]], how=\"left\", on=[\"grup_adi\", \"urun_adi\", \"date\"])\n",
    "\n",
    "data_before_2021[\"model_type\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_before_2021 = data_before_2021.merge(all_comb_for_winners, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_before_2021 = data_before_2021[june_ts_mean_sku_with_combs.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data4 = all_data4.merge(data[[\"yil\", \"ay\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\", \"urun_adi\", \"enflasyon_etkisi\",\n",
    "                                  \"peak\", \"indirim__bins\", \"aktivite_tipi\", \"lockdown\", \"season\", \"trend\", \"date\"]], how=\"left\", on=[\"grup_adi\", \"urun_adi\", \"date\"])\n",
    "\n",
    "all_data4 = all_data4.merge(all_comb_for_winners, how=\"left\")\n",
    "all_data4[\"model_type\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data4 = all_data4[june_ts_mean_sku_with_combs.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june = pd.concat([data_before_2021, all_data4, june_reg_mean_sku_with_combs, june_reg_sku, june_ts_reg_sku, june_ts_mean_sku_with_combs], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_dict = {'Diğer_Horizon': \"horizon\",\n",
    "              'ORTA MARKET': \"horizon\",\n",
    "              'POTANSİYEL MARKET': \"horizon\",\n",
    "              'YEREL ZİNCİR': \"horizon\",\n",
    "              'GELENEKSEL KANAL': \"horizon\",\n",
    "              'A101': \"pasifik\",\n",
    "              'BTT': \"btt\",\n",
    "              'Diğer_Pasifik': \"pasifik\",\n",
    "              'MİGROS': \"pasifik\",\n",
    "              'ŞOK': \"pasifik\",\n",
    "              'BİM': \"pasifik\"}\n",
    "\n",
    "from_ts_june = all_results4[(all_results4[\"data_from\"] == \"time series\") & (all_results4[\"winner\"] == \"time series\")]\n",
    "from_ts_june = from_ts_june[[\"grup\", \"urun\"]].drop_duplicates(subset=[\"grup\", \"urun\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_algorthms_ = all_results4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_algorthms_.rename(columns={\"kanal_l\": \"kanal\",\n",
    "                                  \"grup\": \"grup_adi\",\n",
    "                                  \"urun\": \"urun_adi\",\n",
    "                                  \"winner\": \"winner\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hangi SKU & Grup hangi algoritmadan geldi? Bu dataframe dönüyor. Regresyon ise, hangi winner modelden geldi? Bu ikisini veriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ort_sku[\"kanal\"] = df_ort_sku[\"grup_adi\"].map(kanal_dict)\n",
    "df_ort_sku[\"mape\"], df_ort_sku[\"mape_bins\"] = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ort_winner = df_ort_sku.copy()\n",
    "df_ort_winner = df_ort_winner[[\"kanal\", \"grup_adi\", \"urun_adi\", \"mape\", \"mape_bins\"]]\n",
    "df_ort_winner.rename(columns={\"grup\": \"grup_adi\",\n",
    "                              \"urun\": \"urun_adi\"}, inplace=True)\n",
    "df_ort_winner[\"winner\"] = \"mean\"\n",
    "df_ort_winner[\"data_from\"] = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_algorthms_ = pd.concat([winner_algorthms_, df_ort_winner], axis=0, ignore_index=True)\n",
    "\n",
    "all_reg_winners = pd.concat([from_ts_reg_june_winners, june_reg_sku_winners], ignore_index=True)\n",
    "all_reg_winners.rename(columns={\"Kanal\": \"kanal\"}, inplace=True)\n",
    "\n",
    "winner_algorthms_ = winner_algorthms_.merge(all_reg_winners, on=[\"kanal\", \"grup_adi\", \"urun_adi\"], how=\"left\")\n",
    "urun_isim_kod2 = urun_isim_kod.copy()\n",
    "urun_isim_kod2.rename(columns={\"urun\": \"urun_adi\"}, inplace=True)\n",
    "winner_algorthms_ = winner_algorthms_.merge(urun_isim_kod2, on=\"urun_adi\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_algorthms_ = winner_algorthms_.drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_algorthms_[[\"kanal\", \"grup_adi\", \"urun_adi\", \"winner\", \"model_type\"]].to_excel(\"../results/36_12_siparis/sku_bazinda_winners.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_backup = final_data_with_june.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june.drop(columns=[\"winner\", \"data_from\"], axis=1, inplace=True)\n",
    "winner_algorthms_ = winner_algorthms_.drop_duplicates(subset=[\"grup_adi\", \"urun_adi\"], keep=\"first\", ignore_index=True)\n",
    "winner_algorthms_2 = winner_algorthms_[[\"grup_adi\", \"urun_adi\", \"winner\", \"data_from\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june = final_data_with_june.merge(winner_algorthms_2, how=\"left\")\n",
    "final_data_with_june.drop_duplicates(ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Time Series Kombinasyon Sonuçlarını Alıyoruz\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirim_dictionary = {'MINUS': -1,\n",
    "                       'eight': 8,\n",
    "                       'eleven': 11,\n",
    "                       'five': 5,\n",
    "                       'four': 4,\n",
    "                       'minus': -1,\n",
    "                       'nine': 9,\n",
    "                       'one': 1,\n",
    "                       'seven': 7,\n",
    "                       'six': 6,\n",
    "                       'ten': 10,\n",
    "                       'three': 3,\n",
    "                       'two': 2,\n",
    "                       'zero': 0}\n",
    "\n",
    "aktivite_dictionary = {'FIVE': 5,\n",
    "                       'TWO': 2,\n",
    "                       'ZERO': 0,\n",
    "                       'eight': 8,\n",
    "                       'eleven': 11,\n",
    "                       'five': 5,\n",
    "                       'fourteen': 14,\n",
    "                       'seventeen': 17,\n",
    "                       'two': 2,\n",
    "                       'zero': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_comb_results_backup = ts_comb_results.copy()\n",
    "ts_comb_results[\"data_from\"] = ts_comb_results[\"data_from\"].apply(lambda x: x.replace(\"_01_09_2021\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ts_june_comb_all = []\n",
    "for idx in from_ts_june.index:\n",
    "    tmp = ts_comb_results[(ts_comb_results[\"urun_adi\"] == from_ts_june[\"urun\"][idx]) & \n",
    "                          (ts_comb_results[\"grup_adi\"] == from_ts_june[\"grup\"][idx])]\n",
    "    tmp[\"indirim__bins\"] = tmp[\"data_from\"].apply(lambda x: indirim_dictionary[x.split(\"_\")[2]])\n",
    "    tmp[\"aktivite_tipi\"] = tmp[\"data_from\"].apply(lambda x: aktivite_dictionary[x.split(\"_\")[-1]])\n",
    "    from_ts_june_comb_all.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ts_june_comb_all = pd.concat(from_ts_june_comb_all, ignore_index=True)\n",
    "from_ts_june_comb_all.rename(columns={\"ACTUAL\": \"ytrue\", \"PREDICT\": \"yhat\"}, inplace=True)\n",
    "from_ts_june_comb_all.drop(\"data_from\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations.indirim__bins = all_combinations.indirim__bins.astype(\"int64\")\n",
    "\n",
    "try:\n",
    "    all_combinations.trend = all_combinations.trend.astype(\"int64\")\n",
    "except:\n",
    "    all_combinations.trend.fillna(0, inplace=True)\n",
    "    all_combinations.trend = all_combinations.trend.astype(\"int64\")\n",
    "\n",
    "\n",
    "try:\n",
    "    all_combinations.season = all_combinations.season.astype(\"int64\")\n",
    "except:\n",
    "    all_combinations.season.fillna(0, inplace=True)\n",
    "    all_combinations.season = all_combinations.season.astype(\"int64\")\n",
    "\n",
    "all_combinations.aktivite_tipi = all_combinations.aktivite_tipi.astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ts_june_comb_all = from_ts_june_comb_all.merge(all_combinations[[\"grup_adi\", \"urun_adi\", \"date\", 'ana_kategori_adi', 'ay', \n",
    "                                                                      'enflasyon_etkisi', 'kategori_adi', 'lockdown', 'marka_adi', \n",
    "                                                                      'peak', 'season', 'trend', 'yil', \"indirim__bins\", \"aktivite_tipi\"]], \n",
    "                                                    how=\"left\", \n",
    "                                                    on=[\"grup_adi\", \"urun_adi\", \"date\", \"indirim__bins\", \"aktivite_tipi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ts_june_comb_all = from_ts_june_comb_all[~(from_ts_june_comb_all[\"ana_kategori_adi\"].isna())].reset_index(drop=True)\n",
    "from_ts_june_comb_all[\"model_type\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ts_june_comb_all[\"data_from\"] = \"time series\"\n",
    "from_ts_june_comb_all[\"winner\"] = \"time series\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_ts_june_comb_all = from_ts_june_comb_all[final_data_with_june.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_ = params_[\"time_info_for_debugging\"][\"ay\"] \n",
    "year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "new_date = datetime(year_, month_, 1)\n",
    "\n",
    "idx_to_delete = final_data_with_june[(final_data_with_june[\"date\"] > new_date) & (final_data_with_june[\"winner\"] == \"time series\")].index\n",
    "final_data_with_june = final_data_with_june[~(final_data_with_june.index.isin(idx_to_delete))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_backup = final_data_with_june.copy()\n",
    "final_data_with_june = pd.concat([final_data_with_june, from_ts_june_comb_all], axis=0, ignore_index=True)\n",
    "final_data_with_june.sort_values(by=[\"Kanal\", \"urun_adi\", \"grup_adi\", \"date\"], ascending=[True]*4, ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfoy_lst.rename(columns={\"urun\": \"urun_adi\", \"grup\": \"grup_adi\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfoy_sku = df2[[\"grup_adi\", \"urun_adi\", \"portfoy\"]].drop_duplicates(subset=[\"grup_adi\", \"urun_adi\", \"portfoy\"]).reset_index(drop=True)\n",
    "final_data_with_june = final_data_with_june.merge(portfoy_lst, how=\"left\")\n",
    "final_data_with_june = final_data_with_june[final_data_with_june[\"portfoy\"] == 1].reset_index(drop=True)\n",
    "final_data_with_june2 = final_data_with_june.drop_duplicates(subset=final_data_with_june.columns.to_list(), keep=\"first\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221402, 22)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_with_june2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ts_results_sku.rename(columns={\"grup\": \"grup_adi\", \"urun\": \"urun_adi\", \"kanal_l\": \"Kanal\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "koli_ici_adet = df2[[\"grup_adi\", \"urun_adi\", \"koli_i̇ci_adet\"]].drop_duplicates(subset=[\"grup_adi\", \"urun_adi\"], keep=\"first\", ignore_index=True)\n",
    "\n",
    "final_data_with_june3 = final_data_with_june2.merge(koli_ici_adet, how=\"left\")\n",
    "final_data_with_june3[\"koli\"] = final_data_with_june3[\"yhat\"] / final_data_with_june3[\"koli_i̇ci_adet\"]\n",
    "final_data_with_june_backup = final_data_with_june3.copy()\n",
    "final_data_with_june3 = final_data_with_june3.merge(reg_ts_results_sku[[\"grup_adi\", \"urun_adi\", \"mape\"]], how=\"left\")\n",
    "final_data_with_june4 = final_data_with_june3.copy()\n",
    "final_data_with_june4[\"yhat\"] = np.abs(final_data_with_june4[\"yhat\"])\n",
    "final_data_with_june4[\"mape_for_lower_upper_bound\"] = final_data_with_june4[\"mape\"].apply(lambda x: 100 if x>100 else x)\n",
    "final_data_with_june4[\"lower\"] = final_data_with_june4[\"yhat\"] - ((final_data_with_june4[\"mape_for_lower_upper_bound\"]/100)*final_data_with_june4[\"yhat\"])\n",
    "final_data_with_june4[\"upper\"] = final_data_with_june4[\"yhat\"] + ((final_data_with_june4[\"mape_for_lower_upper_bound\"]/100)*final_data_with_june4[\"yhat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221402, 28)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_with_june4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kontrol = final_data_with_june4[[\"grup_adi\", \"urun_adi\", \"data_from\"]].drop_duplicates()\n",
    "    kontrol = kontrol.groupby([\"grup_adi\", \"urun_adi\"]).count().reset_index()\n",
    "    k = kontrol[kontrol[\"data_from\"] != 1]\n",
    "\n",
    "    kntrl = []\n",
    "    for i in k.index:\n",
    "        kntrl.append(all_results4[(all_results4[\"grup\"] == k[\"grup\"][i]) & \n",
    "                                  (all_results4[\"urun\"] == k[\"urun\"][i])])\n",
    "    kntrl = pd.concat(kntrl, ignore_index=True)\n",
    "\n",
    "    delete_index = []\n",
    "    for idx in kntrl.index:\n",
    "        tmp = list(final_data_with_june4[(final_data_with_june4[\"grup\"] == kntrl[\"grup\"][idx]) & \n",
    "                                         (final_data_with_june4[\"urun\"] == kntrl[\"urun\"][idx]) & \n",
    "                                         (final_data_with_june4[\"data_from\"] == kntrl[\"data_from\"][idx])].index)\n",
    "        delete_index.extend(tmp)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june5 = final_data_with_june4.copy()\n",
    "#final_data_with_june5 = final_data_with_june4[~(final_data_with_june4.index.isin(delete_index))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ort_with_comb = df_ort_combinations.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ort_with_comb[\"model_type\"] = np.nan\n",
    "df_ort_with_comb = df_ort_with_comb.merge(koli_ici_adet, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ort_with_comb[\"upper\"] = df_ort_with_comb[\"yhat\"]\n",
    "df_ort_with_comb[\"lower\"] = df_ort_with_comb[\"yhat\"]\n",
    "df_ort_with_comb[\"koli\"] = df_ort_with_comb[\"koli_i̇ci_adet\"] * df_ort_with_comb[\"yhat\"]\n",
    "df_ort_with_comb[\"mape_for_lower_upper_bound\"] = 1\n",
    "df_ort_with_comb[\"winner\"] = \"mean\"\n",
    "df_ort_with_comb[\"mape\"] = 0\n",
    "df_ort_with_comb[\"model_type\"] = np.nan\n",
    "df_ort_with_comb[\"data_from\"] = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_ort_with_comb = df_ort_with_comb[final_data_with_june5.columns.to_list()]\n",
    "except:\n",
    "    df_ort_with_comb[\"portfoy\"] = 1\n",
    "    df_ort_with_comb = df_ort_with_comb[final_data_with_june5.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june6  = pd.concat([final_data_with_june5, df_ort_with_comb], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june6.drop(columns=[\"winner\", \"data_from\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june6 = final_data_with_june6.merge(winner_algorthms_2, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "urun_isim_kod = df2[[\"urun_adi\", \"en_guncel_kod\"]].drop_duplicates(subset=[\"urun_adi\", \"en_guncel_kod\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june6 = final_data_with_june6.merge(urun_isim_kod, how=\"left\", on=\"urun_adi\")\n",
    "\n",
    "koli_ici_adet2 = df2[[\"grup_adi\", \"en_guncel_kod\", \"koli_i̇ci_adet\"]].drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], keep=\"first\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june7 = final_data_with_june6.copy()\n",
    "final_data_with_june7 = final_data_with_june7.merge(eslenik_kod_df2, on=[\"en_guncel_kod\"], how=\"left\")\n",
    "final_data_with_june7[\"new_urun\"].fillna(final_data_with_june7[\"urun_adi\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_unique_btt = fiyat_unique[fiyat_unique[\"kanal\"] == \"horizon\"]\n",
    "fiyat_unique = pd.concat([fiyat_unique, fiyat_unique_btt], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:fiyat_unique.drop(\"fiyat_gecisi\", axis=1, inplace=True)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_unique.rename(columns={\"fiyat\": \"tl_unit\", \"tl\": \"tl_unit\", \"kg\": \"kg_unit\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_tl_unit = fiyat_unique.merge(koli_birim_agirlik[[\"en_guncel_kod\", \"kg\"]], how=\"left\")\n",
    "kg_tl_unit.drop_duplicates(subset=[\"en_guncel_kod\", \"kanal\"], keep=\"last\", inplace=True, ignore_index=True)\n",
    "kg_tl_unit.rename(columns={\"kg\": \"kg_unit\", \"fiyat\": \"tl_unit\"}, inplace=True)\n",
    "try:\n",
    "    kg_tl_unit.drop(\"fiyat_gecisi\", axis=1, inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_isimleri = {'pasifik': 'PASİFİK', 'horizon': 'HORİZON', 'btt': 'BTT'}\n",
    "kg_tl_unit[\"kanal\"] = kg_tl_unit[\"kanal\"].map(kanal_isimleri)\n",
    "kg_tl_unit.drop(columns=[\"date\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_raw_version = df_all2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "koli_df = df_all_raw_version.drop_duplicates(subset=[\"En Güncel Kod\"], keep=\"last\", ignore_index=True)[[\"En Güncel Kod\", \"Koli İçi Adet\"]]\n",
    "koli_df.rename(columns={\"En Güncel Kod\": \"en_guncel_kod\", \"Koli İçi Adet\": \"koli_ici_adet\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june8 = final_data_with_june7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june8.drop([\"koli\", \"koli_i̇ci_adet\", \"urun_adi\", \"portfoy\"], axis=1, inplace=True)\n",
    "#final_data_with_june8 = final_data_with_june8[final_data_with_june8[\"date\"] != datetime(2021, 7, 1)].reset_index(drop=True)\n",
    "final_data_with_june8 = final_data_with_june8.merge(koli_df, how=\"left\", on=[\"en_guncel_kod\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june8.rename(columns={\"Koli İçi Adet\": \"koli_i̇ci_adet\"}, inplace=True)\n",
    "try:\n",
    "    final_data_with_june8[\"koli\"] = final_data_with_june8[\"yhat\"] / final_data_with_june8[\"koli_i̇ci_adet\"]\n",
    "except:\n",
    "    final_data_with_june8[\"koli\"] = final_data_with_june8[\"yhat\"] / final_data_with_june8[\"koli_ici_adet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_share_with_business_unit(data):\n",
    "    DF = data.copy()\n",
    "    month_ = params_[\"time_info_for_debugging\"][\"ay\"]\n",
    "    year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "    if month_ == 8:\n",
    "        DF = DF[DF[\"date\"] < datetime(year_+1, 1, 1)]\n",
    "    elif month_ == 9:\n",
    "        DF = DF[DF[\"date\"] < datetime(year_+1, 2, 1)]\n",
    "    elif month_ == 10:\n",
    "        DF = DF[DF[\"date\"] < datetime(year_+1, 3, 1)]\n",
    "    elif month_ == 11:\n",
    "        DF = DF[DF[\"date\"] < datetime(year_+1, 4, 1)]\n",
    "    elif month_ == 12:\n",
    "        DF = DF[DF[\"date\"] < datetime(year_+1, 5, 1)]\n",
    "    else:\n",
    "        DF = DF[DF[\"date\"] < datetime(year_, month_+5, 1)]\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june9 = data_to_share_with_business_unit(final_data_with_june8)\n",
    "final_data_with_june_shared_bu = final_data_with_june9.copy()\n",
    "final_data_with_june_shared_bu[\"margin\"] = np.abs(final_data_with_june_shared_bu[\"ytrue\"] - final_data_with_june_shared_bu[\"yhat\"])\n",
    "final_data_with_june_shared_bu.drop([\"margin\", \"mape_for_lower_upper_bound\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_birimi_col_dict = {\"kanal\": \"Kanal\", \n",
    "                      \"grup_adi\": \"Grup Adı\",\n",
    "                      \"date\": \"Tarih\",\n",
    "                      \"ytrue\": \"Gerçekleşen Satış\",\n",
    "                      \"yhat\": \"Tahmin\",\n",
    "                      \"mape\": \"MAPE\",\n",
    "                      \"lower\": \"Tahmin Alt Sınır\",\n",
    "                      \"upper\": \"Tahmin Üst Sınır\",\n",
    "                      \"en_guncel_kod\": \"En Güncel Kod\",\n",
    "                      \"new_urun\": \"Ürün Adı\",\n",
    "                      \"koli_ici_adet\": \"Koli İçi Adet (Tahminde Kullanılan)\",\n",
    "                      \"koli_i̇ci_adet\": \"Koli İçi Adet (Tahminde Kullanılan)\",\n",
    "                      \"koli\": \"Tahmin Koli\",\n",
    "                      \"ana_kategori_adi\": \"Ana Kategori Adı\",\n",
    "                      \"kategori_adi\": \"Kategori Adı\",\n",
    "                      \"marka_adi\": \"Marka Adı\",\n",
    "                      \"indirim__bins\": \"İndirim Yüzdesi\",\n",
    "                      \"aktivite_tipi\": \"Aktivite Tipi\",\n",
    "                      \"winner\": \"Tahmin Edilme Yöntemi\",\n",
    "                      \"data_from\": \"Datanın Etiketlendiği Algoritma\"}\n",
    "\n",
    "final_data_with_june_shared_bu.rename(columns=is_birimi_col_dict, inplace=True)\n",
    "\n",
    "final_data_with_june_shared_bu2 = final_data_with_june_shared_bu.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_shared_bu2 = final_data_with_june_shared_bu2[[\"Tarih\", \"Kanal\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\", \n",
    "                                                                   \"Ürün Adı\", \"En Güncel Kod\", \"Gerçekleşen Satış\", \"Tahmin\", \"Tahmin Alt Sınır\", \n",
    "                                                                   \"Tahmin Üst Sınır\", \"Koli İçi Adet (Tahminde Kullanılan)\", \"Tahmin Koli\", \"MAPE\", \n",
    "                                                                   \"İndirim Yüzdesi\", \"Aktivite Tipi\", \"Tahmin Edilme Yöntemi\", \"Datanın Etiketlendiği Algoritma\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "koli_df_history = df2[[\"grup_adi\", \"en_guncel_kod\", \"date\", \"koli\", \"koli_i̇ci_adet\"]].drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\", \"date\", \"koli\", \"koli_i̇ci_adet\"], ignore_index=True)\n",
    "koli_df_history.rename(columns={\"grup_adi\": \"Grup Adı\",\n",
    "                               \"en_guncel_kod\": \"En Güncel Kod\",\n",
    "                               \"date\": \"Tarih\",\n",
    "                               \"koli\": \"Gerçekleşen Koli\",\n",
    "                               \"koli_i̇ci_adet\": \"Koli İçi Adet (Gerçekleşen Satıştaki)\"}, inplace=True)\n",
    "\n",
    "koli_df_history[\"Tarih\"] = pd.to_datetime(koli_df_history[\"Tarih\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_shared_bu2 = final_data_with_june_shared_bu2.merge(koli_df_history, how=\"left\", on=[\"Tarih\", \"Grup Adı\", \"En Güncel Kod\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_shared_bu2 = final_data_with_june_shared_bu2[[\"Tarih\", \"Kanal\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\", \n",
    "                                                                   \"Ürün Adı\", \"En Güncel Kod\", \"Gerçekleşen Satış\", \"Tahmin\", \"Tahmin Alt Sınır\", \n",
    "                                                                   \"Tahmin Üst Sınır\", \"Koli İçi Adet (Tahminde Kullanılan)\", \"Tahmin Koli\", \"MAPE\",\n",
    "                                                                   \"Koli İçi Adet (Gerçekleşen Satıştaki)\", \"Gerçekleşen Koli\", \"İndirim Yüzdesi\", \n",
    "                                                                   \"Aktivite Tipi\", \"Tahmin Edilme Yöntemi\", \"Datanın Etiketlendiği Algoritma\"]]\n",
    "\n",
    "final_data_with_june_shared_bu2[\"Tahmin Koli\"] = np.ceil(final_data_with_june_shared_bu2[\"Tahmin Koli\"])\n",
    "final_data_with_june_shared_bu2.sort_values(by=[\"En Güncel Kod\", \"Grup Adı\", \"Tarih\"], ignore_index=True, inplace=True)\n",
    "\n",
    "final_data_with_june_shared_bu2.rename(columns={\"Gerçekleşen Satış\": \"Gerçekleşen Satış Adet\",\n",
    "                                               \"Tahmin\": \"Tahmin Adet\",\n",
    "                                               \"Tahmin Alt Sınır\": \"Tahmin Adet (Alt Sınır)\",\n",
    "                                               \"Tahmin Üst Sınır\": \"Tahmin Adet (Üst Sınır)\",\n",
    "                                               \"Koli İçi Adet (Tahminde Kullanılan)\": \"Koli İçi Adet\"}, inplace=True)\n",
    "\n",
    "final_data_with_june_shared_bu2.drop(\"Koli İçi Adet (Gerçekleşen Satıştaki)\", axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all2_backup = df_all2.copy()\n",
    "#### TÜM GEÇMİŞ DATANIN OLDUĞU EXCEL'LERDEN OKUNUP İŞLEM YAPILAN DATANIN BACKUP'I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_all2 = df_all2[[\"Date\", \"Grup adı\", \"En Güncel Kod\", \"Koli\", \"Adet\"]]\n",
    "except:\n",
    "    df_all2 = df_all2[[\"Date\", \"Grup Adı\", \"En Güncel Kod\", \"Koli\", \"Adet\"]]\n",
    "    df_all2.rename(columns={\"Grup Adı\": \"Grup adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_shared_bu3 = final_data_with_june_shared_bu2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all2.rename(columns={\"Date\": \"Tarih\", \"Grup adı\": \"Grup Adı\"}, inplace=True)\n",
    "\n",
    "cols_order = final_data_with_june_shared_bu3.columns.to_list()\n",
    "final_data_with_june_shared_bu3.drop(columns=[\"Gerçekleşen Satış Adet\", \"Gerçekleşen Koli\"], axis=1, inplace=True)\n",
    "\n",
    "final_data_with_june_shared_bu4 = final_data_with_june_shared_bu3.merge(df_all2, on=[\"Tarih\", \"Grup Adı\", \"En Güncel Kod\"], how=\"left\")\n",
    "final_data_with_june_shared_bu4.rename(columns={\"Koli\": \"Gerçekleşen Koli\", \"Adet\": \"Gerçekleşen Satış Adet\"}, inplace=True)\n",
    "final_data_with_june_shared_bu4 = final_data_with_june_shared_bu4[cols_order]\n",
    "#final_data_with_june_shared_bu4.to_excel(\"../results/36_12_siparis/Demand Sensing Satış Tahmin Datası (4 aylık).xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_ = params_[\"time_info_for_debugging\"][\"ay\"] \n",
    "year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "\n",
    "if month_ == 12:\n",
    "    month_ = 1\n",
    "    year_+=1\n",
    "else:\n",
    "    month_+=1\n",
    "new_date = datetime(year_, month_, 1)\n",
    "\n",
    "final_data_with_june_shared_with_akt = final_data_with_june_shared_bu4.copy()\n",
    "final_data_with_june_shared_with_akt = final_data_with_june_shared_with_akt[final_data_with_june_shared_with_akt[\"Tarih\"] >= new_date].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "aktivite_dict_reverse = {0: \"Yok\",\n",
    "                         2: 'Mağaza içi/Dağılım',\n",
    "                         5: 'In&Out',\n",
    "                         8: 'Çoklu Alım',\n",
    "                         11: 'Mutluluk',\n",
    "                         14: 'Kasiyer',\n",
    "                         17: 'CRM'}\n",
    "\n",
    "indirim_reverse = {-1: \"0\",\n",
    "                   0: \"%0 - %1\",\n",
    "                   1: \"%1 - %2\",\n",
    "                   2: \"%2 - %3\",\n",
    "                   3: \"%3 - %4\",\n",
    "                   4: \"%4 - %5\",\n",
    "                   5: \"%5 - %6\",\n",
    "                   6: \"%6 - %7\",\n",
    "                   7: \"%7 - %8\",\n",
    "                   8: \"%8 - %9\",\n",
    "                   9: \"%9 - %10\",\n",
    "                   10: \"%10 - %15\",\n",
    "                   11: \"%15 - max\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_shared_with_akt[\"İndirim Yüzdesi\"] = final_data_with_june_shared_with_akt[\"İndirim Yüzdesi\"].map(indirim_reverse)\n",
    "final_data_with_june_shared_with_akt[\"Aktivite Tipi\"] = final_data_with_june_shared_with_akt[\"Aktivite Tipi\"].map(aktivite_dict_reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_shared_with_akt2 = final_data_with_june_shared_with_akt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "urun_isim_kod3 = df2[[\"grup_adi\", \"urun_adi\", \"en_guncel_kod\"]].drop_duplicates(subset=[\"grup_adi\", \"urun_adi\", \"en_guncel_kod\"], ignore_index=True)\n",
    "urun_isim_kod3.rename(columns={\"grup_adi\": \"Grup Adı\"}, inplace=True)\n",
    "urun_isim_kod3.rename(columns={\"urun\": \"urun_adi\", \"en_guncel_kod\": \"En Güncel Kod\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_shared_with_akt4 = final_data_with_june_shared_with_akt2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SADECE PREDICTIONLAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar = final_data_with_june_shared_with_akt4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_threshold = datetime(params_[\"time_info_for_debugging\"][\"yil\"], params_[\"time_info_for_debugging\"][\"ay\"], 1)\n",
    "#date_threshold = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_delete = list(final_data_with_june_sadece_predictionlar[(final_data_with_june_sadece_predictionlar[\"Tarih\"] > date_threshold) & \n",
    "                                                               (final_data_with_june_sadece_predictionlar[\"Tarih\"] < date_threshold + relativedelta(months=5))].index)\n",
    "final_data_with_june_sadece_predictionlar = final_data_with_june_sadece_predictionlar[(final_data_with_june_sadece_predictionlar.index.isin(idx_to_delete))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar['Tahmin Sapması'] = np.abs(final_data_with_june_sadece_predictionlar['Tahmin Koli'] - \n",
    "                                                                     final_data_with_june_sadece_predictionlar['Gerçekleşen Koli'])/final_data_with_june_sadece_predictionlar['Tahmin Koli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar2 = final_data_with_june_sadece_predictionlar.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "sutun_isimleri = {'Tarih':'tarih',\n",
    "                  'Kanal': 'kanal',\n",
    "                  'Grup Adı': 'grup_adi',\n",
    "                  'Ana Kategori Adı': 'ana_kategori_adi',\n",
    "                  'Kategori Adı': 'kategori_adi',\n",
    "                  'Marka Adı': 'marka_adi',\n",
    "                  'Ürün Adı': 'urun_adi',\n",
    "                  'En Güncel Kod': 'en_guncel_kod',\n",
    "                  'Gerçekleşen Satış Adet': 'gerceklesen_satis_adedi',\n",
    "                  'Tahmin Adet': 'tahmin_adet',\n",
    "                  'Tahmin Adet (Alt Sınır)': 'tahmin_ust_sinir',\n",
    "                  'Tahmin Adet (Üst Sınır)': 'tahmin_alt_sinir',\n",
    "                  'Koli İçi Adet': 'koli_ici_adet',\n",
    "                  'Tahmin Koli': 'tahmin_koli',\n",
    "                  'Gerçekleşen Koli': 'gerceklesen_koli',\n",
    "                  'İndirim Yüzdesi': 'indirim_yuzdesi',\n",
    "                  'Aktivite Tipi': 'aktivite_tipi',\n",
    "                  'Tahmin Edilme Yöntemi': 'tahmin_edilme_yontemi',\n",
    "                  'Datanın Etiketlendiği Algoritma': 'datanin_etiketlendigi_algoritma',\n",
    "                  'MAPE': 'mape',\n",
    "                  'Tahmin Sapması': \"tahmin_sapmasi\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar2.rename(columns=sutun_isimleri, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar2[\"kanal\"] = final_data_with_june_sadece_predictionlar2[\"kanal\"].map(kanal_isimleri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar2 = final_data_with_june_sadece_predictionlar2.merge(kg_tl_unit, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar2[\"kg\"] = final_data_with_june_sadece_predictionlar2[\"tahmin_koli\"] * final_data_with_june_sadece_predictionlar2[\"kg_unit\"]\n",
    "final_data_with_june_sadece_predictionlar2[\"tl\"] = final_data_with_june_sadece_predictionlar2[\"tahmin_koli\"] * final_data_with_june_sadece_predictionlar2[\"tl_unit\"]\n",
    "\n",
    "final_data_with_june_sadece_predictionlar2[\"mape_for_lower_upper_bound\"] = final_data_with_june_sadece_predictionlar2[\"mape\"].apply(lambda x: 100 if x>100 else x)\n",
    "final_data_with_june_sadece_predictionlar2[\"tahmin_koli_alt_sinir\"] = final_data_with_june_sadece_predictionlar2[\"tahmin_koli\"] - ((final_data_with_june_sadece_predictionlar2[\"mape_for_lower_upper_bound\"]/100)*final_data_with_june_sadece_predictionlar2[\"tahmin_koli\"])\n",
    "final_data_with_june_sadece_predictionlar2[\"tahmin_koli_ust_sinir\"] = final_data_with_june_sadece_predictionlar2[\"tahmin_koli\"] + ((final_data_with_june_sadece_predictionlar2[\"mape_for_lower_upper_bound\"]/100)*final_data_with_june_sadece_predictionlar2[\"tahmin_koli\"])\n",
    "\n",
    "final_data_with_june_sadece_predictionlar2.drop(columns=[\"mape_for_lower_upper_bound\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in final_data_with_june_sadece_predictionlar2.select_dtypes(exclude=[\"datetime64[ns]\", \"object\"]).columns:\n",
    "    try:\n",
    "        if col == \"kg\":\n",
    "            final_data_with_june_sadece_predictionlar2[col] = np.round(final_data_with_june_sadece_predictionlar2[col], 3)\n",
    "        elif col == \"tl\":\n",
    "            final_data_with_june_sadece_predictionlar2[col] = np.round(final_data_with_june_sadece_predictionlar2[col], 2)\n",
    "        else:\n",
    "            final_data_with_june_sadece_predictionlar2[col] = np.round(final_data_with_june_sadece_predictionlar2[col])\n",
    "            final_data_with_june_sadece_predictionlar2[col].fillna(0, inplace=True)\n",
    "            final_data_with_june_sadece_predictionlar2[col] = final_data_with_june_sadece_predictionlar2[col].astype(int)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TÜM GRUPLAR TÜM MARKALAR vs yap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar3 = final_data_with_june_sadece_predictionlar2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar3.rename(columns={\"datanin_etiketlendigi_algoritma\": \"datanin_etiketlendigi_algorit\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar3[\"from\"] = \"row\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_all = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "kanal_all[\"grup_adi\"] = \"TÜM GRUPLAR\"\n",
    "kanal_all[\"ana_kategori_adi\"] = \"TÜM ANA KATEGORİLER\"\n",
    "kanal_all[\"kategori_adi\"] = \"TÜM KATEGORİLER\"\n",
    "kanal_all[\"marka_adi\"] = \"TÜM MARKALAR\"\n",
    "kanal_all[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "kanal_all[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "kanal_all[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "kanal_all[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#kanal_all[\"aktivite_etkisi\"] = \"Var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_all[\"from\"] = \"kanal_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "grup_all = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "grup_all[\"ana_kategori_adi\"] = \"TÜM ANA KATEGORİLER\"\n",
    "grup_all[\"kategori_adi\"] = \"TÜM KATEGORİLER\"\n",
    "grup_all[\"marka_adi\"] = \"TÜM MARKALAR\"\n",
    "grup_all[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "grup_all[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "grup_all[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "grup_all[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#grup_all[\"aktivite_etkisi\"] = \"Var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "grup_all[\"from\"] = \"grup_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "kontrol = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).agg({\"kategori_adi\": \"nunique\"}).reset_index()\n",
    "idx_delete = kontrol[kontrol[\"kategori_adi\"] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_kategori_all = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "ana_kategori_all[\"kategori_adi\"] = \"TÜM KATEGORİLER\"\n",
    "ana_kategori_all[\"marka_adi\"] = \"TÜM MARKALAR\"\n",
    "ana_kategori_all[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "ana_kategori_all[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "ana_kategori_all[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "ana_kategori_all[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#ana_kategori_all[\"aktivite_etkisi\"] = \"Var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_kategori_all = ana_kategori_all[~(ana_kategori_all.index.isin(idx_delete))]\n",
    "ana_kategori_all[\"from\"] = \"ana_kategori_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "kontrol = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \n",
    "                                                              \"kategori_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).agg({\"marka_adi\": \"nunique\"}).reset_index()\n",
    "idx_delete = kontrol[kontrol[\"marka_adi\"] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "kategori_all = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \n",
    "                                                                   \"kategori_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "\n",
    "kategori_all[\"marka_adi\"] = \"TÜM MARKALAR\"\n",
    "kategori_all[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "kategori_all[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "kategori_all[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "kategori_all[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#kategori_all[\"aktivite_etkisi\"] = \"Var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "kategori_all = kategori_all[~(kategori_all.index.isin(idx_delete))]\n",
    "kategori_all[\"from\"] = \"kategori_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "kontrol = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \n",
    "                                                              \"kategori_adi\", \"marka_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).agg({\"en_guncel_kod\": \"nunique\"}).reset_index()\n",
    "idx_delete = kontrol[kontrol[\"en_guncel_kod\"] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "marka_all = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \n",
    "                                                                \"kategori_adi\", \"marka_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "\n",
    "marka_all[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "marka_all[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "marka_all[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "marka_all[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#marka_all[\"aktivite_etkisi\"] = \"Var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "marka_all = marka_all[~(marka_all.index.isin(idx_delete))]\n",
    "marka_all[\"from\"] = \"marka_all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiyerarşik detay kırılımları ile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "grup_detail = []\n",
    "for idx in final_data_with_june_sadece_predictionlar3.grup_adi.unique():\n",
    "    tmp = final_data_with_june_sadece_predictionlar3[final_data_with_june_sadece_predictionlar3[\"grup_adi\"] == idx]\n",
    "    tmp = tmp.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "    tmp[\"ana_kategori_adi\"] = \"TÜM ANA KATEGORİLER\"\n",
    "    tmp[\"kategori_adi\"] = \"TÜM KATEGORİLER\"\n",
    "    tmp[\"marka_adi\"] = \"TÜM MARKALAR\"\n",
    "    tmp[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "    tmp[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "    tmp[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "    tmp[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#    tmp[\"aktivite_etkisi\"] = \"Var\"\n",
    "    grup_detail.append(tmp)\n",
    "grup_detail = pd.concat(grup_detail, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "grup_detail[\"from\"] = \"grup_detail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_kategori_detail = []\n",
    "for idx in final_data_with_june_sadece_predictionlar3.ana_kategori_adi.unique():\n",
    "    tmp = final_data_with_june_sadece_predictionlar3[final_data_with_june_sadece_predictionlar3[\"ana_kategori_adi\"] == idx]\n",
    "    kontrol = tmp.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).agg({\"kategori_adi\": \"nunique\"}).reset_index().kategori_adi.unique()[0]\n",
    "    tmp = tmp.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "    if kontrol == 1:\n",
    "        pass\n",
    "    else:\n",
    "        tmp[\"kategori_adi\"] = \"TÜM KATEGORİLER\"\n",
    "        tmp[\"marka_adi\"] = \"TÜM MARKALAR\"\n",
    "        tmp[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "        tmp[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "        tmp[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "        tmp[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#        tmp[\"aktivite_etkisi\"] = \"Var\"\n",
    "        ana_kategori_detail.append(tmp)\n",
    "ana_kategori_detail = pd.concat(ana_kategori_detail, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_kategori_detail[\"from\"] = \"ana_kategori_detail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = final_data_with_june_sadece_predictionlar3.drop_duplicates(subset=[\"kanal\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\"])[[\"kanal\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\"]].reset_index(drop=True)\n",
    "kategori_detail = []\n",
    "for idx in liste.index:\n",
    "    tmp = final_data_with_june_sadece_predictionlar3[(final_data_with_june_sadece_predictionlar3[\"kanal\"] == liste[\"kanal\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"grup_adi\"] == liste[\"grup_adi\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"ana_kategori_adi\"] == liste[\"ana_kategori_adi\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"kategori_adi\"] == liste[\"kategori_adi\"][idx])]\n",
    "    kontrol = len(tmp[\"marka_adi\"].unique())\n",
    "    if kontrol == 1:\n",
    "        pass\n",
    "    else:\n",
    "        tmp = tmp.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "        tmp[\"marka_adi\"] = \"TÜM MARKALAR\"\n",
    "        tmp[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "        tmp[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "        tmp[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "        tmp[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#        tmp[\"aktivite_etkisi\"] = \"Var\"\n",
    "        kategori_detail.append(tmp)\n",
    "kategori_detail = pd.concat(kategori_detail, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "kategori_detail[\"from\"] = \"kategori_detail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = final_data_with_june_sadece_predictionlar3.drop_duplicates(subset=[\"kanal\", \"grup_adi\", \"ana_kategori_adi\", \n",
    "                                                                           \"kategori_adi\", \"marka_adi\"])[[\"kanal\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\"]].reset_index(drop=True)\n",
    "marka_detail = []\n",
    "for idx in liste.index:\n",
    "    tmp = final_data_with_june_sadece_predictionlar3[(final_data_with_june_sadece_predictionlar3[\"kanal\"] == liste[\"kanal\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"grup_adi\"] == liste[\"grup_adi\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"ana_kategori_adi\"] == liste[\"ana_kategori_adi\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"kategori_adi\"] == liste[\"kategori_adi\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"marka_adi\"] == liste[\"marka_adi\"][idx])]\n",
    "    kontrol = len(tmp[\"en_guncel_kod\"].unique())\n",
    "    if kontrol == 1:\n",
    "        pass\n",
    "    else:\n",
    "        tmp = tmp.groupby([\"tarih\", \"kanal\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "        tmp[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "        tmp[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "        tmp[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "        tmp[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#        tmp[\"aktivite_etkisi\"] = \"Var\"\n",
    "        marka_detail.append(tmp)\n",
    "marka_detail = pd.concat(marka_detail, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "marka_detail[\"from\"] = \"marka_detail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "tum_gruplar_ana_kategori = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"ana_kategori_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "tum_gruplar_ana_kategori[\"grup_adi\"] = \"TÜM GRUPLAR\"\n",
    "tum_gruplar_ana_kategori[\"kategori_adi\"] = \"TÜM KATEGORİLER\"\n",
    "tum_gruplar_ana_kategori[\"marka_adi\"] = \"TÜM MARKALAR\"\n",
    "tum_gruplar_ana_kategori[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "tum_gruplar_ana_kategori[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "tum_gruplar_ana_kategori[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "tum_gruplar_ana_kategori[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#tum_gruplar_ana_kategori[\"aktivite_etkisi\"] = \"Var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "tum_gruplar_ana_kategori[\"from\"] = \"tum_gruplar_ana_kategori\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "tum_gruplar_kategori = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"ana_kategori_adi\", \"kategori_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "tum_gruplar_kategori[\"grup_adi\"] = \"TÜM GRUPLAR\"\n",
    "tum_gruplar_kategori[\"marka_adi\"] = \"TÜM MARKALAR\"\n",
    "tum_gruplar_kategori[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "tum_gruplar_kategori[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "tum_gruplar_kategori[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "tum_gruplar_kategori[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#tum_gruplar_kategori[\"aktivite_etkisi\"] = \"Var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "tum_gruplar_kategori[\"from\"] = \"tum_gruplar_kategori\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = final_data_with_june_sadece_predictionlar3.drop_duplicates(subset=[\"kanal\", \"ana_kategori_adi\", \n",
    "                                                                           \"kategori_adi\", \"marka_adi\"])[[\"kanal\", \"ana_kategori_adi\", \n",
    "                                                                                                          \"kategori_adi\", \"marka_adi\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "tum_gruplar_marka = []\n",
    "for idx in liste.index:\n",
    "    tmp = final_data_with_june_sadece_predictionlar3[(final_data_with_june_sadece_predictionlar3[\"kanal\"] == liste[\"kanal\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"ana_kategori_adi\"] == liste[\"ana_kategori_adi\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"kategori_adi\"] == liste[\"kategori_adi\"][idx]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"marka_adi\"] == liste[\"marka_adi\"][idx])]\n",
    "    kontrol = len(tmp[\"en_guncel_kod\"].unique())\n",
    "    if kontrol == 1:\n",
    "        pass\n",
    "    else:\n",
    "        tmp = tmp.groupby([\"tarih\", \"kanal\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "        tmp[\"grup_adi\"] = \"TÜM GRUPLAR\"\n",
    "        tmp[\"en_guncel_kod\"] = \"TÜM SKULAR\"\n",
    "        tmp[\"urun_adi\"] = \"TÜM ÜRÜNLER\"\n",
    "        tmp[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "        tmp[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#        tmp[\"aktivite_etkisi\"] = \"Var\"\n",
    "        tum_gruplar_marka.append(tmp)\n",
    "tum_gruplar_marka = pd.concat(tum_gruplar_marka, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "tum_gruplar_marka[\"from\"] = \"tum_gruplar_marka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "tum_gruplar_en_guncel_kod = final_data_with_june_sadece_predictionlar3.groupby([\"tarih\", \"kanal\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\", \"en_guncel_kod\", \"urun_adi\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).sum().reset_index()\n",
    "tum_gruplar_en_guncel_kod[\"grup_adi\"] = \"TÜM GRUPLAR\"\n",
    "tum_gruplar_en_guncel_kod[\"tahmin_edilme_yontemi\"] = \"N/A\"\n",
    "tum_gruplar_en_guncel_kod[\"datanin_etiketlendigi_algorit\"] = \"N/A\"\n",
    "#tum_gruplar_en_guncel_kod[\"aktivite_etkisi\"] = \"Var\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "tum_gruplar_en_guncel_kod[\"from\"] = \"tum_gruplar_en_guncel_kod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = final_data_with_june_sadece_predictionlar3.groupby([\"grup_adi\", \"en_guncel_kod\", \"tarih\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).count().reset_index()\n",
    "kntrl = chk[chk[\"kanal\"] > 1].drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\", \"tarih\", \"indirim_yuzdesi\", \"aktivite_tipi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "urun_adi__ = []\n",
    "grup_adi__ = []\n",
    "data_from__ = []\n",
    "winner__ = []\n",
    "idx_to_delete = []\n",
    "for index_ in kntrl.index:\n",
    "    tmp = final_data_with_june_sadece_predictionlar3[(final_data_with_june_sadece_predictionlar3[\"en_guncel_kod\"] == kntrl[\"en_guncel_kod\"][index_]) &\n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"grup_adi\"] == kntrl[\"grup_adi\"][index_]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"tarih\"] == kntrl[\"tarih\"][index_]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"indirim_yuzdesi\"] == kntrl[\"indirim_yuzdesi\"][index_]) & \n",
    "                                                     (final_data_with_june_sadece_predictionlar3[\"aktivite_tipi\"] == kntrl[\"aktivite_tipi\"][index_])]\n",
    "    try:\n",
    "        tmp[\"datanin_etiketlendigi_algoritma\"]\n",
    "        if tmp[\"datanin_etiketlendigi_algoritma\"].unique()[0] == \"time series\" and tmp[\"tahmin_edilme_yontemi\"].unique()[0] == \"mean\":\n",
    "            idx_to_delete.extend(tmp.iloc[0:1].index)\n",
    "        elif tmp[\"datanin_etiketlendigi_algoritma\"].unique()[0] == \"regression\" and tmp[\"tahmin_edilme_yontemi\"].unique()[0] == \"mean\":\n",
    "            idx_to_delete.extend(tmp.iloc[-1:].index)\n",
    "        else:\n",
    "            pass\n",
    "    except KeyError:\n",
    "        if tmp[\"datanin_etiketlendigi_algorit\"].unique()[0] == \"time series\" and tmp[\"tahmin_edilme_yontemi\"].unique()[0] == \"mean\":\n",
    "            idx_to_delete.extend(tmp.iloc[0:1].index)\n",
    "        elif tmp[\"datanin_etiketlendigi_algorit\"].unique()[0] == \"regression\" and tmp[\"tahmin_edilme_yontemi\"].unique()[0] == \"mean\":\n",
    "            idx_to_delete.extend(tmp.iloc[-1:].index)\n",
    "        else:\n",
    "            pass        \n",
    "#    urun_adi__.extend(tmp[\"urun_adi\"].values)\n",
    "#    grup_adi__.extend(tmp[\"grup_adi\"].values)\n",
    "#    data_from__.extend(tmp[\"datanin_etiketlendigi_algoritma\"].values)\n",
    "#    winner__.extend(tmp[\"tahmin_edilme_yontemi\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar3 = final_data_with_june_sadece_predictionlar3[~(final_data_with_june_sadece_predictionlar3.index.isin(idx_to_delete))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = final_data_with_june_sadece_predictionlar3.groupby([\"grup_adi\", \"en_guncel_kod\", \"tarih\", \"indirim_yuzdesi\", \"aktivite_tipi\"]).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "kntrl = chk[chk[\"kanal\"] > 1].drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    final_data_with_june_sadece_predictionlar3.drop(\"date\", axis=1, inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "kanal_all = kanal_all[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "grup_all = grup_all[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "ana_kategori_all = ana_kategori_all[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "kategori_all = kategori_all[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "marka_all = marka_all[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "grup_detail = grup_detail[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "ana_kategori_detail = ana_kategori_detail[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "kategori_detail = kategori_detail[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "marka_detail = marka_detail[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "tum_gruplar_ana_kategori = tum_gruplar_ana_kategori[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "tum_gruplar_kategori = tum_gruplar_kategori[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "tum_gruplar_marka = tum_gruplar_marka[final_data_with_june_sadece_predictionlar3.columns.to_list()]\n",
    "tum_gruplar_en_guncel_kod = tum_gruplar_en_guncel_kod[final_data_with_june_sadece_predictionlar3.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar4 = pd.concat([final_data_with_june_sadece_predictionlar3,\n",
    "                                                        kanal_all,\n",
    "                                                        grup_all,\n",
    "                                                        ana_kategori_all,\n",
    "                                                        kategori_all,\n",
    "                                                        marka_all,\n",
    "                                                        grup_detail,\n",
    "                                                        ana_kategori_detail,\n",
    "                                                        kategori_detail,\n",
    "                                                        marka_detail,\n",
    "                                                        tum_gruplar_ana_kategori,\n",
    "                                                        tum_gruplar_kategori,\n",
    "                                                        tum_gruplar_marka,\n",
    "                                                        tum_gruplar_en_guncel_kod], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar4.drop_duplicates(subset=final_data_with_june_sadece_predictionlar4.columns.to_list(), \n",
    "                                                           ignore_index=True, \n",
    "                                                           inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar4.columns = [i[:-2] if len(i) > 30 else i for i in final_data_with_june_sadece_predictionlar4.columns]\n",
    "final_data_with_june_sadece_predictionlar4[\"en_guncel_kod\"] = final_data_with_june_sadece_predictionlar4[\"en_guncel_kod\"].apply(lambda x: \"TÜM SKULAR\" if x == \"TÜM SKU'LAR\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_ = params_[\"time_info_for_debugging\"][\"ay\"] \n",
    "year_ = params_[\"time_info_for_debugging\"][\"yil\"]\n",
    "\n",
    "if month_ == 12:\n",
    "    month_ = 1\n",
    "    year_+=1\n",
    "else:\n",
    "    month_+=1\n",
    "\n",
    "date_threshold = datetime(year_, month_, 1)\n",
    "#date_threshold = datetime(params_[\"time_info_for_debugging\"][\"yil\"], params_[\"time_info_for_debugging\"][\"ay\"], 1) + relativedelta(momths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data = final_data_with_june_shared_bu4[final_data_with_june_shared_bu4[\"Tarih\"] < date_threshold].reset_index(drop=True)\n",
    "grup_and_sku_list = final_data_with_june_sadece_predictionlar4.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], ignore_index=True)[[\"grup_adi\", \"en_guncel_kod\"]]\n",
    "grup_and_sku_list = grup_and_sku_list[grup_and_sku_list[\"en_guncel_kod\"] != \"TÜM SKULAR\"].reset_index(drop=True)\n",
    "\n",
    "sas_raporu_icin_data = final_data_with_june_shared_bu4.copy()\n",
    "sas_raporu_icin_data_raw = sas_raporu_icin_data[(sas_raporu_icin_data[\"Tarih\"] < date_threshold)]\n",
    "sas_raporu_icin_data_comb = sas_raporu_icin_data[(sas_raporu_icin_data[\"Tarih\"] >= date_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar4[\"aktivite_tipi\"] = final_data_with_june_sadece_predictionlar4[\"aktivite_tipi\"].apply(lambda x: \"In-out\" if x==\"In&Out\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar4.columns = [i[:-2] if len(i) > 30 else i for i in final_data_with_june_sadece_predictionlar4.columns]\n",
    "final_data_with_june_sadece_predictionlar4[\"en_guncel_kod\"] = final_data_with_june_sadece_predictionlar4[\"en_guncel_kod\"].apply(lambda x: \"TÜM SKULAR\" if x == \"TÜM SKU'LAR\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    final_data_with_june_sadece_predictionlar4.drop(\"aktivite_etkisi\", axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "grup_and_sku_list = final_data_with_june_sadece_predictionlar4.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], ignore_index=True)[[\"grup_adi\", \"en_guncel_kod\"]]\n",
    "final_data_with_june_sadece_predictionlar4[\"aktivite_etkisi\"] = np.nan\n",
    "for idx in grup_and_sku_list.index:\n",
    "    akt = historic_data[(historic_data[\"Grup Adı\"] == grup_and_sku_list[\"grup_adi\"][idx]) & \n",
    "                        (historic_data[\"En Güncel Kod\"] == grup_and_sku_list[\"en_guncel_kod\"][idx])][\"Aktivite Tipi\"].unique()\n",
    "    ind = historic_data[(historic_data[\"Grup Adı\"] == grup_and_sku_list[\"grup_adi\"][idx]) & \n",
    "                        (historic_data[\"En Güncel Kod\"] == grup_and_sku_list[\"en_guncel_kod\"][idx])][\"İndirim Yüzdesi\"].unique()\n",
    "    indexes = final_data_with_june_sadece_predictionlar4[(final_data_with_june_sadece_predictionlar4[\"grup_adi\"] == grup_and_sku_list[\"grup_adi\"][idx]) & \n",
    "                                                         (final_data_with_june_sadece_predictionlar4[\"en_guncel_kod\"] == grup_and_sku_list[\"en_guncel_kod\"][idx])].index\n",
    "    if (len(akt) == 1 and len(ind) == 1) and (akt[0] == 0 and ind[0] == -1):\n",
    "        final_data_with_june_sadece_predictionlar4.loc[list(indexes), \"aktivite_etkisi\"] = \"Yok\"\n",
    "    else:\n",
    "        final_data_with_june_sadece_predictionlar4.loc[list(indexes), \"aktivite_etkisi\"] = \"Var\"\n",
    "final_data_with_june_sadece_predictionlar4.aktivite_etkisi.fillna(\"Var\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar4[\"aktivite_tipi\"] = final_data_with_june_sadece_predictionlar4[\"aktivite_tipi\"].apply(lambda x: \"In-out\" if x==\"In&Out\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    final_data_with_june_sadece_predictionlar4.drop(columns=\"from\", axis=1, inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_june_sadece_predictionlar4[\"en_guncel_kod\"] = final_data_with_june_sadece_predictionlar4[\"en_guncel_kod\"].apply(lambda x: int(x) if \"T\" not in str(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table DEMAND_SENSING_RESULTS in caslib DSENS_T.\n",
      "NOTE: The table DEMAND_SENSING_RESULTS has been created in caslib DSENS_T from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>DSENS_T</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>DEMAND_SENSING_RESULTS</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('DEMAND_SENSING_RESULTS', caslib='DSENS_T')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 136s</span> &#183; <span class=\"cas-user\">user 3.54s</span> &#183; <span class=\"cas-sys\">sys 1.2s</span> &#183; <span class=\"cas-memory\">mem 82.7MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'DSENS_T'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'DEMAND_SENSING_RESULTS'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('DEMAND_SENSING_RESULTS', caslib='DSENS_T')\n",
       "\n",
       "+ Elapsed: 136s, user: 3.54s, sys: 1.2s, mem: 82.7mb"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: \n",
    "    conn.userinfo()\n",
    "except: \n",
    "    print(\"SWAT bağlantısı koptu. Bağlantı tekrar kuruluyor.\")\n",
    "    access_ = True\n",
    "    while access_:\n",
    "        try:\n",
    "            conn = swat.CAS('yhtrcl-sasccnt1.yildiz.domain', 5570, username=params_[\"login_info\"][\"username\"], password=params_[\"login_info\"][\"password\"])\n",
    "            access_ = False\n",
    "            print(\"Bağlantı kuruldu!\")\n",
    "        except:\n",
    "            print(\"Beklenmedik bir hata oluştu. Bağlantı tekrar kuruluyor...\")\n",
    "            continue\n",
    "ds_table = conn.CASTable(\"DEMAND_SENSING_RESULTS\", caslib=params_[\"caslib_info\"][\"caslib_name\"], replace=True)\n",
    "ds_table.table.dropTable(quiet=True)\n",
    "conn.upload(data=final_data_with_june_sadece_predictionlar4, \n",
    "            casout={'caslib':params_[\"caslib_info\"][\"caslib_name\"], 'name':\"DEMAND_SENSING_RESULTS\", 'promote':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"python ../pyviyatools/submit_jobdef.py -id {}\".format(params_[\"sas_job\"][\"dw_upload_job_id\"]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "final_data_with_june_sadece_predictionlar4[(final_data_with_june_sadece_predictionlar4[\"kanal\"] == \"HORİZON\") & \n",
    "                                           (~(final_data_with_june_sadece_predictionlar4[\"urun_adi\"].str.contains(\"TÜM\"))) & \n",
    "                                           (~(final_data_with_june_sadece_predictionlar4[\"grup_adi\"].str.contains(\"TÜM\")))].to_excel(\"../../../../Desktop/horizon_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "final_data_with_june_sadece_predictionlar4[(final_data_with_june_sadece_predictionlar4[\"kanal\"] == \"PASİFİK\") & \n",
    "                                           (~(final_data_with_june_sadece_predictionlar4[\"urun_adi\"].str.contains(\"TÜM\"))) & \n",
    "                                           (~(final_data_with_june_sadece_predictionlar4[\"grup_adi\"].str.contains(\"TÜM\")))].to_excel(\"../../../../Desktop/pasifik_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "final_data_with_june_sadece_predictionlar4.to_excel(\"../../../../Desktop/all_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historik dataya ihtiyaç duyulursa"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "historic_data.to_csv(\"../data/_historic_data_for_graph.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sigma_1_75 = pd.DataFrame(conn.CASTable(caslib=\"CKLMSZ17\",name=\"DEMAND_SENSING_RESULTS\").to_frame())\n",
    "sigma_1_5 = pd.DataFrame(conn.CASTable(caslib=\"CKLMSZ15\",name=\"DEMAND_SENSING_RESULTS\").to_frame())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sigma_1_75.to_excel(\"../../../sigma_1_75_results.xlsx\", index=False)\n",
    "sigma_1_5.to_excel(\"../../../sigma_1_5_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sonuçları aktivitelere göre filtrelemek için aşağıdaki kod bloklarını kullanıyoruz\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hor_aktivite = pd.read_excel(\"../data/../../../Demand_Sensing_Saha_Aktivite_2021_Ekim.xlsx\", skiprows=1, usecols=\"B:M\")\n",
    "pas_aktivite = pd.read_excel(\"../data/../../../Ekim 2021 Pasifik Aktivite Datası.xlsx\")\n",
    "pas_aktivite = pas_aktivite[[\"Yıl\", \"Ay\", \"Müşteri Grup\", \"Ürün Kodu\", \"İndirim %\", \"Aktivite Tipi\"]]\n",
    "hor_aktivite = hor_aktivite[[\"Yıl\", \"Ay\", \"Saha Müşteri Grup\", \"Ürün Kodu\", \"İskonto %\"]]\n",
    "\n",
    "chng_col_names = {\"Yıl\": \"yil\", \"Ay\": \"ay\", \"Müşteri Grup\": \"grup_adi\", \"Saha Müşteri Grup\": \"grup_adi\", \"Ürün Kodu\": \"en_guncel_kod\", \n",
    "                 \"İndirim %\": \"indirim\", \"İskonto %\": \"indirim\", \"Aktivite Tipi\": \"aktivite_tipi\"}\n",
    "pas_aktivite.rename(columns=chng_col_names, inplace=True)\n",
    "hor_aktivite.rename(columns=chng_col_names, inplace=True)\n",
    "hor_aktivite[\"aktivite_tipi\"] = \"Yok\"\n",
    "pas_aktivite[\"grup_adi\"] = pas_aktivite[\"grup_adi\"].apply(lambda x: \"Diğer_Pasifik\" if x == \"Diğer\" else x)\n",
    "hor_aktivite[\"grup_adi\"] = hor_aktivite[\"grup_adi\"].apply(lambda x: \"Diğer_Horizon\" if x == \"Diğer\" else x)\n",
    "\n",
    "hor_aktivite[\"indirim\"] = hor_aktivite[\"indirim\"].apply(lambda x: 0 if x == \"#DIV/0\" else x)\n",
    "pas_aktivite[\"indirim\"] = pas_aktivite[\"indirim\"].apply(lambda x: 0 if x == \"#DIV/0\" else x)\n",
    "hor_aktivite[\"indirim\"] = hor_aktivite[\"indirim\"].astype(float)\n",
    "pas_aktivite[\"indirim\"] = pas_aktivite[\"indirim\"].astype(float)\n",
    "\n",
    "pas_aktivite[\"indirim\"] = [0 if akt < 0 else akt for akt in pas_aktivite[\"indirim\"]]\n",
    "hor_aktivite[\"indirim\"] = [0 if ((akt >= 0.35) or (akt <=0.01)) else akt for akt in hor_aktivite[\"indirim\"]]\n",
    "\n",
    "pas_aktivite[\"tarih\"] = pd.to_datetime(pas_aktivite[\"yil\"].astype(str) + \"-\" + pas_aktivite[\"ay\"].astype(str) + \"-01\")\n",
    "hor_aktivite[\"tarih\"] = pd.to_datetime(hor_aktivite[\"yil\"].astype(str) + \"-\" + hor_aktivite[\"ay\"].astype(str) + \"-01\")\n",
    "\n",
    "pas_aktivite = pas_aktivite[pas_aktivite[\"tarih\"] > datetime(2021, 9, 1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "indirim_reverse = {-1: \"0\",\n",
    "                   0: \"%0 - %1\",\n",
    "                   1: \"%1 - %2\",\n",
    "                   2: \"%2 - %3\",\n",
    "                   3: \"%3 - %4\",\n",
    "                   4: \"%4 - %5\",\n",
    "                   5: \"%5 - %6\",\n",
    "                   6: \"%6 - %7\",\n",
    "                   7: \"%7 - %8\",\n",
    "                   8: \"%8 - %9\",\n",
    "                   9: \"%9 - %10\",\n",
    "                   10: \"%10 - %15\",\n",
    "                   11: \"%15 - max\"}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "akt_all = pd.concat([pas_aktivite, hor_aktivite], axis=0, ignore_index=True)\n",
    "akt_all[\"aktivite_tipi\"] = akt_all[\"aktivite_tipi\"].apply(lambda x: \"In-Out\" if x == \"İn&out\" else x)\n",
    "\n",
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, akt_all.indirim.max()+1]\n",
    "akt_all['indirim__bins'] = pd.cut(akt_all.indirim, indirim_bins).cat.codes\n",
    "akt_all[\"indirim_yuzdesi_new\"] = akt_all[\"indirim__bins\"].map(indirim_reverse)\n",
    "akt_all[\"aktivite_tipi_new\"] = akt_all[\"aktivite_tipi\"]\n",
    "akt_all.drop([\"yil\", \"ay\", \"indirim__bins\", \"indirim\", \"aktivite_tipi\"], axis=1, inplace=True)\n",
    "akt_all.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\", \"tarih\"], ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "akt_all"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis = final_data_with_june_sadece_predictionlar4.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis = data_analysis[~((data_analysis[\"urun_adi\"].str.contains(\"TÜM\") | (data_analysis[\"grup_adi\"].str.contains(\"TÜM\"))))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis2[(data_analysis2[\"en_guncel_kod\"] == 34500)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis2 = data_analysis.merge(akt_all, on=[\"grup_adi\", \"en_guncel_kod\", \"tarih\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis2_akt = data_analysis2[~(data_analysis2[\"aktivite_tipi_new\"].isna())]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx_all = []\n",
    "for idx in data_analysis2_akt.index:\n",
    "    if data_analysis2_akt.loc[idx, \"aktivite_tipi_new\"] == data_analysis2_akt.loc[idx, \"aktivite_tipi\"] and data_analysis2_akt.loc[idx, \"indirim_yuzdesi\"] == data_analysis2_akt.loc[idx, \"indirim_yuzdesi_new\"]:\n",
    "        idx_all.append(idx)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis2_akt = data_analysis2_akt[data_analysis2_akt.index.isin(idx_all)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis2_baz = data_analysis2[(data_analysis2[\"aktivite_tipi_new\"].isna())]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis2_baz = data_analysis2_baz[(data_analysis2_baz[\"aktivite_tipi\"] == \"Yok\")  & \n",
    "                                        (data_analysis2_baz[\"indirim_yuzdesi\"] == \"0\")]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis3 = pd.concat([data_analysis2_akt, data_analysis2_baz], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_analysis3.to_excel(\"../../../ds_results_ekim.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "akt_all[(akt_all[\"grup_adi\"] == \"Diğer_Horizon\") & (akt_all[\"en_guncel_kod\"] == 9503)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hor_aktivite[(hor_aktivite[\"grup_adi\"] == \"Diğer_Horizon\") & (hor_aktivite[\"en_guncel_kod\"] == 9503)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data_new = data_main_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data_new = historic_data_new[[\"date\", \"Kanal\", \"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\", \"urun_adi\", \n",
    "                                       \"en_guncel_kod\", \"adet\", \"koli_i̇ci_adet\", \"koli\", \"indirim__bins\", \"aktivite_tipi\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data_new[\"Tahmin Adet\"], historic_data_new[\"Tahmin Adet (Alt Sınır)\"], historic_data_new[\"Tahmin Adet (Üst Sınır)\"], historic_data_new[\"Tahmin Edilme Yöntemi\"], historic_data_new[\"Datanın Etiketlendiği Algoritma\"] = np.nan, np.nan, np.nan, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data_new[\"Tahmin Koli\"], historic_data_new[\"MAPE\"] = np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data_new.rename(columns={\"date\": \"Tarih\",\n",
    "                                  \"grup_adi\": \"Grup Adı\",\n",
    "                                  \"ana_kategori_adi\": \"Ana Kategori Adı\",\n",
    "                                  \"kategori_adi\": \"Kategori Adı\",\n",
    "                                  \"marka_adi\": \"Marka Adı\",\n",
    "                                  \"urun_adi\": \"Ürün Adı\",\n",
    "                                  \"en_guncel_kod\": \"En Güncel Kod\",\n",
    "                                  \"adet\": \"Gerçekleşen Satış Adet\",\n",
    "                                  \"koli_i̇ci_adet\": \"Koli İçi Adet\",\n",
    "                                  \"koli\": \"Gerçekleşen Koli\",\n",
    "                                  \"indirim__bins\": \"İndirim Yüzdesi\",\n",
    "                                  \"aktivite_tipi\": \"Aktivite Tipi\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data_new = historic_data_new[historic_data.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_data_new.to_csv(\"../data/_historic_data_for_graph_v2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
