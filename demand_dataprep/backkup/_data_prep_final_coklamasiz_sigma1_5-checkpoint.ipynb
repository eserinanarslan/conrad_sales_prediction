{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tunahan.aktas\\Anaconda3\\lib\\site-packages\\mpl_toolkits\\mplot3d\\__init__.py:1: MatplotlibDeprecationWarning: \n",
      "The deprecated function was deprecated in Matplotlib 3.4 and will be removed two minor releases later.\n",
      "  from .axes3d import Axes3D\n"
     ]
    }
   ],
   "source": [
    "# Prior libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "\n",
    "# To Get Combinatiobs\n",
    "import itertools\n",
    "\n",
    "# Datetime Libraries\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "# Trend Seasonality\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SAS Connection Library\n",
    "import swat\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_all_process = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Excel Files\n",
    "btt_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Demand Sensing Sales History\") or doc_.startswith(\"Demand_Sensing_Sales_\")])\n",
    "hor_lst = sorted([doc_ for doc_ in os.listdir(\"../data/08092021\") if doc_.startswith(\"Horizon_Saha_\") ])\n",
    "pas_lst = sorted([doc_ for doc_ in os.listdir(\"../data/08092021\") if doc_.startswith(\"Siparişe_göre_Sales_History\")])\n",
    "saha_aktivite_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Demand_Sensing_Saha_Aktivit\") or doc_.startswith(\"Demand Sensing Saha Aktivit\")])\n",
    "pasifik_aktivite_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Pasifik Aktivite Datası\")])\n",
    "fiyat_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if \"Fiyat List\" in doc_])\n",
    "portfoy_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Portföy\")])\n",
    "eslenik_kod_lst = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Ürün Eşlenik kodlar\")])\n",
    "kapsam_listeli = sorted([doc_ for doc_ in os.listdir(\"../data\") if doc_.startswith(\"Listeli Ürün\")])\n",
    "pas_siparis_lst = sorted([doc_ for doc_ in os.listdir(\"../data/siparis\") if doc_.startswith(\"Siparişe_göre_Sales_History\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dosya Listelerini Okuma İşlemi: 0:00:00.022006\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Dosya Listelerini Okuma İşlemi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Read Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historik data ve koli içi adetlerini okuyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales Data\n",
    "pasifik_df_all = []\n",
    "btt_df_all = []\n",
    "horizon_saha_df_all = []\n",
    "chng_cols_beginning = {'Year': 'Yıl', 'Quarter': 'Çeyrek', 'Month': 'Ay', \n",
    "                       'Company Code': 'Şirket Kodu', 'Main Category Name': 'Ana Kategori Adı', \n",
    "                       'Category Name': 'Kategori Adı', 'Brand Name': 'Marka Adı', 'Product Code': 'Ürün Kodu', \n",
    "                       'Product Name': 'Ürün Adı', \"Ürün Adı (Mobis)\": 'Ürün Adı'}\n",
    "for docs_ in btt_lst:\n",
    "    btt_df_all.append(pd.read_excel(\"../data/\"+docs_, skiprows=1, sheet_name=\"BTT SAP Satış\", usecols=\"B:N\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "for docs_ in hor_lst:\n",
    "    horizon_saha_df_all.append(pd.read_excel(\"../data/08092021/\"+docs_, skiprows=1, sheet_name=\"Horizon Saha Satış\", usecols=\"B:L\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "for docs_ in pas_lst:\n",
    "    pasifik_df_all.append(pd.read_excel(\"../data/08092021/\"+docs_, sheet_name=\"Ürün Bazlı\", usecols=\"B:O\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "# Pasifik ve Horizon historic datasında koli içi adetini bir önceki versiyon olan excelin içerisinden okuyoruz. Bu yüzden siparis klasörünün içindeki dosyalarda gezeceğiz.\n",
    "pasifik_df_koli_ici_adet = [] \n",
    "for docs_ in pas_siparis_lst:\n",
    "    pasifik_df_koli_ici_adet.append(pd.read_excel(\"../data/siparis/\"+docs_, skiprows=1, sheet_name=\"Koli içi adet\", usecols=\"B:D\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "horizon_df_eski = [] \n",
    "for docs_ in btt_lst:\n",
    "    horizon_df_eski.append(pd.read_excel(\"../data/\"+docs_, skiprows=1, sheet_name=\"Horizon Saha Satış\", usecols=\"B:N\").rename(columns=chng_cols_beginning))\n",
    "\n",
    "pasifik_df_all = pd.concat(pasifik_df_all)\n",
    "btt_df_all = pd.concat(btt_df_all)\n",
    "horizon_saha_df_all = pd.concat(horizon_saha_df_all)\n",
    "\n",
    "pasifik_df_koli_ici_adet = pd.concat(pasifik_df_koli_ici_adet)\n",
    "horizon_df_eski = pd.concat(horizon_df_eski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historik datanın Jupytere yüklenme süresi: 0:09:08.234051\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Historik datanın Jupytere yüklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sadece Gerekli Sütunlar Tutuluyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all.drop(columns=[\"Organizasyon Kodu\", \"Grup Kodu.\", \"Pladis-Non Pladis\"], axis=1, inplace=True)\n",
    "horizon_saha_df_all.drop(columns=[\"Çeyrek\"], axis=1, inplace=True)\n",
    "btt_df_all.drop(columns=[\"Çeyrek\", \"Şirket Kodu\"], axis=1, inplace=True)\n",
    "\n",
    "btt_df_all[\"Grup Adı\"] = \"BTT\"\n",
    "pasifik_df_all.rename(columns={\"Ana Kategori\": \"Ana Kategori Adı\", \"Kategori\": \"Kategori Adı\", \"Ürün Adı (Orjinal)\": \"Ürün Adı\", \"Sipariş Miktarı(Dönüş. Koli)\": \"Koli\", \n",
    "                               \"Sipariş Brüt Tutar\": \"KG\", \"Sipariş Brüt KG\": \"TL\"}, inplace=True)\n",
    "\n",
    "horizon_saha_df_all.rename(columns={\"Horizon müşteri grup\": \"Grup Adı\", \"Ürün Adı (Orjinal)\": \"Ürün Adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik için koli içi adetler \"Koli içi adet\" sheetinde tutuluyor. Buradan alıyoruz fakat historik datada bulunmayan ürün kodları da var. Bu ürün kodlarından bazıları alfabetik harfler içeriyor. Bu durumu ortadan kaldırmak için aşağıdaki işlemi uyguluyoruz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = ['Yıl', 'Ay', 'Grup Adı', 'Ana Kategori Adı', 'Kategori Adı', 'Marka Adı', 'Ürün Kodu', 'Ürün Adı', 'Koli İçi Adet', 'Koli', 'KG', 'TL']\n",
    "ltrs = list(string.ascii_letters) # Alfabede bulunan tüm harfleri tutan liste. Bunu, koli içi adet dataframe'deki harf içeren ürün kodlarını elemek için tutuyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_koli_ici_adet.drop_duplicates(subset=pasifik_df_koli_ici_adet.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True) # Her dosyada 300k'ya yakın satır var. Yaklaşık 2M kadar satır geliyor çünkü 5-6 yıllık data okuyoruz. Bu yüzden drop duplicates ile satır sayısını azaltıyoruz.\n",
    "pasifik_df_koli_ici_adet = pasifik_df_koli_ici_adet[~(pasifik_df_koli_ici_adet[\"Ürün Kodu\"].str.contains(\"|\".join(ltrs), regex=True))] # Alfabetik harflerle başlayan ürün kodlarını elemine ediyoruz.\n",
    "\n",
    "pasifik_df_koli_ici_adet[\"Ürün Kodu\"] = pasifik_df_koli_ici_adet[\"Ürün Kodu\"].str.replace(\" \", \"\") # Bazı ürün kodları 0015 01 şeklinde gelmiş. Yani nümerik gözükse de arada boşluk var. O yüzden boşlukları kaldırıyoruz.\n",
    "pasifik_df_koli_ici_adet[\"Ürün Kodu\"] = pasifik_df_koli_ici_adet[\"Ürün Kodu\"].astype(\"int64\") # Bir üst satırda boşlukları kaldırdıktan sonra integer hale getiriyoruz.\n",
    "pasifik_df_koli_ici_adet = pasifik_df_koli_ici_adet[[\"Ürün Kodu\", \"Koli İçi Adet\"]] # Bu iki satır kalabilir. Left join ile historik dataya ekleyeceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all.merge(pasifik_df_koli_ici_adet, on=\"Ürün Kodu\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Kısmı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her dosyada; Ürün Adı sütununda \"sum:\" diye bir gözlem bulunuyor. Dip toplam yapıp datayı atmışlar. Bu satırları sildim.\n",
    "horizon_saha_df_all = horizon_saha_df_all[~(horizon_saha_df_all[\"Ürün Adı\"].str.contains(\"Sum:|sum:\"))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_kategori = horizon_df_eski[['Ürün Kodu','Kategori Adı']]\n",
    "horizon_kategori.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "horizon_df_koli_ici_adet = horizon_df_eski[['Yıl','Ay','Ürün Kodu','Koli İçi Adet']]\n",
    "horizon_df_koli_ici_adet.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all = horizon_saha_df_all.merge(horizon_kategori, how='left', on='Ürün Kodu')\n",
    "horizon_saha_df_all = horizon_saha_df_all.merge(horizon_df_koli_ici_adet, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[col_order]\n",
    "horizon_saha_df_all = horizon_saha_df_all[col_order]\n",
    "btt_df_all = btt_df_all[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Saha Aktiviteleri\n",
    "saha_aktivite_detay = []\n",
    "saha_aktivite_cat = []\n",
    "\n",
    "for docs_ in saha_aktivite_lst:\n",
    "    saha_aktivite_detay.append(pd.read_excel(\"../data/\"+docs_, skiprows=1, sheet_name=\"Ürün Detay\", usecols=\"B:M\"))\n",
    "    saha_aktivite_cat.append(pd.read_excel(\"../data/\"+docs_, skiprows=1, sheet_name=\"Kategori\", usecols=\"B:I\"))\n",
    "saha_aktivite_detay = pd.concat(saha_aktivite_detay)\n",
    "saha_aktivite_cat = pd.concat(saha_aktivite_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Pasifik Aktiviteleri\n",
    "pasifik_aktivite_df = pd.read_excel(\"../data/\"+pasifik_aktivite_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Fiyat Listesi\n",
    "fiyat_lst_pasifik = pd.read_excel(\"../data/\"+fiyat_lst[0])\n",
    "fiyat_lst_horizon = pd.read_excel(\"../data/\"+fiyat_lst[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Portföy\n",
    "pasifik_portfoy_df = pd.read_excel(\"../data/\"+portfoy_lst[0], sheet_name=\"Pasifik Portföy\", skiprows=3, usecols=\"D:H\")\n",
    "btt_portfoy_df = pd.read_excel(\"../data/\"+portfoy_lst[0], sheet_name=\"BTT Portföy\", skiprows=2, usecols=\"D:H\")\n",
    "horizon_portfoy_df = pd.read_excel(\"../data/\"+portfoy_lst[0], sheet_name=\"Horizon Portföy\", skiprows=2, usecols=\"E:I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Eşlenik Kodları\n",
    "eslenik_kod_df = pd.read_excel(\"../data/\"+eslenik_kod_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Calender\n",
    "calender_df = pd.read_excel(\"../data/Calender_Monthly.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Plasiyer Yarışma\n",
    "yarisma_df = pd.read_excel(\"../data/2018-2021 Yarışmaları_v2.xlsx\", sheet_name=\"yarisma_historik_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eslenik_kod_df[\"En Güncel Kod\"] = eslenik_kod_df[\"En Güncel Kod\"].apply(lambda x: int(x) if x not in ['delist ', \"delist\"] else x.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a101_kapsam = pd.read_excel(\"../data/\"+kapsam_listeli[0], sheet_name=\"A101 Portföy\")\n",
    "sok_kapsam = pd.read_excel(\"../data/\"+kapsam_listeli[0], sheet_name=\"Şok Portföy\")\n",
    "bim_kapsam = pd.read_excel(\"../data/\"+kapsam_listeli[0], sheet_name=\"Bim Portföy\")\n",
    "\n",
    "a101_kapsam[\"grup_adi\"] = \"A101\"\n",
    "sok_kapsam[\"grup_adi\"] = \"ŞOK\"\n",
    "bim_kapsam[\"grup_adi\"] = \"BİM\"\n",
    "\n",
    "kapsam_all = pd.concat([a101_kapsam, sok_kapsam, bim_kapsam], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Enflasyon\n",
    "enflasyon_df = pd.read_excel(\"../data/enflasyon.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diğer dataların Jupytere yüklenme süresi: 0:01:00.258042\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Diğer dataların Jupytere yüklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifik 2016 aktivite verileri olmadığı için 2016 Sales dataları çıkartıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[pasifik_df_all[\"Yıl\"] != 2016].reset_index(drop=True)\n",
    "pasifik_df_all = pasifik_df_all[~((pasifik_df_all[\"Yıl\"] == 2021) & (pasifik_df_all[\"Ay\"].isin([6, 7, 8, 9])))].reset_index(drop=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all[~((horizon_saha_df_all[\"Yıl\"] == 2021) & (horizon_saha_df_all[\"Ay\"].isin([6, 7, 8, 9])))].reset_index(drop=True)\n",
    "btt_df_all = btt_df_all[~((btt_df_all[\"Yıl\"] == 2021) & (btt_df_all[\"Ay\"].isin([6, 7, 8, 9])))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all[\"Yıl\"] = horizon_saha_df_all[\"Yıl\"].astype(int)\n",
    "horizon_saha_df_all[\"Ay\"] = horizon_saha_df_all[\"Ay\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Datası İçin Ürün Kod Eşleme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kategori_adi = btt_df_all.drop_duplicates(subset=[\"Marka Adı\", \"Kategori Adı\"], keep=\"first\")[[\"Marka Adı\", \"Kategori Adı\"]]\n",
    "kategori_adi.sort_values(by=[\"Marka Adı\", \"Kategori Adı\"], ignore_index=True, inplace=True)\n",
    "kategori_adi = dict(kategori_adi.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all[\"Kategori Adı\"] = horizon_saha_df_all[\"Kategori Adı\"].fillna(horizon_saha_df_all[\"Marka Adı\"].map(kategori_adi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50063, 12), (308452, 12), (17353, 12))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pasifik_df_all.shape, horizon_saha_df_all.shape, btt_df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pd.merge(pasifik_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "horizon_saha_df_all = pd.merge(horizon_saha_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "btt_df_all = pd.merge(btt_df_all, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50063, 13), (308452, 13), (17353, 13))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pasifik_df_all.shape, horizon_saha_df_all.shape, btt_df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ürün Eşleme Kodları dosyasında yer almayan kodlar için mevcut ürün kodları verildi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_ = pasifik_df_all[pd.isnull(pasifik_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = pasifik_df_all[~pd.isnull(pasifik_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "pasifik_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "pasifik_df_all = pasifik_df_all.sort_values(pasifik_df_all.columns.to_list()).reset_index(drop=True)\n",
    "\n",
    "empty_ = btt_df_all[pd.isnull(btt_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = btt_df_all[~pd.isnull(btt_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "btt_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "btt_df_all = btt_df_all.sort_values(btt_df_all.columns.to_list()).reset_index(drop=True)\n",
    "\n",
    "empty_ = horizon_saha_df_all[pd.isnull(horizon_saha_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "full_ = horizon_saha_df_all[~pd.isnull(horizon_saha_df_all[\"En Güncel Kod\"])].reset_index(drop=True)\n",
    "empty_[\"En Güncel Kod\"] = empty_[\"Ürün Kodu\"]\n",
    "horizon_saha_df_all = pd.concat([empty_, full_], axis=0, ignore_index=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all.sort_values(horizon_saha_df_all.columns.to_list()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adet adında yeni bir kolon oluşturuldu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all[\"Adet\"] = pasifik_df_all[\"Koli İçi Adet\"] * pasifik_df_all[\"Koli\"]\n",
    "btt_df_all[\"Adet\"] = btt_df_all[\"Koli İçi Adet\"] * btt_df_all[\"Koli\"]\n",
    "horizon_saha_df_all[\"Adet\"] = horizon_saha_df_all[\"Koli İçi Adet\"] * horizon_saha_df_all[\"Koli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50063, 14), (308452, 14), (17353, 14))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pasifik_df_all.shape, horizon_saha_df_all.shape, btt_df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delist olan ürünler veriden çıkartıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all = pasifik_df_all[pasifik_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "btt_df_all = btt_df_all[btt_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "horizon_saha_df_all = horizon_saha_df_all[horizon_saha_df_all[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41432, 14), (266548, 14), (14582, 14))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pasifik_df_all.shape, horizon_saha_df_all.shape, btt_df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aynı yıl, ay, grup adı, ana kategori adı, kategori adı, marka adı ve SKU kodundaki ürünler için toplam alındı. Sadece Koli İçi Adet için maksimum olan alındı."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marka adı dahil değil groupby'a\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_to_sum = {\"Koli İçi Adet\": \"sum\", \"Koli\": \"sum\", \"KG\": \"sum\", \"TL\": \"sum\", \"Adet\": \"sum\"}\n",
    "\n",
    "pasifik_df_all2 = pasifik_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()\n",
    "btt_df_all2 = btt_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()\n",
    "horizon_saha_df_all2 = horizon_saha_df_all.groupby([\"Yıl\", \"Ay\", \"Grup Adı\", \"Ana Kategori Adı\", \"Kategori Adı\", \"En Güncel Kod\"]).agg(dct_to_sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2[\"Date\"] = pasifik_df_all2[\"Yıl\"].astype(str) + \"-\" +  pasifik_df_all2[\"Ay\"].astype(str) + \"-01\"\n",
    "btt_df_all2[\"Date\"] = btt_df_all2[\"Yıl\"].astype(int).astype(str) + \"-\" +  btt_df_all2[\"Ay\"].astype(int).astype(str) + \"-01\"\n",
    "horizon_saha_df_all2[\"Date\"] = horizon_saha_df_all2[\"Yıl\"].astype(int).astype(str) + \"-\" +  horizon_saha_df_all2[\"Ay\"].astype(int).astype(str) + \"-01\"\n",
    "\n",
    "pasifik_df_all2[\"Date\"] = pd.to_datetime(pasifik_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "btt_df_all2[\"Date\"] = pd.to_datetime(btt_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "horizon_saha_df_all2[\"Date\"] = pd.to_datetime(horizon_saha_df_all2[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon ve Pasifikte bulunan \"Diğer\"'lerin yanlarına \"_\" ile Diğer_Pasifik, Diğer_Horizon yazıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2[\"Grup Adı\"] = pasifik_df_all2[\"Grup Adı\"].apply(lambda x: \"Diğer_Pasifik\" if x == \"Diğer\" else x)\n",
    "horizon_saha_df_all2[\"Grup Adı\"] = horizon_saha_df_all2[\"Grup Adı\"].apply(lambda x: \"Diğer_Horizon\" if x == \"Diğer\" else x)\n",
    "df_all2 = pd.concat([pasifik_df_all2, horizon_saha_df_all2, btt_df_all2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifik Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_urun_isimleri = pasifik_df_all[[\"Marka Adı\", \"Ürün Adı\", \"En Güncel Kod\"]].drop_duplicates(subset=[\"Marka Adı\", \"En Güncel Kod\"],ignore_index=True,keep=\"first\")\n",
    "pasifik_urun_isimleri = pasifik_urun_isimleri[~((pasifik_urun_isimleri[\"Marka Adı\"] == \"DANKEK BATON\") & (pasifik_urun_isimleri[\"En Güncel Kod\"] == 80403))]\n",
    "pasifik_df_all2 = pd.merge(pasifik_df_all2, pasifik_urun_isimleri[[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"]].drop_duplicates(subset=[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"], keep=\"first\"), on=\"En Güncel Kod\", how=\"left\")\n",
    "pasifik_df_all2 = pasifik_df_all2[pasifik_df_all2.columns.to_list()[:5]+pasifik_df_all2.columns.to_list()[-2:]+[pasifik_df_all2.columns.to_list()[5]]+pasifik_df_all2.columns.to_list()[6:12]]\n",
    "\n",
    "\n",
    "\n",
    "horizon_urun_isimleri = horizon_saha_df_all[[\"Marka Adı\", \"Ürün Adı\", \"En Güncel Kod\"]].drop_duplicates(subset=[\"Marka Adı\", \"En Güncel Kod\"],ignore_index=True,keep=\"first\")\n",
    "horizon_urun_isimleri = horizon_urun_isimleri[~(((horizon_urun_isimleri[\"Marka Adı\"] == \"DANKEK BATON\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 80403)) | \n",
    "                                                ((horizon_urun_isimleri[\"Marka Adı\"] == \"MAVİ YEŞİL\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 11802)) |\n",
    "                                                ((horizon_urun_isimleri[\"Marka Adı\"] == \"MAVİ YEŞİL\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 74306)) |\n",
    "                                                ((horizon_urun_isimleri[\"Marka Adı\"] == \"AS KRAKER\") & (horizon_urun_isimleri[\"En Güncel Kod\"] == 190502)))]\n",
    "horizon_saha_df_all2 = pd.merge(horizon_saha_df_all2, horizon_urun_isimleri[[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "horizon_saha_df_all2 = horizon_saha_df_all2[horizon_saha_df_all2.columns.to_list()[:5]+horizon_saha_df_all2.columns.to_list()[-2:]+[horizon_saha_df_all2.columns.to_list()[5]]+horizon_saha_df_all2.columns.to_list()[6:12]]\n",
    "\n",
    "\n",
    "\n",
    "btt_urun_isimleri = btt_df_all[[\"Marka Adı\", \"Ürün Adı\", \"En Güncel Kod\"]].drop_duplicates(subset=[\"Marka Adı\", \"En Güncel Kod\"],ignore_index=True,keep=\"first\")\n",
    "btt_urun_isimleri = btt_urun_isimleri[~((btt_urun_isimleri[\"Marka Adı\"] == \"DANKEK BATON\") & (btt_urun_isimleri[\"En Güncel Kod\"] == 80403))]\n",
    "btt_df_all2 = pd.merge(btt_df_all2, btt_urun_isimleri[[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"]].drop_duplicates(subset=[\"En Güncel Kod\", \"Marka Adı\", \"Ürün Adı\"], keep=\"first\"), on=\"En Güncel Kod\", how=\"left\")\n",
    "btt_df_all2 = btt_df_all2[btt_df_all2.columns.to_list()[:5]+btt_df_all2.columns.to_list()[-2:]+[btt_df_all2.columns.to_list()[5]]+btt_df_all2.columns.to_list()[6:12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all_filled = []\n",
    "for idx, test in pasifik_df_all2.groupby([\"En Güncel Kod\", \"Yıl\", \"Grup Adı\"]):\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, 13):\n",
    "        try:\n",
    "            if i == test.loc[i-1, \"Ay\"]:\n",
    "                if i == 13:\n",
    "                    break\n",
    "            else:\n",
    "                test.loc[-1] = test.loc[0]\n",
    "                test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "                test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "                test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        except:\n",
    "            test.loc[-1] = test.loc[0]\n",
    "            test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "            test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "            test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        test[\"Date\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    rows_to_drop = []\n",
    "    start = test.index[0]\n",
    "    length = 1\n",
    "    while (test.loc[start, \"Adet\"] == 1) and (length < len(test)):\n",
    "        rows_to_drop.append(start)\n",
    "        length+=1\n",
    "        start+=1\n",
    "    test.drop(index=rows_to_drop, inplace=True)\n",
    "    pasifik_df_all_filled.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2 = pd.concat(pasifik_df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasifik missing value düzenlenmesi süresi: 0:04:31.129477\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Horizon Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all_filled = []\n",
    "for idx, test in horizon_saha_df_all2.groupby([\"En Güncel Kod\", \"Yıl\", \"Grup Adı\"]):\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, 13):\n",
    "        try:\n",
    "            if i == test.loc[i-1, \"Ay\"]:\n",
    "                if i == 13:\n",
    "                    break\n",
    "            else:\n",
    "                test.loc[-1] = test.loc[0]\n",
    "                test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "                test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "                test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        except:\n",
    "            test.loc[-1] = test.loc[0]\n",
    "            test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "            test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "            test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        test[\"Date\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    \n",
    "    rows_to_drop = []\n",
    "    start = test.index[0]\n",
    "    length = 1\n",
    "    while (test.loc[start, \"Adet\"] == 1) and (length < len(test)):\n",
    "        rows_to_drop.append(start)\n",
    "        length+=1\n",
    "        start+=1\n",
    "    test.drop(index=rows_to_drop, inplace=True)\n",
    "    horizon_saha_df_all_filled.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all2 = pd.concat(horizon_saha_df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# BTT Filling Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_all_filled = []\n",
    "for idx, test in btt_df_all2.groupby([\"En Güncel Kod\", \"Yıl\", \"Grup Adı\"]):\n",
    "    test.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, 13):\n",
    "        try:\n",
    "            if i == test.loc[i-1, \"Ay\"]:\n",
    "                if i == 13:\n",
    "                    break\n",
    "            else:\n",
    "                test.loc[-1] = test.loc[0]\n",
    "                test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "                test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "                test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        except:\n",
    "            test.loc[-1] = test.loc[0]\n",
    "            test.loc[-1, \"Ay\"], test.loc[-1, \"Koli İçi Adet\"], test.loc[-1, \"Koli\"],  \\\n",
    "            test.loc[-1, \"KG\"], test.loc[-1, \"TL\"], test.loc[-1, \"Adet\"], test.loc[-1, \"Date\"] = i, 0, 0, 0, 0, 1, str(test.loc[-1, \"Yıl\"])+\"-\"+str(i)+\"-\"+str(\"01\")\n",
    "            test = test.sort_values(by=[\"Yıl\", \"Ay\"]).reset_index(drop=True)\n",
    "        test[\"Date\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "    rows_to_drop = []\n",
    "    start = test.index[0]\n",
    "    length = 1\n",
    "    while (test.loc[start, \"Adet\"] == 1) and (length < len(test)):\n",
    "        rows_to_drop.append(start)\n",
    "        length+=1\n",
    "        start+=1\n",
    "    test.drop(index=rows_to_drop, inplace=True)\n",
    "    btt_df_all_filled.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_all2 = pd.concat(btt_df_all_filled, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing value düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aktivite Datası İçin Ürün Kod Eşleme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik Aktivite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left join ile güncel kodlar getirildi. Delist olan ürünler listeden çıkartıldı. \"Çeyrek\" sütunu silindi. En güncel kod sütunnuda bulunamayan değerler Ürün Kodu sütunundan çekildi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_aktivite_df2 = pd.merge(pasifik_aktivite_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "pasifik_aktivite_df2 = pasifik_aktivite_df2[pasifik_aktivite_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "pasifik_aktivite_df2.drop(\"Çeyrek\", inplace=True, axis=1)\n",
    "pasifik_aktivite_df2['En Güncel Kod'] = pasifik_aktivite_df2['En Güncel Kod'].fillna(pasifik_aktivite_df2['Ürün Kodu'])\n",
    "pasifik_aktivite_df2.drop(columns=\"Ürün Kodu\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasifik Aktivite Ciro - Promosyon Tutarı ve İskonto Tekilleştirme (ORTALAMA ALARAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_pas = {\"Raf Tavsiye Satış Fiyatı\": \"mean\", \"İndirimli Raf Satış Fiyatı\": \"mean\", \"İndirim %\": \"mean\", \"Aktivite Tipi\": \"first\"}\n",
    "pasifik_aktivite_df3 = pasifik_aktivite_df2.groupby([\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Müşteri Grup\"]).agg(ort_pas).reset_index()\n",
    "pasifik_aktivite_df3 = pd.merge(pasifik_aktivite_df3, pasifik_aktivite_df2[[\"Yıl\", \"Ay\", \"Müşteri Grup\", \"En Güncel Kod\", \n",
    "                                                                            \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\"]],\n",
    "                                how=\"left\", \n",
    "                                on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Müşteri Grup\"])\n",
    "\n",
    "pasifik_aktivite_df3.drop_duplicates(subset=pasifik_aktivite_df3.columns.to_list(), inplace=True)\n",
    "pasifik_aktivite_df3.reset_index(drop=True, inplace=True)\n",
    "pasifik_aktivite_df3 = pasifik_aktivite_df3[pasifik_aktivite_df2.drop(\"Ürün Adı\", axis=1).columns.to_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon Aktivite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay2 = pd.merge(saha_aktivite_detay, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], on=\"Ürün Kodu\", how=\"left\")\n",
    "saha_aktivite_detay2 = saha_aktivite_detay2[saha_aktivite_detay2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "saha_aktivite_detay2.drop(\"Çeyrek\", inplace=True, axis=1)\n",
    "saha_aktivite_detay2['En Güncel Kod'] = saha_aktivite_detay2['En Güncel Kod'].fillna(saha_aktivite_detay2['Ürün Kodu'])\n",
    "saha_aktivite_detay2.drop(columns=\"Ürün Kodu\", axis=1, inplace=True)\n",
    "saha_aktivite_detay2[\"İskonto %\"].replace(\"#DIV/0\", np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizon Aktivite Ciro - Promosyon Tutarı ve İskonto Tekilleştirme (ORTALAMA ALARAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort = {\"Ciro (Kull. İade Düş.)\": \"mean\", \"Promosyon Tutarı\": \"mean\", \"İskonto %\": \"mean\"}\n",
    "saha_aktivite_detay3 = saha_aktivite_detay2.groupby([\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Saha Müşteri Grup\"]).agg(ort).reset_index()\n",
    "\n",
    "saha_aktivite_detay3 = pd.merge(saha_aktivite_detay3, saha_aktivite_detay2[[\"Yıl\", \"Ay\", \"Saha Müşteri Grup\", \"En Güncel Kod\", \n",
    "                                                 \"Ana Kategori Adı\", \"Kategori Adı\", \"Marka Adı\"]],\n",
    "                           how=\"left\", \n",
    "                           on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Saha Müşteri Grup\"])\n",
    "\n",
    "saha_aktivite_detay3.drop_duplicates(subset=saha_aktivite_detay3.columns.to_list(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3 = saha_aktivite_detay3[saha_aktivite_detay2.drop(\"Ürün Adı (Mobis)\", axis=1).columns.to_list()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3.rename(columns={\"Saha Müşteri Grup\": \"Grup Adı\"}, inplace=True)\n",
    "saha_aktivite_detay3[\"Grup Adı\"] = saha_aktivite_detay3[\"Grup Adı\"].apply(lambda x: \"Diğer_Horizon\" if x == \"Diğer\" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiyat Listesi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Fiyatları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon.drop_duplicates(subset=fiyat_lst_horizon.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].str.replace(\"-\", \"\")\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].astype(int)*1\n",
    "except:\n",
    "    fiyat_lst_horizon[\"Malzeme\"] = fiyat_lst_horizon[\"Malzeme\"].astype(int)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df = fiyat_lst_horizon.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df[\"Baslangic_Yıl\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.year)\n",
    "fiyat_lst_horizon_df[\"Baslangic_Ay\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.month)\n",
    "fiyat_lst_horizon_df[\"Baslangic_Gun\"] = fiyat_lst_horizon_df[\"Bşl.tarihi\"].apply(lambda x: x.day)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.year)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Ay\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.month)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Gun\"] = fiyat_lst_horizon_df[\"Gçrl.sonu\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_horizon_df[\"Baslangic_Yıl\"] = fiyat_lst_horizon_df[\"Baslangic_Yıl\"].apply(lambda x: (horizon_saha_df_all2[\"Date\"].max().year)+1 if x > horizon_saha_df_all2[\"Date\"].max().year else x)\n",
    "fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"] = fiyat_lst_horizon_df[\"Gecerlilik_Yıl\"].apply(lambda x: (horizon_saha_df_all2[\"Date\"].max().year)+1 if x > horizon_saha_df_all2[\"Date\"].max().year else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = horizon_saha_df_all2[\"Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fiyat_unique = []\n",
    "\n",
    "for malzeme in fiyat_lst_horizon_df[\"Malzeme\"].unique():\n",
    "    temp_time_df = pd.DataFrame({\"Fiyat\": [np.nan]}, index=time_index)\n",
    "    temp_time_df = temp_time_df.reset_index().rename(columns={\"index\":\"date\"})    \n",
    "    temp_time_df[\"En Güncel Kod\"] = malzeme\n",
    "    temp_time_df[\"fiyat_gecisi\"] = 0\n",
    "    malzeme_df = fiyat_lst_horizon_df[fiyat_lst_horizon_df[\"Malzeme\"] == malzeme].reset_index(drop=True)\n",
    "    malzeme_df.drop(columns=[\"KşTü\", \"Koşul türü\", \"Tanım\", \"Ana Kategori\", \"Kategori\", \"ÖB\"], axis=1, inplace=True)\n",
    "    malzeme_df.drop_duplicates(subset=malzeme_df.columns.to_list(), inplace=True, ignore_index=True)\n",
    "    malzeme_df.sort_values(by=[\"Baslangic_Yıl\", \"Baslangic_Ay\", \"Baslangic_Gun\"], ignore_index=True, inplace=True)\n",
    "    check_idx1 = []\n",
    "    if len(malzeme_df) > 1:\n",
    "        for row1 in malzeme_df.index:\n",
    "            for row2 in malzeme_df[row1+1:].index:\n",
    "                if (malzeme_df.loc[row1][\"Gecerlilik_Yıl\"] == malzeme_df.loc[row2][\"Baslangic_Yıl\"]) and (malzeme_df.loc[row1][\"Gecerlilik_Ay\"] == malzeme_df.loc[row2][\"Baslangic_Ay\"]):\n",
    "                    num_days = calendar.monthrange(int(malzeme_df.loc[row2][\"Baslangic_Yıl\"]), int(malzeme_df.loc[row2][\"Baslangic_Ay\"]))[1]\n",
    "                    fyt=((int(malzeme_df.loc[row1][\"Gecerlilik_Gun\"])*malzeme_df.loc[row1][\"     Tutar\"]) + (num_days - int(malzeme_df.loc[row2][\"Baslangic_Gun\"]) + 1)*malzeme_df.loc[row2][\"     Tutar\"])/num_days\n",
    "\n",
    "                    end_idx1 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx1 = temp_time_df[temp_time_df[\"date\"] == end_idx1].index\n",
    "                    temp_time_df.loc[final_idx1, \"Fiyat\"] = fyt\n",
    "                    temp_time_df.loc[final_idx1,\"fiyat_gecisi\"] = 1\n",
    "\n",
    "                elif (malzeme_df.loc[row1, \"Gecerlilik_Gun\"] == calendar.monthrange(int(malzeme_df.loc[row1][\"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1][\"Gecerlilik_Ay\"]))[1] \\\n",
    "                     and malzeme_df.loc[row2, \"Baslangic_Gun\"] == 1):\n",
    "                    fyt5=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                    fyt6=malzeme_df.loc[row2][\"     Tutar\"]\n",
    "                    end_idx5 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    end_idx6 =  datetime(int(malzeme_df.loc[row2, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row2, \"Baslangic_Ay\"]), 1)\n",
    "                    final_idx5 = temp_time_df[temp_time_df[\"date\"] == end_idx5].index\n",
    "                    final_idx6 = temp_time_df[temp_time_df[\"date\"] == end_idx6].index\n",
    "                    temp_time_df.loc[final_idx5, \"Fiyat\"] = fyt5\n",
    "                    temp_time_df.loc[final_idx6, \"Fiyat\"] = fyt6\n",
    "\n",
    "                else:\n",
    "                    fyt2=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                    start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx2 = temp_time_df[(temp_time_df[\"date\"] > start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                    temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "            if (row1 == len(malzeme_df)-1) or (row1 == len(malzeme_df)-2):\n",
    "                fyt3=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "                start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                final_idx3 = temp_time_df[(temp_time_df[\"date\"] > start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "\n",
    "    else:\n",
    "        for row1 in malzeme_df.index:\n",
    "            fyt4=malzeme_df.loc[row1][\"     Tutar\"]\n",
    "            start_idx4 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "            end_idx4 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "            final_idx4 = temp_time_df[(temp_time_df[\"date\"] >= start_idx4) & (temp_time_df[\"date\"] <= end_idx4)].index\n",
    "            temp_time_df.loc[final_idx4, \"Fiyat\"] = fyt4\n",
    "\n",
    "    if (malzeme_df.loc[0, \"Baslangic_Yıl\"] >= min(horizon_saha_df_all2[\"Yıl\"].unique())) and (len(malzeme_df) > 1):\n",
    "        temp_time_df.loc[temp_time_df[~pd.isnull(temp_time_df[\"Fiyat\"])].index[0]-1, \"Fiyat\"] = malzeme_df.loc[0, \"     Tutar\"]\n",
    "    temp_time_df = temp_time_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    h_fiyat_unique.append(temp_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fiyat_unique = pd.concat(h_fiyat_unique)\n",
    "h_fiyat_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "h_fiyat_unique.rename(columns={\"En Güncel Kod\": \"Ürün Kodu\", \"date\": \"Date\"}, inplace=True)\n",
    "h_fiyat_unique = h_fiyat_unique.merge(eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\")\n",
    "h_fiyat_unique[\"En Güncel Kod\"].fillna(h_fiyat_unique[\"Ürün Kodu\"], inplace=True)\n",
    "h_fiyat_unique = h_fiyat_unique[h_fiyat_unique[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "h_fiyat_unique = h_fiyat_unique.sort_values(by=[\"En Güncel Kod\", \"Date\"]).reset_index(drop=True)\n",
    "h_fiyat_unique = h_fiyat_unique.drop(columns=\"Ürün Kodu\", axis=1)\n",
    "# Aynı aya denk gelen ürünlerin fiyatlarının ortalaması alınıp, herhangi birinde fiyat geçişi varsa 1 alınır.\n",
    "h_fiyat_unique = h_fiyat_unique.groupby([\"Date\", \"En Güncel Kod\"]).agg({\"Fiyat\": \"mean\", \"fiyat_gecisi\": \"max\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon fiyatların düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Fiyatları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik.drop_duplicates(subset=fiyat_lst_pasifik.columns.to_list(), keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik[\"Malzeme\"] = fiyat_lst_pasifik[\"Malzeme\"].str.replace(\"-\", \"\")\n",
    "fiyat_lst_pasifik[\"Malzeme\"] = fiyat_lst_pasifik[\"Malzeme\"].astype(int)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df = fiyat_lst_pasifik.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df[\"Baslangic_Yıl\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.year)\n",
    "fiyat_lst_pasifik_df[\"Baslangic_Ay\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.month)\n",
    "fiyat_lst_pasifik_df[\"Baslangic_Gun\"] = fiyat_lst_pasifik_df[\"Bşl.tarihi\"].apply(lambda x: x.day)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.year)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Ay\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.month)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Gun\"] = fiyat_lst_pasifik_df[\"Gçrl.sonu\"].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiyat_lst_pasifik_df[\"Baslangic_Yıl\"] = fiyat_lst_pasifik_df[\"Baslangic_Yıl\"].apply(lambda x: (pasifik_df_all2[\"Date\"].max().year)+1 if x > pasifik_df_all2[\"Date\"].max().year else x)\n",
    "fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"] = fiyat_lst_pasifik_df[\"Gecerlilik_Yıl\"].apply(lambda x: (pasifik_df_all2[\"Date\"].max().year)+1 if x > pasifik_df_all2[\"Date\"].max().year else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = pasifik_df_all2[\"Date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique = []\n",
    "\n",
    "for malzeme in fiyat_lst_pasifik_df[\"Malzeme\"].unique():\n",
    "    temp_time_df = pd.DataFrame({\"Fiyat\": [np.nan]}, index=time_index)\n",
    "    temp_time_df = temp_time_df.reset_index().rename(columns={\"index\":\"date\"})    \n",
    "    temp_time_df[\"En Güncel Kod\"] = malzeme\n",
    "    temp_time_df[\"fiyat_gecisi\"] = 0\n",
    "    malzeme_df = fiyat_lst_pasifik_df[fiyat_lst_pasifik_df[\"Malzeme\"] == malzeme].reset_index(drop=True)\n",
    "    malzeme_df.drop(columns=[\"KşTü\", \"KşTü.1\", \"Malzeme Tanım\", \"Ana Kategori\", \"Kategori\"], axis=1, inplace=True)\n",
    "    malzeme_df.drop_duplicates(subset=malzeme_df.columns.to_list(), inplace=True, ignore_index=True)\n",
    "    malzeme_df.sort_values(by=[\"Baslangic_Yıl\", \"Baslangic_Ay\", \"Baslangic_Gun\"], ignore_index=True, inplace=True)\n",
    "    check_idx1 = []\n",
    "    if len(malzeme_df) > 1:\n",
    "        for row1 in malzeme_df.index:\n",
    "            for row2 in malzeme_df[row1+1:].index:\n",
    "                if (malzeme_df.loc[row1][\"Gecerlilik_Yıl\"] == malzeme_df.loc[row2][\"Baslangic_Yıl\"]) and (malzeme_df.loc[row1][\"Gecerlilik_Ay\"] == malzeme_df.loc[row2][\"Baslangic_Ay\"]):\n",
    "                    num_days = calendar.monthrange(int(malzeme_df.loc[row2][\"Baslangic_Yıl\"]), int(malzeme_df.loc[row2][\"Baslangic_Ay\"]))[1]\n",
    "                    fyt=((int(malzeme_df.loc[row1][\"Gecerlilik_Gun\"])*malzeme_df.loc[row1][\"Koli TL\"]) + (num_days - int(malzeme_df.loc[row2][\"Baslangic_Gun\"])+1)*malzeme_df.loc[row2][\"Koli TL\"])/num_days\n",
    "\n",
    "                    end_idx1 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx1 = temp_time_df[temp_time_df[\"date\"] == end_idx1].index\n",
    "                    temp_time_df.loc[final_idx1, \"Fiyat\"] = fyt\n",
    "                    temp_time_df.loc[final_idx1,\"fiyat_gecisi\"] = 1\n",
    "\n",
    "                elif (malzeme_df.loc[row1, \"Gecerlilik_Gun\"] == calendar.monthrange(int(malzeme_df.loc[row1][\"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1][\"Gecerlilik_Ay\"]))[1] \\\n",
    "                     and malzeme_df.loc[row2, \"Baslangic_Gun\"] == 1):\n",
    "                    fyt5=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    fyt6=malzeme_df.loc[row2][\"Koli TL\"]\n",
    "                    end_idx5 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    end_idx6 =  datetime(int(malzeme_df.loc[row2, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row2, \"Baslangic_Ay\"]), 1)\n",
    "                    final_idx5 = temp_time_df[temp_time_df[\"date\"] == end_idx5].index\n",
    "                    final_idx6 = temp_time_df[temp_time_df[\"date\"] == end_idx6].index\n",
    "                    temp_time_df.loc[final_idx5, \"Fiyat\"] = fyt5\n",
    "                    temp_time_df.loc[final_idx6, \"Fiyat\"] = fyt6\n",
    "\n",
    "\n",
    "                else:\n",
    "                    if malzeme_df.loc[row1, \"Baslangic_Gun\"] != 1:\n",
    "                        fyt2=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                        start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                        end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                        final_idx2 = temp_time_df[(temp_time_df[\"date\"] > start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                        temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "                    else:\n",
    "                        fyt2=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                        start_idx2 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                        end_idx2 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                        final_idx2 = temp_time_df[(temp_time_df[\"date\"] >= start_idx2) & (temp_time_df[\"date\"] < end_idx2)].index\n",
    "                        temp_time_df.loc[final_idx2, \"Fiyat\"] = fyt2\n",
    "                        \n",
    "            if (row1 == len(malzeme_df)-1) or (row1 == len(malzeme_df)-2):\n",
    "                if malzeme_df.loc[row1, \"Baslangic_Gun\"] != 1:\n",
    "                    fyt3=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx3 = temp_time_df[(temp_time_df[\"date\"] > start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                    temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "                else:\n",
    "                    fyt3=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "                    start_idx3 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "                    end_idx3 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "                    final_idx3 = temp_time_df[(temp_time_df[\"date\"] >= start_idx3) & (temp_time_df[\"date\"] < end_idx3)].index\n",
    "                    temp_time_df.loc[final_idx3, \"Fiyat\"] = fyt3\n",
    "    else:\n",
    "        for row1 in malzeme_df.index:\n",
    "            fyt4=malzeme_df.loc[row1][\"Koli TL\"]\n",
    "            start_idx4 = datetime(int(malzeme_df.loc[row1, \"Baslangic_Yıl\"]), int(malzeme_df.loc[row1, \"Baslangic_Ay\"]), 1)\n",
    "            end_idx4 =  datetime(int(malzeme_df.loc[row1, \"Gecerlilik_Yıl\"]), int(malzeme_df.loc[row1, \"Gecerlilik_Ay\"]), 1)\n",
    "            final_idx4 = temp_time_df[(temp_time_df[\"date\"] >= start_idx4) & (temp_time_df[\"date\"] <= end_idx4)].index\n",
    "            temp_time_df.loc[final_idx4, \"Fiyat\"] = fyt4\n",
    "    \n",
    "    if (malzeme_df.loc[0, \"Baslangic_Yıl\"] >= min(pasifik_df_all2[\"Yıl\"].unique())) and (len(malzeme_df) > 1):\n",
    "        temp_time_df.loc[temp_time_df[~pd.isnull(temp_time_df[\"Fiyat\"])].index[0]-1, \"Fiyat\"] = malzeme_df.loc[0, \"Koli TL\"]\n",
    "    temp_time_df = temp_time_df.dropna().reset_index(drop=True)\n",
    "        \n",
    "    p_fiyat_unique.append(temp_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique = pd.concat(p_fiyat_unique)\n",
    "p_fiyat_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "p_fiyat_unique.rename(columns={\"En Güncel Kod\": \"Ürün Kodu\", \"date\": \"Date\"}, inplace=True)\n",
    "p_fiyat_unique = p_fiyat_unique.merge(eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\")\n",
    "p_fiyat_unique[\"En Güncel Kod\"].fillna(p_fiyat_unique[\"Ürün Kodu\"], inplace=True)\n",
    "p_fiyat_unique = p_fiyat_unique[p_fiyat_unique[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "p_fiyat_unique = p_fiyat_unique.sort_values(by=[\"En Güncel Kod\", \"Date\"]).reset_index(drop=True)\n",
    "p_fiyat_unique = p_fiyat_unique.drop(columns=\"Ürün Kodu\", axis=1)\n",
    "# Aynı aya denk gelen ürünlerin fiyatlarının ortalaması alınıp, herhangi birinde fiyat geçişi varsa 1 alınır.\n",
    "p_fiyat_unique = p_fiyat_unique.groupby([\"Date\", \"En Güncel Kod\"]).agg({\"Fiyat\": \"mean\", \"fiyat_gecisi\": \"max\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik fiyatların düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 0'ları doldururken en son yıl ve ayın ötesi de 0 ile dolduruyor. (Örn: Sales datasında 2021'in 3. ayına kadar data olduğu durumda 0'lar ile doldururken 2021 12. aya kadar 0 atıyor.\n",
    "# Bu durumun önüne geçmek için aşağıdaki işlemler yapılmaktadır.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_pas = pasifik_df_all2.copy()\n",
    "backup_btt = btt_df_all2.copy()\n",
    "backup_hor = horizon_saha_df_all2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_max_year = pasifik_df_all[\"Yıl\"].max()\n",
    "pas_max_month = pasifik_df_all[pasifik_df_all[\"Yıl\"] == pasifik_df_all[\"Yıl\"].max()][\"Ay\"].max()\n",
    "hor_max_year = horizon_saha_df_all[\"Yıl\"].max()\n",
    "hor_max_month = horizon_saha_df_all[horizon_saha_df_all[\"Yıl\"] == horizon_saha_df_all[\"Yıl\"].max()][\"Ay\"].max()\n",
    "btt_max_year = btt_df_all[\"Yıl\"].max()\n",
    "btt_max_month = btt_df_all[btt_df_all[\"Yıl\"] == btt_df_all[\"Yıl\"].max()][\"Ay\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup_1 = pasifik_df_all2.copy()\n",
    "hor_backup_1 = horizon_saha_df_all2.copy()\n",
    "btt_backup_1 = btt_df_all2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all2 = pasifik_df_all2[~((pasifik_df_all2[\"Yıl\"] == pas_max_year) & (pasifik_df_all2[\"Ay\"] > pas_max_month))].reset_index(drop=True)\n",
    "horizon_saha_df_all2 = horizon_saha_df_all2[~((horizon_saha_df_all2[\"Yıl\"] == hor_max_year) & \n",
    "                                            (horizon_saha_df_all2[\"Ay\"] > hor_max_month))].reset_index(drop=True)\n",
    "btt_df_all2 = btt_df_all2[~((btt_df_all2[\"Yıl\"] == btt_max_year) & (btt_df_all2[\"Ay\"] > btt_max_month))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portföy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasifik Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_portfoy_df2 = pd.merge(pasifik_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "pasifik_portfoy_df2[\"En Güncel Kod\"] = pasifik_portfoy_df2[\"En Güncel Kod\"].fillna(pasifik_portfoy_df2[\"Kod\"])\n",
    "pasifik_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "pasifik_portfoy_df2 = pasifik_portfoy_df2[pasifik_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "pasifik_portfoy_df2[\"Portfoy\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_portfoy_df2 = pd.merge(horizon_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "horizon_portfoy_df2[\"En Güncel Kod\"] = horizon_portfoy_df2[\"En Güncel Kod\"].fillna(horizon_portfoy_df2[\"Kod\"])\n",
    "horizon_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "horizon_portfoy_df2 = horizon_portfoy_df2[horizon_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "horizon_portfoy_df2[\"Portfoy\"] = 1\n",
    "horizon_portfoy_df2 = horizon_portfoy_df2[~((horizon_portfoy_df2[\"Kod\"] == 135901))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTT Portföy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_portfoy_df2 = pd.merge(btt_portfoy_df, eslenik_kod_df[[\"Ürün Kodu\", \"En Güncel Kod\"]], how=\"left\", left_on=\"Kod\", right_on=\"Ürün Kodu\")\n",
    "btt_portfoy_df2[\"En Güncel Kod\"] = btt_portfoy_df2[\"En Güncel Kod\"].fillna(btt_portfoy_df2[\"Kod\"])\n",
    "btt_portfoy_df2.drop(\"Ürün Kodu\", axis=1, inplace=True)\n",
    "btt_portfoy_df2 = btt_portfoy_df2[btt_portfoy_df2[\"En Güncel Kod\"] != \"delist\"].reset_index(drop=True)\n",
    "btt_portfoy_df2[\"Portfoy\"] = 1\n",
    "btt_portfoy_df2 = btt_portfoy_df2[~((btt_portfoy_df2[\"Kod\"] == 135901))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portföy Kapsamındaki Sales Dataları"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pd.merge(pasifik_df_all2,pasifik_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "btt_df_all3 = pd.merge(btt_df_all2,btt_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "horizon_saha_df_all3 = pd.merge(horizon_saha_df_all2,horizon_portfoy_df2[[\"En Güncel Kod\", \"Portfoy\"]], on=\"En Güncel Kod\", how=\"left\")\n",
    "pasifik_df_all3[\"Portfoy\"].fillna(0, inplace=True)\n",
    "btt_df_all3[\"Portfoy\"].fillna(0, inplace=True)\n",
    "horizon_saha_df_all3[\"Portfoy\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datalara Calender Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calender_df.pop(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pd.merge(pasifik_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")\n",
    "btt_df_all3 = pd.merge(btt_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")\n",
    "horizon_saha_df_all3 = pd.merge(horizon_saha_df_all3, calender_df, on=[\"Yıl\", \"Ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataların Fiyat Ve Aktiviteler İle Birleştirilmesi\n",
    "---\n",
    "\n",
    "# Not:\n",
    "---\n",
    "### 1) BTT aktivite verisi için Horizon kısmındaki \"Geleneksel Kanal\" kullanılması istendi.\n",
    "### 2) BTT fiyat geçişleri için Horizon fiyat geçişleri baz alındı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pasifik_df_all3.merge(p_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "pasifik_aktivite_df3.rename(columns={\"Müşteri Grup\": \"Grup Adı\", \"Grup adı\": \"Grup Adı\"}, inplace=True)\n",
    "pasifik_df_all3 = pd.merge(pasifik_df_all3, pasifik_aktivite_df3[[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\", \n",
    "                                                                  \"Raf Tavsiye Satış Fiyatı\", \"İndirimli Raf Satış Fiyatı\", \"İndirim %\",\n",
    "                                                                  \"Aktivite Tipi\"]], \n",
    "                           left_on=[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\"], \n",
    "                           right_on=[\"Yıl\", \"Ay\", \"Grup Adı\", \"En Güncel Kod\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saha_aktivite_detay3.rename(columns={\"Grup adı\": \"Grup Adı\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(h_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(saha_aktivite_detay3[['Ciro (Kull. İade Düş.)', 'Promosyon Tutarı', \n",
    "                                                                        'İskonto %', 'En Güncel Kod', \"Yıl\", \"Ay\", \"Grup Adı\"]],\n",
    "                                                  on=[\"En Güncel Kod\", \"Yıl\", \"Ay\", \"Grup Adı\"], how=\"left\")\n",
    "btt_df_all3 = btt_df_all3.merge(h_fiyat_unique, how=\"left\", on=[\"Date\", \"En Güncel Kod\"])\n",
    "btt_aktivite = saha_aktivite_detay3[saha_aktivite_detay3[\"Grup Adı\"] == \"GELENEKSEL KANAL\"].reset_index(drop=True)\n",
    "\n",
    "btt_df_all3 = btt_df_all3.merge(btt_aktivite[['Ciro (Kull. İade Düş.)', 'Promosyon Tutarı', \n",
    "                                              'İskonto %', 'En Güncel Kod', \"Yıl\", \"Ay\"]],\n",
    "                                on=[\"En Güncel Kod\", \"Yıl\", \"Ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sütun İsimlerini İngilizce Karaktere Çevirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_col_name(dff_):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    \n",
    "    dff_: dataframe\n",
    "    Sütun ismini değiştirmek istediğiniz dataframe'i yazınız.\n",
    "    \n",
    "    Returns: Liste\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    chng_letters = list(zip([\"ç\", \"ğ\", \"ı\", \"ö\", \"ş\", \"ü\", \" \", \"%\", \".\", \"(\", \")\", \"-\"], \n",
    "                            [\"c\", \"g\", \"i\", \"o\", \"s\", \"u\", \"_\", \"\", \"\", \"\", \"\", \"_\"]))\n",
    "    new_cols = []\n",
    "    for col in dff_.columns.str.lower():\n",
    "        for letter in range(len(chng_letters)):\n",
    "            col = col.replace(chng_letters[letter][0], chng_letters[letter][1])\n",
    "            if letter == len(chng_letters) - 1:\n",
    "                new_cols.append(col)\n",
    "            else:\n",
    "                pass\n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3.columns = change_col_name(pasifik_df_all3)\n",
    "horizon_saha_df_all3.columns = change_col_name(horizon_saha_df_all3)\n",
    "btt_df_all3.columns = change_col_name(btt_df_all3)\n",
    "enflasyon_df.columns = change_col_name(enflasyon_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pasifikte Aktivite Tipi Verisi Eksik Olan Verilere \"Yok\" yazıldı\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3[\"aktivite_tipi\"].fillna(\"Yok\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Enflasyon Verilerinin Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_all3 = pasifik_df_all3.merge(enflasyon_df, on=[\"date\"], how=\"left\")\n",
    "horizon_saha_df_all3 = horizon_saha_df_all3.merge(enflasyon_df, on=[\"date\"], how=\"left\")\n",
    "btt_df_all3 = btt_df_all3.merge(enflasyon_df, on=[\"date\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df_all3.copy()\n",
    "horizon_saha_df_sorted = horizon_saha_df_all3.copy()\n",
    "btt_df_sorted = btt_df_all3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Satış Olmayan Aylar Flaglendi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"satis_var\"] = [0 if adet <= 1 else 1 for adet in pasifik_df_sorted[\"adet\"]]\n",
    "horizon_saha_df_sorted[\"satis_var\"] = [0 if adet <= 1 else 1 for adet in horizon_saha_df_sorted[\"adet\"]]\n",
    "btt_df_sorted[\"satis_var\"] = [0 if adet <= 1 else 1 for adet in btt_df_sorted[\"adet\"]]\n",
    "\n",
    "df_pasifik = pasifik_df_sorted.copy()\n",
    "df_btt = btt_df_sorted.copy()\n",
    "df_horizon = horizon_saha_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sütun isim uzunluğunun 32'yi geçmemesi için\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik.columns = [i[:32] if len(i) > 32 else i for i in df_pasifik.columns]\n",
    "df_horizon.columns = [i[:32] if len(i) > 32 else i for i in df_horizon.columns]\n",
    "df_btt.columns = [i[:32] if len(i) > 32 else i for i in df_btt.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sales datasındaki son yıl ve aya kadar ulaşmayan, yarıda kesilen verilerin, son tarihe kadar NaN ile doldurulması\n",
    "## Sales verisindeki en son tarih Haziran 2021 ise, herhangi bir SKU'nun son gözlemi Şubat 2019'da olsa dahi Haizran 2021'e kadar devam ettiriliyor\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_and_horizon_cols_to_drop = [\"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\",\n",
    "                                \"urun_adi\", \"en_guncel_kod\", \"portfoy\", \"koli_i̇ci_adet\", \"koli\", \"kg\", \"tl\", \"adet\", \n",
    "                                \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\",\n",
    "                                \"i̇skonto_\", \"satis_var\"]\n",
    "\n",
    "pasifik_cols_to_drop = [\"grup_adi\", \"ana_kategori_adi\", \"kategori_adi\", \"marka_adi\",\n",
    "                        \"urun_adi\", \"en_guncel_kod\", \"portfoy\", \"koli_i̇ci_adet\", \"koli\", \"kg\", \"tl\", \"adet\",\n",
    "                        \"fiyat\", \"fiyat_gecisi\", \"raf_tavsiye_satis_fiyati\", \"i̇ndirimli_raf_satis_fiyati\",\n",
    "                        \"aktivite_tipi\", \"i̇ndirim_\", \"satis_var\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik = df_pasifik[df_pasifik[\"date\"] < datetime(2021, 6, 1)]\n",
    "df_horizon = df_horizon[df_horizon[\"date\"] < datetime(2021, 6, 1)]\n",
    "df_btt = df_btt[df_btt[\"date\"] < datetime(2021, 6, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hor_backup_for_null_date = df_horizon.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_template = df_pasifik.drop(columns=pasifik_cols_to_drop, axis=1)\n",
    "pasifik_template = pasifik_template.drop_duplicates(subset=pasifik_template.columns.to_list()).sort_values(by=[\"yil\", \"ay\"])\n",
    "\n",
    "btt_template = df_btt.drop(columns=btt_and_horizon_cols_to_drop, axis=1)\n",
    "btt_template = btt_template.drop_duplicates(subset=btt_template.columns.to_list()).sort_values(by=[\"yil\", \"ay\"])\n",
    "\n",
    "horizon_template = df_horizon.drop(columns=btt_and_horizon_cols_to_drop, axis=1)\n",
    "horizon_template = horizon_template.drop_duplicates(subset=horizon_template.columns.to_list()).sort_values(by=[\"yil\", \"ay\"])\n",
    "\n",
    "pas_date = pasifik_df_sorted.date.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "hor_date = horizon_saha_df_sorted.date.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "btt_date = btt_df_sorted.date.drop_duplicates().sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_sku_fill = []\n",
    "pasifik_grup_fill = []\n",
    "for sku in df_pasifik[\"en_guncel_kod\"].unique():\n",
    "    for grup in df_pasifik[\"grup_adi\"].unique():\n",
    "        temp = df_pasifik[(df_pasifik[\"en_guncel_kod\"] == sku) & (df_pasifik[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "        if temp[\"date\"].max() < pasifik_template[\"date\"].max():\n",
    "            pasifik_sku_fill.append(sku)\n",
    "            pasifik_grup_fill.append(grup)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        temp_date = pd.DataFrame(pas_date)[pd.DataFrame(pas_date)[\"date\"] >= temp[\"date\"].min()]\n",
    "        \n",
    "        if len(temp_date) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            pasifik_sku_fill.append(sku)\n",
    "            pasifik_grup_fill.append(grup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing imputation düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_sku_fill = []\n",
    "btt_grup_fill = []\n",
    "for sku in df_btt[\"en_guncel_kod\"].unique():\n",
    "    for grup in df_btt[\"grup_adi\"].unique():\n",
    "        temp = df_btt[(df_btt[\"en_guncel_kod\"] == sku) & (df_btt[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "        if temp[\"date\"].max() < btt_template[\"date\"].max():\n",
    "            btt_sku_fill.append(sku)\n",
    "            btt_grup_fill.append(grup)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        temp_date = pd.DataFrame(btt_date)[pd.DataFrame(btt_date)[\"date\"] >= temp[\"date\"].min()]\n",
    "        \n",
    "        if len(temp_date) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            btt_sku_fill.append(sku)\n",
    "            btt_grup_fill.append(grup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing imputation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_sku_fill = []\n",
    "horizon_grup_fill = []\n",
    "for sku in df_horizon[\"en_guncel_kod\"].unique():\n",
    "    for grup in df_horizon[\"grup_adi\"].unique():\n",
    "        temp = df_horizon[(df_horizon[\"en_guncel_kod\"] == sku) & (df_horizon[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "        if temp[\"date\"].max() < horizon_template[\"date\"].max():\n",
    "            horizon_sku_fill.append(sku)\n",
    "            horizon_grup_fill.append(grup)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        temp_date = pd.DataFrame(hor_date)[pd.DataFrame(hor_date)[\"date\"] >= temp[\"date\"].min()]\n",
    "        \n",
    "        if len(temp_date) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            horizon_sku_fill.append(sku)\n",
    "            horizon_grup_fill.append(grup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing imputation düzenlenmesi süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_pasifik_to_append = []\n",
    "\n",
    "ffill_cols = ['grup_adi', 'ana_kategori_adi', 'kategori_adi',\n",
    "             'marka_adi', 'urun_adi', 'en_guncel_kod', \"portfoy\"]\n",
    "\n",
    "zero_fill_cols = ['koli_i̇ci_adet', 'koli', 'kg', 'tl', 'satis_var']\n",
    "\n",
    "one_fill_cols = [\"adet\"]\n",
    "\n",
    "for row in range(len(pasifik_sku_fill)):\n",
    "    temp = df_pasifik[(df_pasifik[\"en_guncel_kod\"] == pasifik_sku_fill[row]) & \n",
    "                      (df_pasifik[\"grup_adi\"] == pasifik_grup_fill[row])].reset_index(drop=True)\n",
    "    \n",
    "    pasifik_template2 = pasifik_template[pasifik_template[\"date\"] > temp.date.max()].reset_index(drop=True)    \n",
    "    df_to_append = temp.merge(pasifik_template2, how=\"outer\", on=list(set(temp).intersection(set(pasifik_template2))))\n",
    "    \n",
    "    temp_date = pd.DataFrame(pas_date)[pd.DataFrame(pas_date)[\"date\"] >= df_to_append[\"date\"].min()][\"date\"]\n",
    "    date_df = pd.DataFrame(list(set(temp_date) - set(df_to_append[\"date\"])), columns=[\"date\"])\n",
    "    \n",
    "    if len(date_df) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_to_append = df_to_append.merge(date_df, how=\"outer\", on=\"date\")\n",
    "        df_to_append[\"yil\"] = df_to_append[\"date\"].apply(lambda x: x.date().year)\n",
    "        df_to_append[\"ay\"] = df_to_append[\"date\"].apply(lambda x: x.date().month)\n",
    "        df_to_append = df_to_append.merge(pasifik_template2, how=\"outer\", on=list(set(df_to_append).intersection(set(pasifik_template2))))\n",
    "\n",
    "    for f in ffill_cols:\n",
    "        df_to_append[f].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    for z in zero_fill_cols:\n",
    "        df_to_append[z].fillna(0, inplace=True)\n",
    "    \n",
    "    for o in one_fill_cols:\n",
    "        df_to_append[o].fillna(1, inplace=True)\n",
    "    dfs_pasifik_to_append.append(df_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik missing append süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_horizon_to_append = []\n",
    "\n",
    "ffill_cols = ['grup_adi', 'ana_kategori_adi', 'kategori_adi',\n",
    "             'marka_adi', 'urun_adi', 'en_guncel_kod', \"portfoy\"]\n",
    "\n",
    "zero_fill_cols = ['koli_i̇ci_adet', 'koli', 'kg', 'tl', 'satis_var']\n",
    "\n",
    "one_fill_cols = [\"adet\"]\n",
    "\n",
    "for row in range(len(horizon_sku_fill)):\n",
    "    temp = df_horizon[(df_horizon[\"en_guncel_kod\"] == horizon_sku_fill[row]) & \n",
    "                      (df_horizon[\"grup_adi\"] == horizon_grup_fill[row])].reset_index(drop=True)\n",
    "    \n",
    "    horizon_template2 = horizon_template[horizon_template[\"date\"] > temp.date.max()].reset_index(drop=True)\n",
    "    df_to_append = temp.merge(horizon_template2, how=\"outer\", on=list(set(temp).intersection(set(horizon_template2))))\n",
    "    \n",
    "    temp_date = pd.DataFrame(hor_date)[pd.DataFrame(hor_date)[\"date\"] >= df_to_append[\"date\"].min()][\"date\"]\n",
    "    date_df = pd.DataFrame(list(set(temp_date) - set(df_to_append[\"date\"])), columns=[\"date\"])\n",
    "    \n",
    "    if len(date_df) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_to_append = df_to_append.merge(date_df, how=\"outer\", on=\"date\")\n",
    "        df_to_append[\"yil\"] = df_to_append[\"date\"].apply(lambda x: x.date().year)\n",
    "        df_to_append[\"ay\"] = df_to_append[\"date\"].apply(lambda x: x.date().month)\n",
    "        df_to_append = df_to_append.merge(horizon_template2, how=\"outer\", on=list(set(df_to_append).intersection(set(horizon_template2))))\n",
    "\n",
    "    for f in ffill_cols:\n",
    "        df_to_append[f].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    for z in zero_fill_cols:\n",
    "        df_to_append[z].fillna(0, inplace=True)\n",
    "    \n",
    "    for o in one_fill_cols:\n",
    "        df_to_append[o].fillna(1, inplace=True)\n",
    "    dfs_horizon_to_append.append(df_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Horizon missing append süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_btt_to_append = []\n",
    "\n",
    "ffill_cols = ['grup_adi', 'ana_kategori_adi', 'kategori_adi',\n",
    "             'marka_adi', 'urun_adi', 'en_guncel_kod', \"portfoy\"]\n",
    "\n",
    "zero_fill_cols = ['koli_i̇ci_adet', 'koli', 'kg', 'tl', 'satis_var']\n",
    "\n",
    "one_fill_cols = [\"adet\"]\n",
    "\n",
    "for row in range(len(btt_sku_fill)):\n",
    "    temp = df_btt[(df_btt[\"en_guncel_kod\"] == btt_sku_fill[row]) & \n",
    "                  (df_btt[\"grup_adi\"] == btt_grup_fill[row])].reset_index(drop=True)\n",
    "    \n",
    "    btt_template2 = btt_template[btt_template[\"date\"] > temp.date.max()].reset_index(drop=True)\n",
    "\n",
    "    df_to_append = temp.merge(btt_template2, how=\"outer\", on=list(set(temp).intersection(set(btt_template2))))\n",
    "    \n",
    "    temp_date = pd.DataFrame(btt_date)[pd.DataFrame(btt_date)[\"date\"] >= df_to_append[\"date\"].min()][\"date\"]\n",
    "    date_df = pd.DataFrame(list(set(temp_date) - set(df_to_append[\"date\"])), columns=[\"date\"])\n",
    "    \n",
    "    if len(date_df) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df_to_append = df_to_append.merge(date_df, how=\"outer\", on=\"date\")\n",
    "        df_to_append[\"yil\"] = df_to_append[\"date\"].apply(lambda x: x.date().year)\n",
    "        df_to_append[\"ay\"] = df_to_append[\"date\"].apply(lambda x: x.date().month)\n",
    "        df_to_append = df_to_append.merge(btt_template2, how=\"outer\", on=list(set(df_to_append).intersection(set(btt_template2))))\n",
    "\n",
    "    for f in ffill_cols:\n",
    "        df_to_append[f].fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    for z in zero_fill_cols:\n",
    "        df_to_append[z].fillna(0, inplace=True)\n",
    "    \n",
    "    for o in one_fill_cols:\n",
    "        df_to_append[o].fillna(1, inplace=True)\n",
    "    dfs_btt_to_append.append(df_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('BTT missing append süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik2 = df_pasifik.copy()\n",
    "df_horizon2 = df_horizon.copy()\n",
    "df_btt2 = df_btt.copy()\n",
    "\n",
    "dfs_pasifik_to_append = pd.concat(dfs_pasifik_to_append)\n",
    "dfs_horizon_to_append = pd.concat(dfs_horizon_to_append)\n",
    "dfs_btt_to_append = pd.concat(dfs_btt_to_append)\n",
    "\n",
    "df_pasifik3 = pd.concat([df_pasifik2, dfs_pasifik_to_append], axis=0, ignore_index=True)\n",
    "df_pasifik3.drop_duplicates(subset=df_pasifik3.columns.to_list(), ignore_index=True, inplace=True)\n",
    "\n",
    "df_horizon3 = pd.concat([df_horizon2, dfs_horizon_to_append], axis=0, ignore_index=True)\n",
    "df_horizon3.drop_duplicates(subset=df_horizon3.columns.to_list(), ignore_index=True, inplace=True)\n",
    "\n",
    "df_btt3 = pd.concat([df_btt2, dfs_btt_to_append], axis=0, ignore_index=True)\n",
    "df_btt3.drop_duplicates(subset=df_btt3.columns.to_list(), ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_fiyat_unique.rename(columns={\"Date\": \"date\", \"En Güncel Kod\": \"en_guncel_kod\", \"Fiyat\": \"fiyat\"}, inplace=True)\n",
    "h_fiyat_unique.rename(columns={\"Date\": \"date\", \"En Güncel Kod\": \"en_guncel_kod\", \"Fiyat\": \"fiyat\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik4 = df_pasifik3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_pasifik4 = df_pasifik4.merge(p_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])\n",
    "\n",
    "df_horizon4 = df_horizon3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_horizon4 = df_horizon4.merge(h_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])\n",
    "\n",
    "df_btt4 = df_btt3.drop(columns=['fiyat', 'fiyat_gecisi'], axis=1)\n",
    "df_btt4 = df_btt4.merge(h_fiyat_unique, how=\"left\", on=[\"date\", \"en_guncel_kod\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_aktivite_df3.columns = change_col_name(pasifik_aktivite_df3)\n",
    "saha_aktivite_detay3.columns = change_col_name(saha_aktivite_detay3)\n",
    "pasifik_aktivite_df3.rename(columns={\"i̇ndirim_\": \"indirim_\"}, inplace=True)\n",
    "saha_aktivite_detay3.rename(columns={\"i̇i̇skonto_\": \"iskonto_\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasifik4 = df_pasifik4.drop(columns=['raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', \n",
    "                                        'i̇ndirim_', 'aktivite_tipi'], axis=1)\n",
    "\n",
    "df_horizon4 = df_horizon4.drop(columns=['ciro_kull_i̇ade_dus', 'promosyon_tutari', 'i̇skonto_'], axis=1)\n",
    "df_btt4 = df_btt4.drop(columns=['ciro_kull_i̇ade_dus', 'promosyon_tutari', 'i̇skonto_'], axis=1)\n",
    "\n",
    "pasifik_aktivite_df3.rename(columns={\"Müşteri Grup\": \"Grup adı\"}, inplace=True)\n",
    "df_pasifik5 = pd.merge(df_pasifik4, pasifik_aktivite_df3[['yil', 'ay', 'grup_adi', 'en_guncel_kod', \n",
    "                                                          'raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', 'indirim_', \n",
    "                                                          'aktivite_tipi']], \n",
    "                           left_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                           right_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], how=\"left\")\n",
    "\n",
    "df_horizon5 = df_horizon4.merge(saha_aktivite_detay3[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                      'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], how=\"left\")\n",
    "\n",
    "btt_aktivite = saha_aktivite_detay3[saha_aktivite_detay3[\"grup_adi\"] == \"GELENEKSEL KANAL\"].reset_index(drop=True)\n",
    "btt_aktivite[\"grup_adi\"] = \"BTT\"\n",
    "\n",
    "df_btt5 = df_btt4.merge(btt_aktivite[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                      'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                        on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = df_pasifik5.copy()\n",
    "horizon_saha_df_sorted = df_horizon5.copy()\n",
    "btt_df_sorted = df_btt5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Yarışma ve Enflasyon Verisinin Eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.drop(columns=\"enflasyon_etkisi\", axis=1, inplace=True)\n",
    "horizon_saha_df_sorted.drop(columns=\"enflasyon_etkisi\", axis=1, inplace=True)\n",
    "btt_df_sorted.drop(columns=\"enflasyon_etkisi\", axis=1, inplace=True)\n",
    "\n",
    "pasifik_df_sorted = pasifik_df_sorted.merge(enflasyon_df, how=\"left\", on=\"date\")\n",
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(enflasyon_df, how=\"left\", on=\"date\")\n",
    "btt_df_sorted = btt_df_sorted.merge(enflasyon_df, how=\"left\", on=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yarisma_df[\"date\"] = pd.to_datetime(yarisma_df[\"date\"], errors=\"coerce\", format=\"%d.%m.%Y\")\n",
    "yarisma_df = yarisma_df.dropna().reset_index(drop=True)\n",
    "\n",
    "pasifik_df_sorted[\"yarisma\"] = 0\n",
    "horizon_saha_df_sorted[\"yarisma\"] = 0\n",
    "btt_df_sorted[\"yarisma\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(yarisma_df)):\n",
    "    yarisma_df[\"type\"].fillna(0, inplace=True)\n",
    "    if yarisma_df.loc[idx, \"type\"] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        temp_df = horizon_saha_df_sorted[(horizon_saha_df_sorted[\"date\"] == yarisma_df.loc[idx, \"date\"]) & \n",
    "                                         (horizon_saha_df_sorted[\"grup_adi\"] == yarisma_df.loc[idx, \"grup_adi\"]) & \n",
    "                                         (horizon_saha_df_sorted[yarisma_df.loc[idx, \"type\"]] == yarisma_df.loc[idx, \"col1\"])]\n",
    "        idx_to_replace = temp_df.index\n",
    "        horizon_saha_df_sorted.loc[idx_to_replace, \"yarisma\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Yarışam verisinin eklenme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Arada tarih verisi olmayan gözlemler\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SKU Sayıları\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month_ = datetime.now().month\n",
    "# year_ = datetime.now().year\n",
    "month_ = 5\n",
    "year_ = 2021\n",
    "\n",
    "date__ = datetime(year_, month_, 1)\n",
    "\n",
    "pas_df_all2 = pas_backup.copy()\n",
    "hor_df_all2 = hor_backup.copy()\n",
    "btt_df_all2_ = btt_backup.copy()\n",
    "\n",
    "pas_df_all2 = pas_df_all2[pas_df_all2[\"date\"] <= date__]\n",
    "hor_df_all2 = hor_df_all2[hor_df_all2[\"date\"] <= date__]\n",
    "btt_df_all2_ = btt_df_all2_[btt_df_all2_[\"date\"] <= date__]\n",
    "\n",
    "pas_df_all2.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "hor_df_all2.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "btt_df_all2_.rename(columns={\"Date\": \"date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku = []\n",
    "grup_adi = []\n",
    "portfoy = []\n",
    "oran = []\n",
    "gozlem_sayisi = []\n",
    "toplam_satir = []\n",
    "baslangic_tarih = []\n",
    "bitis_tarih = []\n",
    "son_kac_ay_eksik = []\n",
    "kanal = []\n",
    "repeat_num = [] # 1'lerin kaç aralıkla geldiği\n",
    "for idx, temp_df in pas_df_all2[[\"grup_adi\", \"en_guncel_kod\", \"portfoy\", \"adet\", \"date\"]].groupby([\"grup_adi\", \"en_guncel_kod\", \"portfoy\"]):\n",
    "    sku.append(temp_df[\"en_guncel_kod\"].iloc[0])\n",
    "    grup_adi.append(temp_df[\"grup_adi\"].iloc[0])\n",
    "    portfoy.append(temp_df[\"portfoy\"].iloc[0])\n",
    "    oran.append(len(temp_df[temp_df[\"adet\"] != 1]) / len(temp_df))\n",
    "    gozlem_sayisi.append(len(temp_df[temp_df[\"adet\"] != 1]))\n",
    "    baslangic_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].min())\n",
    "    bitis_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].max())\n",
    "    toplam_satir.append(len(temp_df))\n",
    "    son_kac_ay_eksik.append(round((pas_df_all2.date.max() - temp_df[temp_df[\"adet\"] != 1].date.max())/np.timedelta64(1, 'M'),1))\n",
    "    kanal.append(\"Pasifik\")\n",
    "\n",
    "    \n",
    "    lst = temp_df[temp_df[\"adet\"] == 1].index\n",
    "    lst = sorted(lst, reverse=True)\n",
    "    counter = 0\n",
    "    for idx_ in range(len(lst) - 1):\n",
    "        if lst[idx_] - lst[idx_+1] != 1:\n",
    "            counter+=1\n",
    "        else: \n",
    "            pass\n",
    "    if counter != 0:\n",
    "        counter+=1\n",
    "    repeat_num.append(counter)\n",
    "\n",
    "for idx, temp_df in hor_df_all2[[\"grup_adi\", \"en_guncel_kod\", \"portfoy\", \"adet\", \"date\"]].groupby([\"grup_adi\", \"en_guncel_kod\", \"portfoy\"]):\n",
    "    sku.append(temp_df[\"en_guncel_kod\"].iloc[0])\n",
    "    grup_adi.append(temp_df[\"grup_adi\"].iloc[0])\n",
    "    portfoy.append(temp_df[\"portfoy\"].iloc[0])\n",
    "    oran.append(len(temp_df[temp_df[\"adet\"] != 1]) / len(temp_df))\n",
    "    gozlem_sayisi.append(len(temp_df[temp_df[\"adet\"] != 1]))\n",
    "    baslangic_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].min())\n",
    "    bitis_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].max())\n",
    "    toplam_satir.append(len(temp_df))\n",
    "    son_kac_ay_eksik.append(round((hor_df_all2.date.max() - temp_df[temp_df[\"adet\"] != 1].date.max())/np.timedelta64(1, 'M'),1))\n",
    "    kanal.append(\"Horizon\")\n",
    "\n",
    "\n",
    "    lst = temp_df[temp_df[\"adet\"] == 1].index\n",
    "    lst = sorted(lst, reverse=True)\n",
    "    counter = 0\n",
    "    for idx_ in range(len(lst) - 1):\n",
    "        if lst[idx_] - lst[idx_+1] != 1:\n",
    "            counter+=1\n",
    "        else: \n",
    "            pass\n",
    "    if counter != 0:\n",
    "        counter+=1\n",
    "    repeat_num.append(counter)\n",
    "\n",
    "for idx, temp_df in btt_df_all2_[[\"grup_adi\", \"en_guncel_kod\", \"portfoy\", \"adet\", \"date\"]].groupby([\"grup_adi\", \"en_guncel_kod\", \"portfoy\"]):\n",
    "    sku.append(temp_df[\"en_guncel_kod\"].iloc[0])\n",
    "    grup_adi.append(temp_df[\"grup_adi\"].iloc[0])\n",
    "    portfoy.append(temp_df[\"portfoy\"].iloc[0])\n",
    "    oran.append(len(temp_df[temp_df[\"adet\"] != 1]) / len(temp_df))\n",
    "    gozlem_sayisi.append(len(temp_df[temp_df[\"adet\"] != 1]))\n",
    "    baslangic_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].min())\n",
    "    bitis_tarih.append(temp_df[temp_df[\"adet\"] != 1][\"date\"].max())\n",
    "    toplam_satir.append(len(temp_df))\n",
    "    son_kac_ay_eksik.append(round((btt_df_all2_.date.max() - temp_df[temp_df[\"adet\"] != 1].date.max())/np.timedelta64(1, 'M'),1))\n",
    "    kanal.append(\"BTT\")\n",
    "\n",
    "\n",
    "    lst = temp_df[temp_df[\"adet\"] == 1].index\n",
    "    lst = sorted(lst, reverse=True)\n",
    "    counter = 0\n",
    "    for idx_ in range(len(lst) - 1):\n",
    "        if lst[idx_] - lst[idx_+1] != 1:\n",
    "            counter+=1\n",
    "        else: \n",
    "            pass\n",
    "    if counter != 0:\n",
    "        counter+=1\n",
    "    repeat_num.append(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_sayilari = pd.DataFrame({\"sku\": sku,\n",
    "                             \"grup_adi\": grup_adi,\n",
    "                             \"portfoy\": portfoy,\n",
    "                             \"gozlem_sayisi\": gozlem_sayisi,\n",
    "                             \"toplam_satir\": toplam_satir,\n",
    "                             \"oran\": oran,\n",
    "                             \"baslangic_tarih\": baslangic_tarih,\n",
    "                             \"bitis_tarih\": bitis_tarih,\n",
    "                             \"son_kac_ay_eksik\": son_kac_ay_eksik,\n",
    "                             \"eksik_repeat_sayisi\": repeat_num,\n",
    "                             \"kanal\": kanal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_sayilari.sort_values(by=[\"sku\", \"grup_adi\", \"oran\"], ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('SKU gözlem sayılarının elde edilme süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Adetteki düzenlemeleri new_adet üzerinden devam ettiriyoruz\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"new_adet\"] = pasifik_df_sorted[\"adet\"]\n",
    "horizon_saha_df_sorted[\"new_adet\"] = horizon_saha_df_sorted[\"adet\"]\n",
    "btt_df_sorted[\"new_adet\"] = btt_df_sorted[\"adet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SKU Sayıları Dataya Ekleme, Scope Belirlenmesi İçin Gerekli\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_insights = sku_sayilari.copy()\n",
    "sku_insights.drop(columns=[\"portfoy\", \"kanal\"], axis=1, inplace=True)\n",
    "sku_insights.rename(columns={\"sku\": \"en_guncel_kod\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sku_sayilari = sku_sayilari[[\"sku\"] + sku_insights.columns.to_list()[1:]]\n",
    "    sku_sayilari.columns = sku_insights.columns\n",
    "except:\n",
    "    sku_sayilari.columns = sku_insights.columns    \n",
    "\n",
    "pasifik_df_sorted = pasifik_df_sorted.merge(sku_sayilari, on=[\"en_guncel_kod\", \"grup_adi\"], how=\"left\")\n",
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(sku_sayilari, on=[\"en_guncel_kod\", \"grup_adi\"], how=\"left\")\n",
    "btt_df_sorted = btt_df_sorted.merge(sku_sayilari, on=[\"en_guncel_kod\", \"grup_adi\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SKU Labellama\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"scope\"] = 0\n",
    "horizon_saha_df_sorted[\"scope\"] = 0\n",
    "btt_df_sorted[\"scope\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_pas1 = pasifik_df_sorted.copy()\n",
    "backup_hor1 = horizon_saha_df_sorted.copy()\n",
    "backup_btt1 = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope(data, obs_threshold, obs_mean_threshold, obs_ratio, latest_obs_date):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    obs_threshold:        int\n",
    "    obs_mean_threshold:   int\n",
    "    obs_ratio:            float\n",
    "    latest_obs_date:      datetime\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    obs_threshold:        Toplam kaç adet gözlemi olsun?\n",
    "    obs_mean_threshold:   Kaç gözlem altında ortalama basılıp geçilsin?\n",
    "    obs_ratio:            Gözlem oranı yüzde kaç olmalı? %50'nin altında ise alınmasın vb.\n",
    "    latest_obs_date:      En son hangi tarihte gözlemi olursa analize dahil edilsin?\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    data[\"scope\"] = 0\n",
    "    for sku in data[\"en_guncel_kod\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "            \n",
    "            temp_df = data[(data[\"en_guncel_kod\"] == sku) & \n",
    "                           (data[\"grup_adi\"] == grup)].reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            if len(temp_df) > 1:\n",
    "                gozlem_sayisi = temp_df[\"gozlem_sayisi\"][0]\n",
    "                oran = temp_df[\"oran\"][0]\n",
    "                bitis_tarihi = temp_df[\"bitis_tarih\"][0]\n",
    "                \n",
    "                if (gozlem_sayisi < obs_threshold) and (oran < obs_ratio) and (bitis_tarihi <= latest_obs_date):\n",
    "                    temp_df[\"scope\"] = 0 # Kapsam Dışı\n",
    "                else:\n",
    "                    if (gozlem_sayisi < obs_mean_threshold) and (bitis_tarihi > latest_obs_date):\n",
    "                        temp_df[\"scope\"] = 1 # Ortalama basılacak olan\n",
    "                    elif (gozlem_sayisi >= obs_threshold) and (oran >= obs_ratio) and (bitis_tarihi > latest_obs_date):\n",
    "                        temp_df[\"scope\"] = 2 # Time Series\n",
    "                    elif (gozlem_sayisi >= obs_mean_threshold) and (bitis_tarihi > latest_obs_date):\n",
    "                        temp_df[\"scope\"] = 3 # Regresyon\n",
    "            else:\n",
    "                pass\n",
    "            all_data.append(temp_df)\n",
    "    all_data = pd.concat(all_data)\n",
    "    all_data.reset_index(drop=True, inplace=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = scope(pasifik_df_sorted, 36, 12, 0.85, datetime(2020, 12, 1))\n",
    "horizon_saha_df_sorted = scope(horizon_saha_df_sorted, 36, 12, 0.85, datetime(2020, 12, 1))\n",
    "btt_df_sorted = scope(btt_df_sorted, 36, 12, 0.85, datetime(2020, 12, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kapsam labellaması süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Kapsamı yeniden düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kapsam_all.columns = change_col_name(kapsam_all)\n",
    "eslenik_kod_df.columns = change_col_name(eslenik_kod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kapsam_all[\"urun_kodu\"] = kapsam_all[\"urun_kodu\"].apply(lambda x: int(x.split(\"-\")[0]+x.split(\"-\")[1]))\n",
    "kapsam_all = kapsam_all.merge(eslenik_kod_df[[\"urun_kodu\", \"en_guncel_kod\"]], how=\"left\", on=\"urun_kodu\")\n",
    "kapsam_all[\"en_guncel_kod\"].fillna(kapsam_all[\"urun_kodu\"], inplace=True)\n",
    "kapsam_all.drop_duplicates(subset=[\"en_guncel_kod\", \"grup_adi\"], inplace=True, ignore_index=True)\n",
    "kapsam_all = kapsam_all[kapsam_all[\"en_guncel_kod\"] != \"delist\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKUP_PASIFIK = pasifik_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df_sorted.merge(kapsam_all[[\"en_guncel_kod\", \"grup_adi\", \"durum\"]], how=\"left\", on=[\"en_guncel_kod\", \"grup_adi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_write_deneme = list(pasifik_df_sorted[(pasifik_df_sorted[\"durum\"].isna()) & (pasifik_df_sorted[\"grup_adi\"].isin([\"A101\", \"ŞOK\", \"BİM\"]))].index)\n",
    "check = pasifik_df_sorted[(pasifik_df_sorted[\"grup_adi\"] == \"BİM\") & (pasifik_df_sorted[\"portfoy\"] == 1)]\n",
    "check = check[[\"en_guncel_kod\", \"grup_adi\", \"portfoy\", \"durum\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.loc[idx_to_write_deneme, \"durum\"] = \"DENEME\"\n",
    "horizon_saha_df_sorted[\"durum\"] = np.nan\n",
    "btt_df_sorted[\"durum\"] = np.nan\n",
    "pasifik_df_sorted[\"Kanal\"] = \"pasifik\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_backup_kapsam = pasifik_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_scope_index = pasifik_df_sorted[(pasifik_df_sorted[\"portfoy\"] == 1) & \n",
    "                                       (pasifik_df_sorted[\"Kanal\"] == \"pasifik\") & \n",
    "                                       (pasifik_df_sorted[\"durum\"].isin(['DENEME+BÖLGESEL SATIŞ', 'DENEME'])) & \n",
    "                                       (~pasifik_df_sorted[\"grup_adi\"].isin([\"Diğer_Pasifik\", \"MİGROS\"]))].index\n",
    "\n",
    "pasifik_df_sorted.loc[change_scope_index, \"scope\"] = 0\n",
    "pasifik_df_sorted.loc[change_scope_index, \"scope_type\"] = \"kapsam_disi\"\n",
    "pasifik_df_sorted.drop(\"Kanal\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Peak ve Dip Noktalarını Belirleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_detection(data, sigma):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    sigma:                float\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    sigma:                Standart sapma ile çarpılacak olan sayı. Verinin yüzdelik olarak hangi kısmının threshold olarak alınacağına bu değer ile karar verilir. Örn: 1.96 verilmesi durumunda %95'e tekabul eder. \n",
    "                          (https://www.socscistatistics.com/pvalues/normaldistribution.aspx)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    demand = data.copy()\n",
    "    demand[\"peak\"] = 0\n",
    "    demand_non_portfoy = demand[demand[\"portfoy\"] == 0]\n",
    "    demand = demand[demand[\"portfoy\"] == 1]\n",
    "\n",
    "    grup = demand[\"grup_adi\"].unique()\n",
    "    urun = demand[\"en_guncel_kod\"].unique()\n",
    "    for grp in grup:\n",
    "        for sku in urun:\n",
    "            df=demand[(demand['en_guncel_kod']==sku) & \n",
    "                      (demand[\"grup_adi\"] == grp)]\n",
    "            points=df['adet']\n",
    "\n",
    "            mean = points.mean()\n",
    "            std = points.std()\n",
    "\n",
    "            peaks=[]\n",
    "            index=[]\n",
    "            for idx in points.index:\n",
    "                if points[idx]>=(sigma*std+mean):\n",
    "                    index.append(idx)\n",
    "\n",
    "            for idx_ in index:\n",
    "                demand.loc[idx_, \"peak\"] = 1\n",
    "    demand = pd.concat([demand, demand_non_portfoy], axis=0)\n",
    "    demand.sort_index(inplace=True)\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_detection(data, sigma):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    sigma:                float\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    sigma:                Standart sapma ile çarpılacak olan sayı. Verinin yüzdelik olarak hangi kısmının threshold olarak alınacağına bu değer ile karar verilir. Örn: 1.96 verilmesi durumunda %95'e tekabul eder. \n",
    "                          (https://www.socscistatistics.com/pvalues/normaldistribution.aspx)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    demand = data.copy()\n",
    "    demand_non_portfoy = demand[demand[\"portfoy\"] == 0]\n",
    "    demand = demand[demand[\"portfoy\"] == 1]\n",
    "\n",
    "    grup = demand[\"grup_adi\"].unique()\n",
    "    urun = demand[\"en_guncel_kod\"].unique()\n",
    "    for grp in grup:\n",
    "        for sku in urun:\n",
    "            df=demand[(demand['en_guncel_kod']==sku) & \n",
    "                      (demand[\"grup_adi\"] == grp)]\n",
    "            points=df['adet']\n",
    "\n",
    "            mean = points.mean()\n",
    "            std = points.std()\n",
    "\n",
    "            peaks=[]\n",
    "            index=[]\n",
    "            if mean <= 100000:\n",
    "                for idx in points.index:\n",
    "                    if (points[idx]<=(mean-sigma*std)):\n",
    "                        index.append(idx)\n",
    "\n",
    "\n",
    "                for idx_ in sorted(index, reverse=True):\n",
    "                    try:\n",
    "                        demand.loc[idx_-1, \"new_adet\"] += demand.loc[idx_, \"new_adet\"] \n",
    "                        demand.loc[idx_, \"new_adet\"] = 1\n",
    "                    except KeyError:\n",
    "                        index.remove(idx_)\n",
    "            else:\n",
    "                for idx in points.index:\n",
    "                    if (points[idx]<=(mean-2*std)):\n",
    "                        index.append(idx)\n",
    "\n",
    "\n",
    "                for idx_ in sorted(index, reverse=True):\n",
    "                    try:\n",
    "                        demand.loc[idx_-1, \"new_adet\"] += demand.loc[idx_, \"new_adet\"] \n",
    "                        demand.loc[idx_, \"new_adet\"] = 1\n",
    "                    except KeyError:\n",
    "                        index.remove(idx_)\n",
    "    demand = pd.concat([demand, demand_non_portfoy], axis=0)\n",
    "    demand.sort_index(inplace=True)\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_distribution(data, sigma):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    data:                 dataFrame\n",
    "    sigma:                float\n",
    "    \n",
    "    Description\n",
    "    ----------\n",
    "    \n",
    "    data:                 Düzenlemenin yapılacağı veri seti.\n",
    "    sigma:                Standart sapma ile çarpılacak olan sayı. Verinin yüzdelik olarak hangi kısmının threshold olarak alınacağına bu değer ile karar verilir. Örn: 1.96 verilmesi durumunda %95'e tekabul eder. \n",
    "                          (https://www.socscistatistics.com/pvalues/normaldistribution.aspx)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    demand = data.copy()\n",
    "    demand_non_portfoy = demand[demand[\"portfoy\"] == 0]\n",
    "    demand = demand[demand[\"portfoy\"] == 1]\n",
    "\n",
    "    grup = demand[\"grup_adi\"].unique()\n",
    "    urun = demand[\"en_guncel_kod\"].unique()\n",
    "    for grp in grup:\n",
    "        for sku in urun:\n",
    "            df=demand[(demand['en_guncel_kod']==sku) & \n",
    "                      (demand[\"grup_adi\"] == grp)]\n",
    "            points=df['adet']\n",
    "            \n",
    "            mean = points.mean()\n",
    "            std = points.std()\n",
    "            peak_index = df[df[\"peak\"] == 1].index\n",
    "            idx_to_add = list(df[(df[\"peak\"] != 1) & (df[\"adet\"] != 1)].index)\n",
    "            for idx in peak_index:\n",
    "                peak_value = demand.loc[idx, \"adet\"]\n",
    "                will_add = (peak_value - (mean+std*sigma)) / len(idx_to_add)\n",
    "                demand.loc[idx_to_add, \"new_adet\"] += will_add\n",
    "                demand.loc[idx, \"new_adet\"] = mean+std*sigma\n",
    "    demand = pd.concat([demand, demand_non_portfoy], axis=0)\n",
    "    demand.sort_index(inplace=True)\n",
    "    return demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pas_backup.copy()\n",
    "horizon_saha_df_sorted = hor_backup.copy()\n",
    "btt_df_sorted = btt_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted, btt_df_sorted, horizon_saha_df_sorted = peak_detection(pasifik_df_sorted, 1.5), peak_detection(btt_df_sorted, 1.5), peak_detection(horizon_saha_df_sorted, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Peak Detection süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup2 = pasifik_df_sorted.copy()\n",
    "hor_backup2 = horizon_saha_df_sorted.copy()\n",
    "btt_backup2 = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = peak_distribution(pasifik_df_sorted, 1.5)\n",
    "btt_df_sorted = peak_distribution(btt_df_sorted, 1.5)\n",
    "horizon_saha_df_sorted = peak_distribution(horizon_saha_df_sorted, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Peak Distribution süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted, btt_df_sorted, horizon_saha_df_sorted = deep_detection(pasifik_df_sorted, 1.5), deep_detection(btt_df_sorted, 1.5), deep_detection(horizon_saha_df_sorted, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Deep Detection süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pas_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"raf_tavsiye_satis_fiyati\", \"i̇ndirimli_raf_satis_fiyati\", \n",
    "                        \"aktivite_tipi\", \"indirim_\"]\n",
    "\n",
    "    btt_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"i̇skonto_\"]\n",
    "\n",
    "    hor_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"i̇skonto_\"]\n",
    "\n",
    "    pasifik_df_sorted.drop(columns=pas_cols_to_drop, inplace=True)\n",
    "    horizon_saha_df_sorted.drop(columns=hor_cols_to_drop, inplace=True)\n",
    "    btt_df_sorted.drop(columns=btt_cols_to_drop, inplace=True)\n",
    "except:\n",
    "    pas_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"raf_tavsiye_satis_fiyati\", \"i̇ndirimli_raf_satis_fiyati\", \n",
    "                        \"aktivite_tipi\", \"indirim_\"]\n",
    "\n",
    "    btt_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"iskonto_\"]\n",
    "\n",
    "    hor_cols_to_drop = [\"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                        \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                        \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\", \"fiyat\", \"fiyat_gecisi\", \"ciro_kull_i̇ade_dus\", \"promosyon_tutari\", \n",
    "                        \"iskonto_\"]\n",
    "\n",
    "    pasifik_df_sorted.drop(columns=pas_cols_to_drop, inplace=True)\n",
    "    horizon_saha_df_sorted.drop(columns=hor_cols_to_drop, inplace=True)\n",
    "    btt_df_sorted.drop(columns=btt_cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pd.merge(pasifik_df_sorted, pasifik_aktivite_df3[['yil', 'ay', 'grup_adi', 'en_guncel_kod', \n",
    "                                                                      'raf_tavsiye_satis_fiyati', 'i̇ndirimli_raf_satis_fiyati', 'indirim_', \n",
    "                                                                      'aktivite_tipi']], \n",
    "                             left_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                             right_on=['yil', 'ay', 'grup_adi', 'en_guncel_kod'], \n",
    "                             how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted = horizon_saha_df_sorted.merge(saha_aktivite_detay3[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                                            'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                                      on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], \n",
    "                                                      how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btt_df_sorted = btt_df_sorted.merge(btt_aktivite[['yil', 'ay', 'grup_adi', 'ciro_kull_i̇ade_dus', \n",
    "                                                  'promosyon_tutari', 'i̇skonto_', 'en_guncel_kod']],\n",
    "                                    on=['en_guncel_kod', 'yil', 'ay', 'grup_adi'], \n",
    "                                    how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"indirim__\"] = [0 if akt < 0 else akt for akt in pasifik_df_sorted[\"indirim_\"]]\n",
    "pasifik_df_sorted.drop(\"indirim_\", axis=1, inplace=True)\n",
    "\n",
    "horizon_saha_df_sorted[\"i̇skonto__\"] = [0 if ((akt >= 0.35) or (akt <=0.01)) else akt for akt in horizon_saha_df_sorted[\"i̇skonto_\"]]\n",
    "horizon_saha_df_sorted.drop(\"i̇skonto_\", axis=1, inplace=True)\n",
    "\n",
    "btt_df_sorted[\"i̇skonto__\"] = [0 if ((akt >= 0.35) or (akt <=0.01)) else akt for akt in btt_df_sorted[\"i̇skonto_\"]]\n",
    "btt_df_sorted.drop(\"i̇skonto_\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chng_cols = dict(zip(['Yıl', 'Ay', 'No_of_days', 'Weekdays_n', 'Weekdays_Ratio', 'Weekend_n',\n",
    "                      'Weekend_Ratio', 'Actual_Holiday_n', 'Actual_Holiday_Ratio',\n",
    "                      'Total_Holiday_n', 'Total_Holiday_Ratio', 'School_Day_n',\n",
    "                      'School_Day_Ratio', 'School_Day_brdg_n', 'School_Day_brdg_Ratio',\n",
    "                      'Ramadan_n', 'Ramadan_Ratio', 'Pandemic', 'Lockdown'],\n",
    "                     \n",
    "                     [\"yil\", \"ay\", \"no_of_days\", \"weekdays_n\", \"weekdays_ratio\", \"weekend_n\", \n",
    "                      \"weekend_ratio\", \"actual_holiday_n\", \"actual_holiday_ratio\",\n",
    "                      \"total_holiday_n\", \"total_holiday_ratio\", \"school_day_n\", \n",
    "                      \"school_day_ratio\", \"school_day_brdg_n\", \"school_day_brdg_ratio\",\n",
    "                      \"ramadan_n\", \"ramadan_ratio\", \"pandemic\", \"lockdown\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calender_df.rename(columns=chng_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pd.merge(pasifik_df_sorted, calender_df, on=[\"yil\", \"ay\"], how=\"left\")\n",
    "btt_df_sorted = pd.merge(btt_df_sorted, calender_df, on=[\"yil\", \"ay\"], how=\"left\")\n",
    "horizon_saha_df_sorted = pd.merge(horizon_saha_df_sorted, calender_df, on=[\"yil\", \"ay\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"indirim__\"].fillna(0, inplace=True)\n",
    "horizon_saha_df_sorted[\"i̇skonto__\"].fillna(0, inplace=True)\n",
    "btt_df_sorted[\"i̇skonto__\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted.rename(columns={\"i̇skonto__\": \"indirim__\"}, inplace=True)\n",
    "btt_df_sorted.rename(columns={\"i̇skonto__\": \"indirim__\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bir önceki aya yansımış aktiviteleri düzenleme\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aktivite_regulation(df):\n",
    "    df_all = []\n",
    "    for sku in df[\"en_guncel_kod\"].unique():\n",
    "        for grup in df[\"grup_adi\"].unique():\n",
    "            test = df[(df[\"en_guncel_kod\"] == sku) & (df[\"grup_adi\"] == grup)]\n",
    "            for idx in test.index:\n",
    "                if (idx-2 not in test.index) and (idx-1 in test.index): # bir öncekine bakacak. ilk ve sonraki satıra bakacak. -1 götür\n",
    "                    if test.loc[idx-1, \"adet\"] > test.loc[idx, \"adet\"]:\n",
    "                        test.loc[idx-1, \"indirim__\"] += test.loc[idx, \"indirim__\"]\n",
    "                        test.loc[idx, \"indirim__\"] = 0\n",
    "                elif (idx-1 not in test.index): # ilk satırdayız. pass\n",
    "                    pass\n",
    "                else:\n",
    "                    dic = {}\n",
    "                    dic.update({idx-2: test.loc[idx-2, \"adet\"], \n",
    "                                idx-1: test.loc[idx-1, \"adet\"],\n",
    "                                idx: test.loc[idx, \"adet\"]})\n",
    "                    max_idx = max(dic, key=dic.get)\n",
    "                    if max_idx == idx:\n",
    "                        pass\n",
    "                    else:\n",
    "                        test.loc[max_idx, \"indirim__\"] += test.loc[idx, \"indirim__\"]\n",
    "                        test.loc[idx, \"indirim__\"] = 0\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasifik_df_sorted, horizon_saha_df_sorted, btt_df_sorted = aktivite_regulation(pasifik_df_sorted), aktivite_regulation(horizon_saha_df_sorted), aktivite_regulation(btt_df_sorted)\n",
    "#PASİFİK İÇİN İPTAL\n",
    "horizon_saha_df_sorted, btt_df_sorted = aktivite_regulation(horizon_saha_df_sorted), aktivite_regulation(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Aktivite Regulation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pasifik_aktivite_regulation(df):\n",
    "    df_all = []\n",
    "    for sku in df[\"en_guncel_kod\"].unique():\n",
    "        for grup in df[\"grup_adi\"].unique():\n",
    "            test = df[(df[\"en_guncel_kod\"] == sku) & (df[\"grup_adi\"] == grup)]\n",
    "            aktivite_tip_index = test[~test.aktivite_tipi.isna()].index.to_list()\n",
    "            for idx in aktivite_tip_index:\n",
    "                if idx == test.index[0]: # ilk satırsa atla\n",
    "                    pass\n",
    "                elif idx-1 in aktivite_tip_index: #bir önceki satırda aktivite varsa atla\n",
    "                    pass\n",
    "                else:\n",
    "                    if test.loc[idx, 'adet'] < test.loc[idx-1, 'adet']: # adet sayısı bir önceki satırdan küçükse \n",
    "                        test.loc[idx-1, 'aktivite_tipi'] = test.loc[idx, 'aktivite_tipi'] # aktiviteyi bir önceki satıra yaz\n",
    "                        test.loc[idx, 'aktivite_tipi'] = np.nan\n",
    "                        aktivite_tip_index.remove(idx) #listeden remove et ki bir alt satırda varsa önceki var mı kontrolüne takılmasın\n",
    "                        aktivite_tip_index.insert(0,0) #döngü listedeki elementlerin index'ine göre devam ettiği için en başa 0 insert et\n",
    "\n",
    "            indirim_index = test[test.indirim__ != 0].index.to_list()\n",
    "            for idx in indirim_index:\n",
    "                if idx == test.index[0]: # ilk satır\n",
    "                    pass \n",
    "                elif idx-1 in indirim_index: #bir önceki satırda indirim yüzdesi varsa atla\n",
    "                    pass \n",
    "                else:\n",
    "                    if test.loc[idx, 'adet'] < test.loc[idx-1, 'adet']:\n",
    "                        test.loc[idx-1, 'indirim__'] = test.loc[idx, 'indirim__'] #bir üste taşındı\n",
    "                        test.loc[idx, 'indirim__'] = 0  \n",
    "                        indirim_index.remove(idx)  #listeden remove et ki bir alt satırda varsa önceki var mı kontrolüne takılmasın\n",
    "                        indirim_index.insert(0, 0) #döngü listedeki elementlerin index'ine göre devam ettiği için en başa 0 insert et\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_aktivite_regulation(pasifik_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Pasifik Aktivite Regulation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Binslere ayırma\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, pasifik_df_sorted.indirim__.max()+1]\n",
    "#indirim_bins = [0, 0.009, 0.05, 0.10, 0.15, 0.20, 0.25, 0.50, pasifik_df_sorted.indirim__.max()+1]\n",
    "pasifik_df_sorted.indirim__.hist(bins=indirim_bins)\n",
    "len(indirim_bins) , pasifik_df_sorted.indirim__.value_counts(bins=indirim_bins).sort_index()\n",
    "pasifik_df_sorted['indirim__bins'] = pd.cut(pasifik_df_sorted.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, horizon_saha_df_sorted.indirim__.max()+1]\n",
    "horizon_saha_df_sorted.indirim__.hist(bins=indirim_bins)\n",
    "len(indirim_bins) , horizon_saha_df_sorted.indirim__.value_counts(bins=indirim_bins)\n",
    "horizon_saha_df_sorted['indirim__bins'] = pd.cut(horizon_saha_df_sorted.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirim_bins = [0, 0.01, 0.02, 0.03, 0.04, 0.05, \n",
    "                0.06, 0.07, 0.08, 0.09, 0.10, \n",
    "                0.15, btt_df_sorted.indirim__.max()+1]\n",
    "btt_df_sorted.indirim__.hist(bins=indirim_bins)\n",
    "len(indirim_bins) , btt_df_sorted.indirim__.value_counts(bins=indirim_bins)\n",
    "btt_df_sorted['indirim__bins'] = pd.cut(btt_df_sorted.indirim__, indirim_bins).cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Missing Imputation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_imputation(data):\n",
    "    df_all = []\n",
    "    data[\"new_adet\"] = data[\"adet\"]\n",
    "    for sku in data[\"en_guncel_kod\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "            test = data[(data[\"en_guncel_kod\"] == sku) & (data[\"grup_adi\"] == grup)]\n",
    "            index_full = list(test[test[\"new_adet\"] != 1].index)\n",
    "            for idx in range(len(index_full) - 1):\n",
    "                if index_full[idx+1] - index_full[idx] != 1:\n",
    "                    index_na = list(range(index_full[idx]+1, index_full[idx+1]))\n",
    "                    fark = test.loc[index_full[idx+1], \"new_adet\"] - test.loc[index_full[idx], \"new_adet\"]\n",
    "                    bol = len(index_na)\n",
    "                    ekle = fark/(bol+1)\n",
    "                    for i in index_na:\n",
    "                        test.loc[i, \"new_adet\"] = 0\n",
    "                        test.loc[i, \"new_adet\"] += ekle+test.loc[i-1, \"new_adet\"]\n",
    "                else:\n",
    "                    pass\n",
    "            df_all.append(test)\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas_backup = pasifik_df_sorted.copy()\n",
    "hor_backup = horizon_saha_df_sorted.copy()\n",
    "btt_backup = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = missing_imputation(pasifik_df_sorted)\n",
    "horizon_saha_df_sorted = missing_imputation(horizon_saha_df_sorted) \n",
    "btt_df_sorted = missing_imputation(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Missing Imputation süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_regression = pasifik_df_sorted.copy()\n",
    "horizon_regression = horizon_saha_df_sorted.copy()\n",
    "btt_regression = btt_df_sorted.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_regression = pasifik_regression[pasifik_regression[\"new_adet\"] != 1].reset_index(drop=True)\n",
    "horizon_regression = horizon_regression[horizon_regression[\"new_adet\"] != 1].reset_index(drop=True)\n",
    "btt_regression = btt_regression[btt_regression[\"new_adet\"] != 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_seasonality_decomp(data):\n",
    "    df_all = []\n",
    "    for sku in data[\"en_guncel_kod\"].unique():\n",
    "        for grup in data[\"grup_adi\"].unique():\n",
    "            temp_df = data[(data[\"en_guncel_kod\"] == sku) & \n",
    "                           (data[\"grup_adi\"] == grup)]\n",
    "            if len(temp_df) > 2:\n",
    "#                print(sku, grup)\n",
    "                df_ts = temp_df[['new_adet','date']]\n",
    "                df_ts.set_index('date',inplace=True)\n",
    "\n",
    "                result = STL(df_ts).fit()\n",
    "                temp_df['season'] = list(result.seasonal)\n",
    "                temp_df['trend']  = list(result.trend)\n",
    "                temp_df['residual']  = list(result.resid)\n",
    "                df_all.append(temp_df)\n",
    "            else:\n",
    "                pass\n",
    "    df_all = pd.concat(df_all)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "horizon_saha_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "btt_df_sorted.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], ignore_index=True, inplace=True)\n",
    "\n",
    "horizon_saha_df_sorted.drop_duplicates(subset=[\"date\", \"grup_adi\", \"en_guncel_kod\"], keep=\"first\", ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_reg = pasifik_df_sorted[pasifik_df_sorted[\"scope\"] == 3]\n",
    "horizon_reg = horizon_saha_df_sorted[horizon_saha_df_sorted[\"scope\"] == 3]\n",
    "btt_reg = btt_df_sorted[btt_df_sorted[\"scope\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df = trend_seasonality_decomp(pasifik_df_sorted)\n",
    "horizon_df = trend_seasonality_decomp(horizon_saha_df_sorted)\n",
    "btt_df = trend_seasonality_decomp(btt_df_sorted)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pasifik_reg = trend_seasonality_decomp(pasifik_reg)\n",
    "horizon_reg = trend_seasonality_decomp(horizon_reg)\n",
    "btt_reg = trend_seasonality_decomp(btt_reg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pasifik_df_sorted[\"season\"] = np.nan\n",
    "pasifik_df_sorted[\"trend\"] = np.nan\n",
    "pasifik_df_sorted[\"residual\"] = np.nan\n",
    "\n",
    "horizon_saha_df_sorted[\"season\"] = np.nan\n",
    "horizon_saha_df_sorted[\"trend\"] = np.nan\n",
    "horizon_saha_df_sorted[\"residual\"] = np.nan\n",
    "\n",
    "btt_df_sorted[\"season\"] = np.nan\n",
    "btt_df_sorted[\"trend\"] = np.nan\n",
    "btt_df_sorted[\"residual\"] = np.nan"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pasifik_reg = pasifik_reg[pasifik_reg[\"new_adet\"] != 1]\n",
    "horizon_reg = horizon_reg[horizon_reg[\"new_adet\"] != 1]\n",
    "btt_reg = btt_reg[btt_reg[\"new_adet\"] != 1]\n",
    "\n",
    "pasifik_df = pd.concat([pasifik_df_sorted[pasifik_df_sorted[\"scope\"] != 3], pasifik_reg], axis=0, ignore_index=True)\n",
    "horizon_df = pd.concat([horizon_saha_df_sorted[horizon_saha_df_sorted[\"scope\"] != 3], horizon_reg], axis=0, ignore_index=True)\n",
    "btt_df = pd.concat([btt_df_sorted[btt_df_sorted[\"scope\"] != 3], btt_reg], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Trend Seasonality süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Yeni adet flaglendi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df[\"adet_flag\"] = np.where(pasifik_df[\"adet\"] - pasifik_df[\"new_adet\"] == 0, 0, 1)\n",
    "horizon_df[\"adet_flag\"] = np.where(horizon_df[\"adet\"] - horizon_df[\"new_adet\"] == 0, 0, 1)\n",
    "btt_df[\"adet_flag\"] = np.where(btt_df[\"adet\"] - btt_df[\"new_adet\"] == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted = pasifik_df.copy()\n",
    "horizon_saha_df_sorted = horizon_df.copy()\n",
    "btt_df_sorted = btt_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_saha_df_sorted[\"aktivite_tipi\"] = np.nan\n",
    "btt_df_sorted[\"aktivite_tipi\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted.rename(columns={'i̇ndirimli_raf_satis_fiyati': 'ciro_kull_i̇ade_dus',\n",
    "                                  'raf_tavsiye_satis_fiyati': 'promosyon_tutari'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"Kanal\"] = \"pasifik\"\n",
    "horizon_saha_df_sorted[\"Kanal\"] = \"horizon\"\n",
    "btt_df_sorted[\"Kanal\"] = \"btt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_dict = {0: \"kapsam_disi\", 1: \"ortalama_basilacak\", 2: \"ts\", 3: \"regresyon\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasifik_df_sorted[\"scope_type\"] = pasifik_df_sorted[\"scope\"].map(scope_dict)\n",
    "horizon_saha_df_sorted[\"scope_type\"] = horizon_saha_df_sorted[\"scope\"].map(scope_dict)\n",
    "btt_df_sorted[\"scope_type\"] = btt_df_sorted[\"scope\"].map(scope_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([pasifik_df_sorted, horizon_saha_df_sorted, btt_df_sorted], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Aktivite Dictionary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aktivite_dict = {0:0, 'Mağaza içi/Dağılım':2, 'İn&out':5, \n",
    "                 'Çoklu Alım':8, 'Mutluluk':11, 'Kasiyer':14, 'CRM':17}\n",
    "\n",
    "df_all.aktivite_tipi.fillna(0, inplace=True)\n",
    "df_all.aktivite_tipi = df_all.aktivite_tipi.map(aktivite_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datanın dışarıya aktarılması\n",
    "### Fiyata Göre Sıralanmış Datanın Dışarıya Aktarılması\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[~((df_all[\"yil\"] == 2021) & (df_all[\"ay\"].isin([6, 7, 8, 9])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"../data/_all_data_36_12_siparis_coklamasiz_s1_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Datayı dışarıya çıkarma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Datanı altına kombinasyonların eklenmesi\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_all[(df_all[\"portfoy\"] == 1) & (df_all[\"scope_type\"] != \"kapsam_disi\")].reset_index(drop=True)\n",
    "data_backup = data.copy()\n",
    "sku_list = data.drop_duplicates(subset=[\"grup_adi\", \"en_guncel_kod\"], keep=\"first\", ignore_index=True)[[\"grup_adi\", \"en_guncel_kod\", \"Kanal\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enflasyon(df):\n",
    "#    month_ = datetime.now().month + 1\n",
    "#    year_ = datetime.now().year\n",
    "#    if month_ == 12:\n",
    "#        month_ = 1\n",
    "#    else:\n",
    "#        pass\n",
    "    month_ = 6\n",
    "    year_ = 2021\n",
    "    enflasyon_data = df[(df[\"date\"] >= datetime(year_, month_, 1)-relativedelta(month=2)) & (df[\"date\"] < datetime(year_, month_, 1))]\n",
    "    enflasyon_data = enflasyon_data[[\"yil\", \"ay\", \"date\", \"enflasyon_etkisi\"]].drop_duplicates(subset=[\"ay\", \"yil\", \"date\", \"enflasyon_etkisi\"], ignore_index=True)\n",
    "    enflasyon_data = pd.concat([enflasyon_data]*2, ignore_index=True)\n",
    "    for idx in range(4, len(enflasyon_data)):\n",
    "        enflasyon_data.loc[idx, \"enflasyon_etkisi\"] = np.nan\n",
    "        enflasyon_data.loc[idx, \"yil\"] = year_\n",
    "        enflasyon_data.loc[idx, \"ay\"] = idx+2\n",
    "        enflasyon_data.loc[idx, \"date\"] = datetime(year_, idx+2, 1)\n",
    "    for idx in range(4, len(enflasyon_data)):\n",
    "        enflasyon_data.loc[idx, \"enflasyon_etkisi\"] = enflasyon_data.loc[idx-4: idx-1, \"enflasyon_etkisi\"].mean()\n",
    "    enf_prev = data[data[\"date\"] < datetime(year_, month_, 1)][[\"yil\", \"ay\", \n",
    "                                                                \"date\", \"enflasyon_etkisi\"]].drop_duplicates(subset=[\"yil\", \"ay\", \n",
    "                                                                                                                     \"date\", \"enflasyon_etkisi\"]).sort_values(by=\"date\", ignore_index=True)\n",
    "    enflasyon_final = pd.concat([enf_prev, enflasyon_data.iloc[-4:, :]], axis=0, ignore_index=True)\n",
    "    return enflasyon_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enflasyon_data = create_enflasyon(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enf_dict_fill = {}\n",
    "for idx in range(len(enflasyon_data)):\n",
    "    enf_dict_fill.update({enflasyon_data[\"date\"][idx]: enflasyon_data[\"enflasyon_etkisi\"][idx]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comb(excels):\n",
    "#    month_ = datetime.now().month\n",
    "#    year_ = datetime.now().year\n",
    "    month_ = 5\n",
    "    year_ = 2021\n",
    "    pas_s=list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0, 2, 5, 8, 11, 14, 17]))\n",
    "    diger_s=list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0]))\n",
    "    df_all__=[]\n",
    "    for i in range(len(sku_list)):\n",
    "        tmp_=excels[(excels[\"grup_adi\"] == sku_list.loc[i, \"grup_adi\"]) & \n",
    "                   (excels[\"en_guncel_kod\"] == sku_list.loc[i, \"en_guncel_kod\"])]\n",
    "        new_date = datetime(year_, month_, 1)\n",
    "        if tmp_['Kanal'].values[0]=='pasifik':\n",
    "            for trh in range(1, 5):\n",
    "                new_date_ = new_date + relativedelta(months=trh)\n",
    "                create_new = pd.DataFrame(tmp_.iloc[-1])\n",
    "                create_new.loc[\"yil\"] = new_date_.year\n",
    "                create_new.loc[\"ay\"] = new_date_.month\n",
    "                create_new.loc[\"date\"] = datetime(new_date_.year, new_date_.month, 1)\n",
    "                create_new.loc[\"new_adet\"] = np.nan\n",
    "                create_new.loc[\"adet\"] = np.nan\n",
    "                create_new.loc[\"koli\"] = np.nan\n",
    "                create_new.loc[\"kg\"] = np.nan\n",
    "                create_new.loc[\"tl\"] = np.nan\n",
    "                create_new.loc[\"satis_var\"] = np.nan\n",
    "                create_new.loc[\"enflasyon_etkisi\"] = np.nan\n",
    "                create_new.loc[\"yarisma\"] = np.nan\n",
    "                create_new.loc[\"peak\"] = 0\n",
    "                create_new.loc[\"fiyat\"] = np.nan\n",
    "                create_new.loc[\"fiyat_gecisi\"] = np.nan\n",
    "                create_new.loc[\"pandemic\"] = 1\n",
    "                create_new.loc[\"lockdown\"] = 0\n",
    "                create_new.loc[\"season\"] = np.nan\n",
    "                create_new.loc[\"trend\"] = np.nan\n",
    "                tmp = pd.concat([create_new.T]*len(pas_s), ignore_index=True)\n",
    "                for j in range(len(pas_s)):\n",
    "                    tmp.loc[j,'indirim__bins'] = pas_s[j][0]\n",
    "                    tmp.loc[j,'aktivite_tipi'] = pas_s[j][1]\n",
    "                df_all__.append(tmp)\n",
    "        else:\n",
    "            for trh in range(1, 5):\n",
    "                new_date_ = new_date + relativedelta(months=trh)\n",
    "                create_new = pd.DataFrame(tmp_.iloc[-1])\n",
    "                create_new.loc[\"yil\"] = new_date_.year\n",
    "                create_new.loc[\"ay\"] = new_date_.month\n",
    "                create_new.loc[\"date\"] = datetime(new_date_.year, new_date_.month, 1)\n",
    "                create_new.loc[\"new_adet\"] = np.nan\n",
    "                create_new.loc[\"adet\"] = np.nan\n",
    "                create_new.loc[\"koli\"] = np.nan\n",
    "                create_new.loc[\"kg\"] = np.nan\n",
    "                create_new.loc[\"tl\"] = np.nan\n",
    "                create_new.loc[\"satis_var\"] = np.nan\n",
    "                create_new.loc[\"enflasyon_etkisi\"] = np.nan\n",
    "                create_new.loc[\"yarisma\"] = np.nan\n",
    "                create_new.loc[\"peak\"] = 0\n",
    "                create_new.loc[\"fiyat\"] = np.nan\n",
    "                create_new.loc[\"fiyat_gecisi\"] = np.nan\n",
    "                create_new.loc[\"pandemic\"] = 1\n",
    "                create_new.loc[\"lockdown\"] = 0\n",
    "                create_new.loc[\"season\"] = np.nan\n",
    "                create_new.loc[\"trend\"] = np.nan\n",
    "                tmp = pd.concat([create_new.T]*len(diger_s), ignore_index=True)\n",
    "                for k in range(len(diger_s)):\n",
    "                    tmp.loc[k,'indirim__bins']=diger_s[k][0]\n",
    "                    tmp.loc[k,'aktivite_tipi']=diger_s[k][1]\n",
    "                df_all__.append(tmp)\n",
    "    df_all__ = pd.concat(df_all__, ignore_index=True)\n",
    "    return df_all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = create_comb(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations.date.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = all_combinations[['new_adet', 'yil', 'ay', 'date', 'Kanal', 'grup_adi', 'ana_kategori_adi', \n",
    "                                     'kategori_adi', 'marka_adi', 'urun_adi', 'enflasyon_etkisi',  'peak', 'indirim__bins', \n",
    "                                     'aktivite_tipi', 'lockdown', 'season', 'trend', 'scope', 'scope_type', 'portfoy']]\n",
    "\n",
    "all_combinations[\"enflasyon_etkisi\"] = all_combinations[\"date\"].map(enf_dict_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combination_df(df_):\n",
    "    \"\"\" \n",
    "    Tüm kombinasyonlar için bir dictionary içerisinde 91 tane dataframe oluşturuyor. Input olarak aldığı df'in sonuna 3 satır \n",
    "    ekleyip indirim__bins ve aktivite_tipi'ni df_indirimbins_aktivitetipi olacak şekilde giriyor. new_adet değişkenine son 3 satır için NA atıyor.  \n",
    "    \"\"\"\n",
    "\n",
    "    indirim_bins_list = ['minus_one','zero','one','two','three','four','five','six','seven','eight','nine','ten','eleven']\n",
    "    aktivite_tipi_list = ['zero','two','five','eight','eleven','fourteen','seventeen']\n",
    "    df_names_dict = list(itertools.product(indirim_bins_list, aktivite_tipi_list))\n",
    "\n",
    "    ind_akt = list(itertools.product([-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],[0, 2, 5, 8, 11, 14, 17]))\n",
    "\n",
    "    df_names = []\n",
    "    for idx in df_names_dict:\n",
    "        df_names.append('df_' + idx[0] + '_' + idx[1])\n",
    "\n",
    "    df_dict = {}\n",
    "    for i in range(len(df_names)):\n",
    "        main_data = data.copy()\n",
    "        tmp = df_[(df_[\"aktivite_tipi\"] == ind_akt[i][1]) & \n",
    "                  (df_[\"indirim__bins\"] == ind_akt[i][0])]\n",
    "\n",
    "        new_comb_with_main_data = pd.concat([main_data, tmp], ignore_index=True)\n",
    "        new_comb_with_main_data.sort_values(by=[\"en_guncel_kod\", \"grup_adi\", \"date\"], inplace=True, ignore_index=True)\n",
    "        df_dict[df_names[i]] = new_comb_with_main_data.copy()\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combs_df = create_combination_df(all_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = swat.CAS('yhtrcl-sasccnt1.yildiz.domain', 5570, username='tunahan.aktas', password='34m153294T...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list(all_combs_df.keys())\n",
    "\n",
    "for dfs in data_list:\n",
    "    tmp = all_combs_df[dfs]\n",
    "    tmp = tmp[tmp[\"scope_type\"] == \"ts\"]\n",
    "    tmp = tmp[[\"date\", \"new_adet\", \"ana_kategori_adi\", \"grup_adi\", \"Kanal\", \"kategori_adi\", \"marka_adi\", \"urun_adi\", \"aktivite_tipi\",\n",
    "               \"enflasyon_etkisi\", \"indirim__bins\", \"lockdown\", \"peak\", \"season\", \"trend\"]]\n",
    "    tmp[\"enflasyon_etkisi\"] = tmp[\"date\"].map(enf_dict_fill)\n",
    "    tmp[\"new_adet\"] = tmp[\"new_adet\"].astype(\"float\")\n",
    "    tmp[\"aktivite_tipi\"] = tmp[\"aktivite_tipi\"].astype(\"float\")\n",
    "    tmp[\"indirim__bins\"] = tmp[\"indirim__bins\"].astype(\"float\")\n",
    "    tmp[\"lockdown\"] = tmp[\"lockdown\"].astype(\"float\")\n",
    "    tmp[\"peak\"] = tmp[\"peak\"].astype(\"float\")\n",
    "    tmp[\"date\"] = tmp[\"date\"].apply(lambda x: (x - datetime(1960, 1, 1)).days)\n",
    "    conn.upload(data=tmp, casout={'caslib':'CKLMSZ15', 'name':dfs, 'promote':True}, \n",
    "                importoptions={'vars':{'date':{'format': 'MMDDYY', \"type\": \"double\", \"length\": 10}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_list = [i.replace(\"df_\", \"ts_results_\") for i in data_list]\n",
    "input_names = pd.DataFrame({\"tables\": data_list})\n",
    "output_names = pd.DataFrame({\"tables\": output_data_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.upload(data=input_names, casout={'caslib':'CKLMSZ15', 'name':'DS_COMB_INPUT_TABLE_NAMES', 'promote':True})\n",
    "conn.upload(data=output_names, casout={'caslib':'CKLMSZ15', 'name':'DS_COMB_OUTPUT_TABLE_NAMES', 'promote':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_time - start_time))\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_all_process = datetime.now()\n",
    "print('Kodun çalışma süresi: {}'.format(end_all_process - start_all_process))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
